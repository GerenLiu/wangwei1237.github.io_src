{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"themes/ayer/source/404.html","path":"404.html","modified":1,"renderable":1},{"_id":"themes/ayer/source/favicon.ico","path":"favicon.ico","modified":1,"renderable":1},{"_id":"themes/ayer/source/google441c42e6b4b19747.html","path":"google441c42e6b4b19747.html","modified":1,"renderable":1},{"_id":"themes/ayer/source/css/clipboard.styl","path":"css/clipboard.styl","modified":1,"renderable":1},{"_id":"themes/ayer/source/css/custom.styl","path":"css/custom.styl","modified":1,"renderable":1},{"_id":"themes/ayer/source/dist/main.css","path":"dist/main.css","modified":1,"renderable":1},{"_id":"themes/ayer/source/dist/main.js","path":"dist/main.js","modified":1,"renderable":1},{"_id":"themes/ayer/source/images/404.jpg","path":"images/404.jpg","modified":1,"renderable":1},{"_id":"themes/ayer/source/images/ayer.png","path":"images/ayer.png","modified":1,"renderable":1},{"_id":"themes/ayer/source/images/ayer-side.svg","path":"images/ayer-side.svg","modified":1,"renderable":1},{"_id":"themes/ayer/source/images/ayer.svg","path":"images/ayer.svg","modified":1,"renderable":1},{"_id":"themes/ayer/source/images/forkme.png","path":"images/forkme.png","modified":1,"renderable":1},{"_id":"themes/ayer/source/images/cover7.jpg","path":"images/cover7.jpg","modified":1,"renderable":1},{"_id":"themes/ayer/source/images/sponsor.jpg","path":"images/sponsor.jpg","modified":1,"renderable":1},{"_id":"themes/ayer/source/js/busuanzi-2.3.pure.min.js","path":"js/busuanzi-2.3.pure.min.js","modified":1,"renderable":1},{"_id":"themes/ayer/source/js/clickLove.js","path":"js/clickLove.js","modified":1,"renderable":1},{"_id":"themes/ayer/source/js/tocbot.min.js","path":"js/tocbot.min.js","modified":1,"renderable":1},{"_id":"themes/ayer/source/js/lazyload.min.js","path":"js/lazyload.min.js","modified":1,"renderable":1},{"_id":"themes/ayer/source/js/search.js","path":"js/search.js","modified":1,"renderable":1},{"_id":"themes/ayer/source/images/alipay.jpg","path":"images/alipay.jpg","modified":1,"renderable":1},{"_id":"themes/ayer/source/images/cover2.jpg","path":"images/cover2.jpg","modified":1,"renderable":1},{"_id":"themes/ayer/source/images/wechat.jpg","path":"images/wechat.jpg","modified":1,"renderable":1},{"_id":"themes/ayer/source/js/jquery-2.0.3.min.js","path":"js/jquery-2.0.3.min.js","modified":1,"renderable":1},{"_id":"themes/ayer/source/images/cover6.jpg","path":"images/cover6.jpg","modified":1,"renderable":1},{"_id":"themes/ayer/source/images/cover1.jpg","path":"images/cover1.jpg","modified":1,"renderable":1},{"_id":"themes/ayer/source/images/cover3.jpg","path":"images/cover3.jpg","modified":1,"renderable":1},{"_id":"themes/ayer/source/images/cover4.jpg","path":"images/cover4.jpg","modified":1,"renderable":1},{"_id":"themes/ayer/source/images/cover5.jpg","path":"images/cover5.jpg","modified":1,"renderable":1}],"Cache":[{"_id":"themes/ayer/LICENSE","hash":"064037836065f908ffa619cbdae26dea2989ebeb","modified":1581038108891},{"_id":"themes/ayer/README.md","hash":"02e86adf578f0b54971eb7c094199004786dba20","modified":1588070066363},{"_id":"themes/ayer/_config.yml","hash":"c4a063bb9bfbed93bd5f85c4e300c137ddd39e4b","modified":1586838176532},{"_id":"themes/ayer/package.json","hash":"8c5a97d394b56d0e81d87dc63184543c5f2551a8","modified":1585725635146},{"_id":"themes/ayer/logo.png","hash":"16fbb131601570b21890a922cd6e596691ccff42","modified":1586609950120},{"_id":"themes/ayer/languages/de.yml","hash":"f1391e17d0422e6050171bfbc1780b4b01df4c55","modified":1587035968878},{"_id":"themes/ayer/languages/default.yml","hash":"cc74124ebfe5b673de248f6379153261b5371b43","modified":1587035968880},{"_id":"themes/ayer/languages/en.yml","hash":"cc74124ebfe5b673de248f6379153261b5371b43","modified":1587035968881},{"_id":"themes/ayer/languages/ko.yml","hash":"e0b760d295cb00fe6c10e12db1d1a0b3f74f7244","modified":1587035968884},{"_id":"themes/ayer/languages/fr.yml","hash":"78ed4392ac0cbdc8d2b64d1d9ecba2daa31bdb8d","modified":1587035968882},{"_id":"themes/ayer/languages/es.yml","hash":"2eaa4085a31960536bf603f0a86ea7fb58467b0a","modified":1587035968882},{"_id":"themes/ayer/languages/nl.yml","hash":"4ca66bec40af5fc9e06b3a1090c2c0962710ad66","modified":1587035968884},{"_id":"themes/ayer/languages/ja.yml","hash":"6a77987d7f406c14d85c8a1fd20484af5a1da5c3","modified":1587035968883},{"_id":"themes/ayer/languages/ru.yml","hash":"c829c571bbb14f2f523417e11d08cbcff2470e88","modified":1587035968886},{"_id":"themes/ayer/languages/no.yml","hash":"cacdfe2df667f2b62ac425ff2fa7db7e4fdb6ec3","modified":1587035968885},{"_id":"themes/ayer/languages/zh-CN.yml","hash":"1eb3233b86f9ac34798f9e3ad286b53e1c90f094","modified":1587035968888},{"_id":"themes/ayer/languages/vi.yml","hash":"2f31424cf9ed431a252e1dd3a3907e6a5bacbe75","modified":1587035968887},{"_id":"themes/ayer/languages/zh-TW.yml","hash":"9341ba22128facea892ef91d5204db689c05bc7d","modified":1587035968888},{"_id":"themes/ayer/languages/pt.yml","hash":"16ac6324fd43939ff6f1264d82b90c1ce06e7912","modified":1587035968885},{"_id":"themes/ayer/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1587035987112},{"_id":"themes/ayer/layout/categories.ejs","hash":"673e8cb93639525c08ee0b0cb9db44b3a628960c","modified":1587035987112},{"_id":"themes/ayer/layout/index.ejs","hash":"77347cbb8635611d8b1fb84bde75f9dedaf8632d","modified":1587035987113},{"_id":"themes/ayer/layout/post.ejs","hash":"a9a48ae63f5d68a36382951166fdd6e482b901f1","modified":1587035987114},{"_id":"themes/ayer/layout/layout.ejs","hash":"ad0d04b0d5322ccb6f371f8c9cd62d2ab87f4bca","modified":1587035987113},{"_id":"themes/ayer/scripts/default_config.js","hash":"70d6ee032d75410d540b6789b5949739a47f0125","modified":1587522234400},{"_id":"themes/ayer/layout/tags.ejs","hash":"88b34dd8d7b1e64fa27aa6ed72af996cf6700809","modified":1587035987115},{"_id":"themes/ayer/layout/page.ejs","hash":"a9a48ae63f5d68a36382951166fdd6e482b901f1","modified":1587035987114},{"_id":"themes/ayer/source/404.html","hash":"6e56aa9341d0443521851b0c7803e3471e30e635","modified":1587036002293},{"_id":"themes/ayer/source/.DS_Store","hash":"866eb25421614f569969f2d2c54abecc82c2adee","modified":1583289922353},{"_id":"themes/ayer/source/favicon.ico","hash":"473ba682e828a7e34f24fae320e77b6bed4260c7","modified":1587036002300},{"_id":"themes/ayer/source/google441c42e6b4b19747.html","hash":"4d49e478280b2e928e2c5ba3aca77b72843882e8","modified":1582703450037},{"_id":"themes/ayer/source-src/main.js","hash":"77e753cd66579316c8c9e07e673c31ddb6e8e7f2","modified":1587522295434},{"_id":"source/_posts/.DS_Store","hash":"cb726d9dadaea65c0cc300e69d3bf2c6dae94bd9","modified":1584539183190},{"_id":"source/_posts/analysize-SRT-protocol-live-stream-with-wireshark.md","hash":"43bc367f071bb8f294f39a4b30f6d970923b273c","modified":1586423054371},{"_id":"source/_posts/Matplotlib-s-backends-and-non-interactive-backends-for-rendering.md","hash":"542686dafbe898d3b1807d167470317f46c72159","modified":1588141507132},{"_id":"source/_posts/compile-libyuv-for-Android.md","hash":"823fd76b602667f185c099b518aef1abb2a6fd11","modified":1585633204165},{"_id":"source/_posts/digital-video-concept.md","hash":"3ada9ec69ad3bcd15b1a5272d8429c1b214b5103","modified":1585560658261},{"_id":"source/_posts/ffmpeg-compiled-in-macOS-Catalina-runs-crash.md","hash":"1abd21e04685587e537b55288ab806534a919409","modified":1586329018487},{"_id":"source/_posts/handle-the-bug-of-hexo-asset-image-plugin.md","hash":"6029690f18cdb32b1ce195147f14ec101d0fbd60","modified":1580908779931},{"_id":"source/_posts/how-to-calculate-the-MS-SSIM.md","hash":"29e05189ad5ed3921084073ba843440a48d07ae9","modified":1585560298431},{"_id":"source/_posts/how-to-calculate-the-SSIM-in-FFMpeg.md","hash":"b0db3ea4bbb578917efa8acdb424fe65b052c2eb","modified":1585560627786},{"_id":"source/_posts/introduction-to-image-pyramid.md","hash":"1d576817d588503cc53580c484b6c41111525125","modified":1584880333199},{"_id":"source/_posts/the-proof-of-the-SSIM-in-FFMpeg.md","hash":"b8f8804b615f8f01acfc5fb3223d853804127d1c","modified":1581993161736},{"_id":"source/_posts/thinking-about-QA-my-years-for-work-as-a-QA.md","hash":"2f5f10345ff3398d7771f75d629c81f782e22194","modified":1580886058278},{"_id":"source/_posts/use-hexo-and-github-for-blog.md","hash":"04058a1ef2c0d51212704a4d3a7cd935b7a50e31","modified":1582960794069},{"_id":"source/_posts/where-of-what-is-past-is-prologue.md","hash":"1cdd07de08159c75bd2c1d4464e69114611dfa4f","modified":1581045920604},{"_id":"source/aboutme/index.md","hash":"dc1d11cf2e29ea368b43b3dec15f2ca467aa542d","modified":1586432773295},{"_id":"source/categories/index.md","hash":"e0f2e695c7f124550e1155313bce4885bbedcc76","modified":1580894736311},{"_id":"source/tags/index.md","hash":"4307d7e0a1661258e8a8ae03b1bc916b16a02c7b","modified":1580894736312},{"_id":"source/_posts/Introduction-to-digital-video-technology.md","hash":"b43bb0a4e4441974b15e7e2e536f414f147489e3","modified":1585559815380},{"_id":"themes/ayer/layout/_partial/after-footer.ejs","hash":"73b5cb5d6e1411ff7ac161f2a15a2e288931e869","modified":1588070052180},{"_id":"themes/ayer/layout/_partial/archive-post.ejs","hash":"9be7173badcca6582c1136204adb3aa432aada21","modified":1588070052181},{"_id":"themes/ayer/layout/_partial/archive.ejs","hash":"a7065fad7532c5f727e8d0e18771616fea662944","modified":1588070052182},{"_id":"themes/ayer/layout/_partial/ayer.ejs","hash":"ca75120c1b9f9c3d3bd88287c70258a0eb081f83","modified":1588070052184},{"_id":"themes/ayer/layout/_partial/article.ejs","hash":"4556d3a4bfcd8cba596adc11e7097d3d5be1de55","modified":1588070052183},{"_id":"themes/ayer/layout/_partial/footer.ejs","hash":"a8dcd61156d35cb8b270870b2e65d56c60c4dd38","modified":1588070052185},{"_id":"themes/ayer/layout/_partial/google-analytics.ejs","hash":"5b389110f4d3b727c33c48f088a8bc7f8b577d19","modified":1588070052186},{"_id":"themes/ayer/layout/_partial/katex.ejs","hash":"ff2822fd2f6e8a02ce781aecf5bb031db076e882","modified":1588070052187},{"_id":"themes/ayer/layout/_partial/head.ejs","hash":"26dceb25434ee7bf8f6ce01b58daafe2a482ab53","modified":1588070052186},{"_id":"themes/ayer/layout/_partial/baidu-analytics.ejs","hash":"f0e6e88f9f7eb08b8fe51449a8a3016273507924","modified":1588070052184},{"_id":"themes/ayer/layout/_partial/mathjax.ejs","hash":"4d633c6cc375a0c81fe0ed2a114b1ac4c3cec2d9","modified":1588070052188},{"_id":"themes/ayer/layout/_partial/music.ejs","hash":"cde5caf73f120b1300ec7539bbf675597688c734","modified":1588070052190},{"_id":"themes/ayer/layout/_partial/modal.ejs","hash":"cf441365eff1f8143e3c9ae54954d0ebe0a358e5","modified":1588070052189},{"_id":"themes/ayer/scripts/filters/index.js","hash":"e435b782178da75656f9616c7af564fb9ff0de50","modified":1587522234402},{"_id":"themes/ayer/scripts/helpers/ayer-plus-vendors.js","hash":"7b3df58faaa875d17afbf74ddef5601116f2ce3d","modified":1587522234407},{"_id":"themes/ayer/scripts/filters/meta_generator.js","hash":"58f4c93d22e4eb9743915223444335fb6fe06d21","modified":1587522234403},{"_id":"themes/ayer/scripts/helpers/wordcount.js","hash":"f9c8fceb2130929b8f11e22b1c3476c99d1574a8","modified":1587522234406},{"_id":"themes/ayer/layout/_partial/sidebar.ejs","hash":"e3b411aef6e5f25f6c5c66b93d653ef70da8d4af","modified":1588070052203},{"_id":"themes/ayer/scripts/lib/core.js","hash":"7908a82a86d63f35c6f785cacb1cfaf99ae7e0f9","modified":1587522234408},{"_id":"themes/ayer/layout/_partial/totop.ejs","hash":"43ba0cd1f9e898bcbd873b1e9a2d47249d6aaf77","modified":1588070052204},{"_id":"themes/ayer/source/css/clipboard.styl","hash":"017f9b628806eeb199da2e77485cd9ac8e2117b3","modified":1587036002296},{"_id":"themes/ayer/layout/_partial/viewer.ejs","hash":"6663c30882e2b375b15c147080c70d0dcce54a6f","modified":1588070052204},{"_id":"themes/ayer/source/css/custom.styl","hash":"19c274cd268e72af427a5937deaf43296b64ec9f","modified":1587036002294},{"_id":"themes/ayer/source/dist/main.css","hash":"3db86b8f33531dba19614251a886becfcbdb7972","modified":1588070092004},{"_id":"themes/ayer/source/dist/main.js","hash":"eac86de3ed854247e50b04d6908816b1837e08cf","modified":1588070092004},{"_id":"themes/ayer/source/images/404.jpg","hash":"4f36a8d378712427cded03f5166949f5e0ba754c","modified":1587036002302},{"_id":"themes/ayer/source/images/ayer.png","hash":"0466c05244273f645d239cd27513bfa3c50308aa","modified":1587036002306},{"_id":"themes/ayer/source/images/ayer-side.svg","hash":"ad004ce7a873de0f91774f3d5923e010396a07bd","modified":1587036002303},{"_id":"themes/ayer/source/images/.DS_Store","hash":"f11e216b10abd812337aa78690f9fac63fff4c86","modified":1585725762741},{"_id":"themes/ayer/source/images/ayer.svg","hash":"379c3307f97c364718a1dbc1e52fb14de12eb11a","modified":1587036002332},{"_id":"themes/ayer/source/images/forkme.png","hash":"99c3e21a169421e4f249befb428396c729863a75","modified":1587036002337},{"_id":"themes/ayer/source/images/cover7.jpg","hash":"573bff6899d2d9c5bcba0dc9c60cd1ec9eb8b029","modified":1587036002325},{"_id":"themes/ayer/source/images/sponsor.jpg","hash":"b3efa167f50cad85404c83f21dec2be570ed21dc","modified":1587036002301},{"_id":"themes/ayer/source/js/busuanzi-2.3.pure.min.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1587036002346},{"_id":"themes/ayer/source/js/clickLove.js","hash":"a35dfb7ff19796c005ee30b55fd95e39d6d59a89","modified":1587036002345},{"_id":"themes/ayer/source/js/tocbot.min.js","hash":"bae97e8a24a05a99335f8e725641c8ca9c50502a","modified":1587036002342},{"_id":"themes/ayer/source/js/lazyload.min.js","hash":"b801b3946fb9b72e03512c0663458e140e1fa77b","modified":1587036002344},{"_id":"themes/ayer/source/js/search.js","hash":"118be0e0918532ac1225f62e1a0a6f0673e0b173","modified":1587036002345},{"_id":"themes/ayer/source-src/css/_mixins.styl","hash":"6959409df2dd0a1ca05be0c0e9b2a884efdfb82d","modified":1587522295371},{"_id":"themes/ayer/source-src/css/_extend.styl","hash":"4980e6333c9e28408b65fcd9d89e1cddfa4508dc","modified":1587522295432},{"_id":"themes/ayer/source-src/css/_darkmode.styl","hash":"ddb61f23b078101ae8ecad32972155147585eb8c","modified":1587522295368},{"_id":"themes/ayer/source-src/css/_normalize.styl","hash":"630e719b9e805a26182a37152435d4787c0f1734","modified":1587522295370},{"_id":"themes/ayer/source-src/css/_variables.styl","hash":"910dd0682971a616e1610daec23e7ff5a556df98","modified":1587522295369},{"_id":"themes/ayer/source-src/css/style.styl","hash":"f9ae046355867c3e87c4aef309d1fc50aa493e0d","modified":1587522295432},{"_id":"themes/ayer/source-src/js/share.js","hash":"a65e4645818e4eb8d4a40d0499e1d04b6ee224bd","modified":1588070116816},{"_id":"themes/ayer/source-src/js/ayer.js","hash":"531f881d513e5be1119670db5334d3e2e6bf0194","modified":1588070116815},{"_id":"themes/ayer/source-src/css/_remixicon.styl","hash":"138284a4842820a7f2bf38c26d3d5c5b0e2219e9","modified":1587522295369},{"_id":"source/_posts/Introduction-to-digital-video-technology/PAR.png","hash":"c70e69dc198ec3839ef3733009871c4878376bf4","modified":1582851545042},{"_id":"source/_posts/Introduction-to-digital-video-technology/H.264_block_diagram_with_quality_score.jpg","hash":"3967abd6bc5c1d712229fde39d8b04e15aa30f2b","modified":1582851545074},{"_id":"source/_posts/Introduction-to-digital-video-technology/DAR.png","hash":"3e6dbaf9603920192e6f4e4166b71a7ea41b4fce","modified":1582851545065},{"_id":"source/_posts/Introduction-to-digital-video-technology/adaptive_streaming.png","hash":"0a538e6289d9a999f23921984dac5d06907d0b9c","modified":1582851545062},{"_id":"source/_posts/Introduction-to-digital-video-technology/arithimetic_range.png","hash":"6fbb622dbade48a18f5be4da09c21b654daf40bf","modified":1582851545004},{"_id":"source/_posts/Introduction-to-digital-video-technology/applicat.jpg","hash":"c9586003d8d0aa8b1a3bbacf94e5fcc83052cfd9","modified":1582851545025},{"_id":"source/_posts/Introduction-to-digital-video-technology/codec_history_timeline.png","hash":"c3f9b31c0bc3d74f7bb6fcea99cdda6eb9f0416f","modified":1582851545000},{"_id":"source/_posts/Introduction-to-digital-video-technology/dct_coefficient_image.png","hash":"759c7f0a7967d03372327e637fc13835841969c2","modified":1582851545044},{"_id":"source/_posts/Introduction-to-digital-video-technology/chroma_subsampling_examples.jpg","hash":"8f80a735370ad4c089747c13b2de3afb70cbc88d","modified":1582851545028},{"_id":"source/_posts/Introduction-to-digital-video-technology/dct_basis.png","hash":"79574d6b73bcdf6800f15ebb7f6194ab4486d2e5","modified":1582851545053},{"_id":"source/_posts/Introduction-to-digital-video-technology/comparison_delta_vs_motion_estimation.png","hash":"743f28976298f2a2d5fe53d656029799b7ad4c86","modified":1582851545059},{"_id":"source/_posts/Introduction-to-digital-video-technology/dct_coefficient_zeroed.png","hash":"0fa16d99d840777d8d82630a03d0410c1d7b6aac","modified":1582851545060},{"_id":"source/_posts/Introduction-to-digital-video-technology/dct_coefficient_values.png","hash":"12bfbc207a3e65e3eaab9b24c4d2492581390fca","modified":1582851545002},{"_id":"source/_posts/Introduction-to-digital-video-technology/drm.png","hash":"ca04b55cf66ff0266eaf1f0ba9946e50c3126de8","modified":1582851545033},{"_id":"source/_posts/Introduction-to-digital-video-technology/dctfrequ.jpg","hash":"323574aa5d932b997e47307087d05c4bae18d6ce","modified":1582851545070},{"_id":"source/_posts/Introduction-to-digital-video-technology/difference_frames.png","hash":"8cd2a73205a5150a41dfa4c10d0453b389ce00a1","modified":1582851545069},{"_id":"source/_posts/Introduction-to-digital-video-technology/eyes.jpg","hash":"31f674344d4de9760f05bbc0dbb3d97fdd7bee40","modified":1582851545068},{"_id":"source/_posts/Introduction-to-digital-video-technology/general_architecture.png","hash":"d286cfd4cd5f0879ff5c6dd6a1444e0d9688b067","modified":1582851545071},{"_id":"source/_posts/Introduction-to-digital-video-technology/frame_types.png","hash":"ca9e44729981dc90350522310046f6b6877a058b","modified":1582851545005},{"_id":"source/_posts/Introduction-to-digital-video-technology/gray_image.png","hash":"829439d9bbbab14c5aede4eb458dbd747d3d7f54","modified":1582851545023},{"_id":"source/_posts/Introduction-to-digital-video-technology/h264_bitstream_macro_diagram.png","hash":"c85786d4ed6998920e0324d53766420cc0c80e3c","modified":1582851545044},{"_id":"source/_posts/Introduction-to-digital-video-technology/image_3d_matrix_rgb.png","hash":"d5046d5897becc2bd67ba800d52c8d2c169a8827","modified":1582851545031},{"_id":"source/_posts/Introduction-to-digital-video-technology/h264_intra_predictions.png","hash":"0b2b59a570c7b78e89cee9178e01e8b81a71a26b","modified":1582851544996},{"_id":"source/_posts/Introduction-to-digital-video-technology/luminance_vs_color.png","hash":"374ba02bbc65847b75404480eb3b3b33f0d6dd02","modified":1582851545071},{"_id":"source/_posts/Introduction-to-digital-video-technology/jpeg_quantization_table.png","hash":"f6bc13d2e38c759d86e6aea19ca79f21d808be49","modified":1582851545063},{"_id":"source/_posts/Introduction-to-digital-video-technology/kernel_convolution.jpg","hash":"d4738259da8f98a64103db35dd929567d8c56cc7","modified":1582851545053},{"_id":"source/_posts/Introduction-to-digital-video-technology/minimal.png","hash":"d12660c567bbf861f680e46edeb213e8b1d61202","modified":1582851545070},{"_id":"source/_posts/Introduction-to-digital-video-technology/mediainfo_details_1.png","hash":"d66a66c45e16fe57d11d37c206aaacc25f2adfb0","modified":1582851545036},{"_id":"source/_posts/Introduction-to-digital-video-technology/minimal_yuv420_bin.png","hash":"a43ddd4ca136adfb1187274804bfe1aa5c673776","modified":1582851544998},{"_id":"source/_posts/Introduction-to-digital-video-technology/minimal_yuv420_hex.png","hash":"df61b32017844a26e43365632bf4d7c6902dd879","modified":1582851545065},{"_id":"source/_posts/Introduction-to-digital-video-technology/mediainfo_details_2.png","hash":"c1b08d567ed8af2e4f792b9014a60509b5f96ce9","modified":1582851545026},{"_id":"source/_posts/Introduction-to-digital-video-technology/nal_units.png","hash":"36f5941829dd5e16aff37aa12475497c8a436e6e","modified":1582851545063},{"_id":"source/_posts/Introduction-to-digital-video-technology/motion_estimation.png","hash":"6733715226f45172f1229531155aae5c72270199","modified":1582851545055},{"_id":"source/_posts/Introduction-to-digital-video-technology/minimal_yuv420_hex_string.png","hash":"415c9605f5c5e4b0daa069f3e3c865dd10a21968","modified":1582851545056},{"_id":"source/_posts/Introduction-to-digital-video-technology/nes-color-palette.png","hash":"61e45f08a6e876d95a2a31dcbdd9e30f63d94255","modified":1582851545067},{"_id":"source/_posts/Introduction-to-digital-video-technology/original_frames.png","hash":"b2a66ad2340b7882d544d635710a5f1d5bb9960c","modified":1582851545026},{"_id":"source/_posts/Introduction-to-digital-video-technology/original_frames_motion_estimation.png","hash":"f3431bd6393801153e0a7d927c814e1f74efc75d","modified":1582851545066},{"_id":"source/_posts/Introduction-to-digital-video-technology/original_vs_quantized.png","hash":"3103989178d4629076da51e79c6c6b4984a5ee25","modified":1582851545049},{"_id":"source/_posts/Introduction-to-digital-video-technology/pixel_matrice.png","hash":"1d38579358def020ef49af0ae5c74e5c44b83456","modified":1582851545051},{"_id":"source/_posts/Introduction-to-digital-video-technology/picture_partitioning.png","hash":"88ffdc679b2a42c6e7f8606f174e26c7ad73b5e6","modified":1582851545064},{"_id":"source/_posts/Introduction-to-digital-video-technology/progressive_download.png","hash":"c73a91a8862321979b705c51b87a35d124037e41","modified":1582851545072},{"_id":"source/_posts/Introduction-to-digital-video-technology/range.png","hash":"ab5441e9cd2fa2498fd393e66fc407295921a1eb","modified":1582851545067},{"_id":"source/_posts/Introduction-to-digital-video-technology/quantize.png","hash":"e1c7283c23436e7f2765585b10b1976dfd60a0cb","modified":1582851545006},{"_id":"source/_posts/Introduction-to-digital-video-technology/re-quantize.png","hash":"caa1fb8e5fb0ea8e855f2178ca54b46bade356b4","modified":1582851545003},{"_id":"source/_posts/Introduction-to-digital-video-technology/range_show.png","hash":"741709b132a51ee1f661f7944f71f5a6b88f596f","modified":1582851545039},{"_id":"source/_posts/Introduction-to-digital-video-technology/resolution.png","hash":"967825f91f47079229579dc26ffa056a51d00539","modified":1582851545054},{"_id":"source/_posts/Introduction-to-digital-video-technology/repetition_in_time.png","hash":"cbae59485c7d35e34348588a5a96700fa69b650b","modified":1582851545040},{"_id":"source/_posts/Introduction-to-digital-video-technology/second_subrange.png","hash":"ba2ad2798da6ea952c64d39006ed9bfebc11b808","modified":1582851545014},{"_id":"source/_posts/Introduction-to-digital-video-technology/repetitions_in_space.png","hash":"cb54eecc040b86413c51dcd063f0c7f1ca03b494","modified":1582851545068},{"_id":"source/_posts/Introduction-to-digital-video-technology/slice_nal_idr_bin.png","hash":"82e19fd4025aaa1c33ba7001f2858ede5e7cf5fa","modified":1582851544995},{"_id":"source/_posts/Introduction-to-digital-video-technology/slice_header.png","hash":"c9b5aae2f792bb37eea778cac1bfe0cac9157577","modified":1582851544997},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_background.png","hash":"73a9455bbf9dd322369932d9942b292fca6c0d9b","modified":1582851545019},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_background_ball_2.png","hash":"ab72447c3277e5c7bdf8dfeadd35baa75e456416","modified":1582851545058},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_background_ball_1.png","hash":"89be621f40ed5462fc8cbced436dde40fac00445","modified":1582851545062},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_background_ball_3.png","hash":"e114d32c4ca716b37ac4ff57e06d3054aee5c09d","modified":1582851545057},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_background_ball_2_diff.png","hash":"ea51fa687ae1f10dd4d80d642f6888003fba9967","modified":1582851545045},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_background_ball_4.png","hash":"25bb55f7c91003f820015efcf89c5c9b489b378b","modified":1582851545047},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_bg.png","hash":"f68548bfec81428d2cd9cd639797f8a1154d42c7","modified":1582851545001},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_bg_prediction.png","hash":"3cb2d48ab72207f48bc6adde832239459765df66","modified":1582851545069},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_bg_block.png","hash":"edd0e7cfc866811d641f391f0c004ac2ff1ca4e6","modified":1582851545033},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_residual.png","hash":"3b2fcec030d91d76033340a145c3416eeac89ee4","modified":1582851545061},{"_id":"source/_posts/Introduction-to-digital-video-technology/solid_background.png","hash":"4915697969371ff7fd763684b3034a6bb0b22055","modified":1582851545009},{"_id":"source/_posts/Introduction-to-digital-video-technology/solid_background_1.png","hash":"4915697969371ff7fd763684b3034a6bb0b22055","modified":1582851545049},{"_id":"source/_posts/Introduction-to-digital-video-technology/solid_background_3.png","hash":"4915697969371ff7fd763684b3034a6bb0b22055","modified":1582851545048},{"_id":"source/_posts/Introduction-to-digital-video-technology/solid_background_2.png","hash":"4915697969371ff7fd763684b3034a6bb0b22055","modified":1582851545046},{"_id":"source/_posts/Introduction-to-digital-video-technology/solid_background_ball_1.png","hash":"e528a2d79f0db4026f4283cddec0b274fc5e9b5b","modified":1582851545054},{"_id":"source/_posts/Introduction-to-digital-video-technology/solid_background_4.png","hash":"4915697969371ff7fd763684b3034a6bb0b22055","modified":1582851545057},{"_id":"source/_posts/Introduction-to-digital-video-technology/solid_background_ball_2.png","hash":"526949c9ad33f66de5e0f6f76fe60d10a4089534","modified":1582851545048},{"_id":"source/_posts/Introduction-to-digital-video-technology/super_mario_head.png","hash":"77be19b64b991529b264b3db7e8d12d6f02f9f05","modified":1582851545052},{"_id":"source/_posts/Introduction-to-digital-video-technology/super_mario_world.png","hash":"8b527d98f56a629de2df806a5c4672438c668d82","modified":1582851545039},{"_id":"source/_posts/Introduction-to-digital-video-technology/super_mario_head.jpg","hash":"dca3d9c22a35ef65fd866063f1eb1263a4811457","modified":1582851545050},{"_id":"source/_posts/Introduction-to-digital-video-technology/thor_codec_block_diagram.png","hash":"0c763f00a42adb88c79fe3a9923c447dd7a72e6a","modified":1582851545030},{"_id":"source/_posts/Introduction-to-digital-video-technology/token_protection.png","hash":"3ef9e9355cc9f8a2e6ba8d79393ce5ef1071ee2b","modified":1582851545029},{"_id":"source/_posts/Introduction-to-digital-video-technology/video.png","hash":"3e16bc901961acd938292d32231dd983c49387f9","modified":1582851545060},{"_id":"source/_posts/Introduction-to-digital-video-technology/vbr.png","hash":"445dec1c0287fa1db386297b535d413309b853bf","modified":1582851545056},{"_id":"source/_posts/Introduction-to-digital-video-technology/ycbcr_420_merge.png","hash":"d138dbdf8bfd3babbf3004f22d2af5c7d974953f","modified":1582851545074},{"_id":"source/_posts/Introduction-to-digital-video-technology/z_char_8x8.png","hash":"d6063b7999a10e00028130f40aec3025f7fa1648","modified":1582851545042},{"_id":"source/_posts/Introduction-to-digital-video-technology/ycbcr_subsampling_resolution.png","hash":"cd3f42a1a7564b30a0b2fac823cb3463a2b85e11","modified":1582851545038},{"_id":"source/_posts/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/1.png","hash":"a80a86773f19344f4f14db7f53e926cb90530ad1","modified":1588071793189},{"_id":"source/_posts/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/3.jpg","hash":"398689e0cfb25dc9e8e791199c9513d7ceeb6639","modified":1588072717105},{"_id":"source/_posts/compile-libyuv-for-Android/1.jpg","hash":"8299203f204adab5b5ff7d70374affd9be80c46a","modified":1585629705143},{"_id":"source/_posts/ffmpeg-compiled-in-macOS-Catalina-runs-crash/2.jpg","hash":"c7788bcbfabe7eaa08452114aa17c40f026db977","modified":1586327145941},{"_id":"source/_posts/how-to-calculate-the-MS-SSIM/test_msssim.cpp","hash":"7694c9543e918a9a1c51bc17a9d8414e46c0d16d","modified":1585560089886},{"_id":"source/_posts/how-to-calculate-the-SSIM-in-FFMpeg/3.jpg","hash":"d93f46f5955f3aec8c7552d8722e899d81e54c0e","modified":1581823949388},{"_id":"source/_posts/introduction-to-image-pyramid/.DS_Store","hash":"1803fe3b937939e889d7e9deb0a8b8d462c027dc","modified":1584539183200},{"_id":"source/_posts/introduction-to-image-pyramid/apple.jpg","hash":"a9f2c40030d8ddbd136aa544af6b92deec785237","modified":1584589059765},{"_id":"source/_posts/introduction-to-image-pyramid/orange.jpg","hash":"ca78aa4ba0c8dec15e16cac0713f387cee3760dd","modified":1584589059766},{"_id":"source/_posts/thinking-about-QA-my-years-for-work-as-a-QA/4.jpg","hash":"479ce99609fba3a54e7deb327f4e63aa107a1da6","modified":1580878395109},{"_id":"source/_posts/thinking-about-QA-my-years-for-work-as-a-QA/5.jpg","hash":"0cb84d2a2ffa4468e1b60245072408edd4bfda0b","modified":1580878471969},{"_id":"source/_posts/use-hexo-and-github-for-blog/1.jpg","hash":"bbc9d73ff51283fc2c72f0c5ad3b6289e53cc127","modified":1580904910724},{"_id":"source/_posts/use-hexo-and-github-for-blog/3.jpg","hash":"0de3ba5c10fd5d590f7754150e4722a2cb2caaa0","modified":1580905451085},{"_id":"source/_posts/where-of-what-is-past-is-prologue/2.jpg","hash":"e8687566ed849f2b2396925329cff398a8bdf4a6","modified":1581045734397},{"_id":"themes/ayer/screenshots/hexo-theme-ayer.png","hash":"4111670e622ce09837b6b9cc641782af75805079","modified":1586609983697},{"_id":"themes/ayer/source/images/alipay.jpg","hash":"8f5409e29764fca573f1d274003910aa3c919de1","modified":1587036002317},{"_id":"themes/ayer/source/images/cover2.jpg","hash":"f61dd08c95327468c5f6bc5175eff68d00f05b46","modified":1587036002341},{"_id":"themes/ayer/source/images/wechat.jpg","hash":"93a362574a8498e75dca469b7bceb0b321fda387","modified":1587036002336},{"_id":"themes/ayer/source/js/jquery-2.0.3.min.js","hash":"800edb7787c30f4982bf38f2cb8f4f6fb61340e9","modified":1587036002343},{"_id":"source/_posts/Introduction-to-digital-video-technology/drm_decoder_flow.jpeg","hash":"d87b37c56a865d003f4518348ef4c8bd0cefffc5","modified":1582851545043},{"_id":"source/_posts/Introduction-to-digital-video-technology/interlaced_vs_progressive.png","hash":"263b22737aee3ca5ef237483c1a1ea559e2beef3","modified":1582851545010},{"_id":"source/_posts/Introduction-to-digital-video-technology/new_pixel_geometry.jpg","hash":"f2dd319c1be29d425def1155342e4cb739e2801e","modified":1582851544996},{"_id":"source/_posts/Introduction-to-digital-video-technology/paritions_view_intel_video_pro_analyzer.png","hash":"220cf827dcbf6a50e5638dce3efcc6791c5a2010","modified":1582851545075},{"_id":"source/_posts/Introduction-to-digital-video-technology/real_world_motion_compensation.png","hash":"4c561ecca6437e116fe5be4c9acee24dcb17ca88","modified":1582851545041},{"_id":"source/_posts/Introduction-to-digital-video-technology/rgb_channels_intensity.png","hash":"5bb27ab974e41e6183e14d31464c955f2d4a4629","modified":1582851545037},{"_id":"source/_posts/Introduction-to-digital-video-technology/ycbcr.png","hash":"c433af6bb0a99a137e96a5c2e571a55c4219569a","modified":1582851545072},{"_id":"source/_posts/Introduction-to-digital-video-technology/yuv_histogram.png","hash":"26f1cf761872a9d0cb9daebcd2accbaf2b5d877a","modified":1582851545073},{"_id":"source/_posts/analysize-SRT-protocol-live-stream-with-wireshark/1.png","hash":"4c73d790314f16422a88751f059ae1004bb29628","modified":1586415111550},{"_id":"source/_posts/analysize-SRT-protocol-live-stream-with-wireshark/3.jpg","hash":"afbae05a6a3e4ef72fb66124b5a0a0d92fb975aa","modified":1586418609133},{"_id":"source/_posts/compile-libyuv-for-Android/2.png","hash":"8045a119da321b14843d8a469acffff22da3e511","modified":1585629920515},{"_id":"source/_posts/ffmpeg-compiled-in-macOS-Catalina-runs-crash/1.jpg","hash":"3d2506cb29fedab642a010307ce3bceb1984cd2c","modified":1586309425053},{"_id":"source/_posts/how-to-calculate-the-MS-SSIM/1.jpg","hash":"0dd41bada2063671f8e0ec2d10c84520c97c2706","modified":1582616193906},{"_id":"source/_posts/ffmpeg-compiled-in-macOS-Catalina-runs-crash/4.jpg","hash":"d7ae1c86a3f8379c1b48c397781ff2eb3c4d2ff1","modified":1586328660310},{"_id":"source/_posts/introduction-to-image-pyramid/6.jpg","hash":"89321083d322bf7c1066c45b49be052b7f859d6a","modified":1584538748445},{"_id":"source/_posts/introduction-to-image-pyramid/7.jpg","hash":"9758790ead0cb8c74c251e9ee8c47d31b98b52d2","modified":1584538748446},{"_id":"source/_posts/thinking-about-QA-my-years-for-work-as-a-QA/1.jpg","hash":"85557498ad4dd75c93d818e99b79643e91881d52","modified":1580876715179},{"_id":"source/_posts/thinking-about-QA-my-years-for-work-as-a-QA/3.jpg","hash":"c055cf4f4a7e682a89364e4da1a5e1cbb11d08e8","modified":1580878245675},{"_id":"source/_posts/use-hexo-and-github-for-blog/2.jpg","hash":"08c05a019f978e2f7749152381f28005061b7d95","modified":1580904881766},{"_id":"themes/ayer/layout/_partial/post/category.ejs","hash":"85f0ebeceee1c32623bfa1e4170dbe1e34442fea","modified":1588070052192},{"_id":"themes/ayer/layout/_partial/post/gallery.ejs","hash":"5f8487fe7bed9a09001c6655244ff35f583cf1eb","modified":1588070052194},{"_id":"themes/ayer/layout/_partial/post/clipboard.ejs","hash":"d6f82b35bbb060ec22c5facf0eea67cf44c396f6","modified":1588070052193},{"_id":"themes/ayer/layout/_partial/post/albums.ejs","hash":"cfb16c9dda7a609776a28702e0c3854a52e422a7","modified":1588070052199},{"_id":"themes/ayer/layout/_partial/post/gitalk.ejs","hash":"917ef5daea9c926aea4703ab3e9911c9f6d16b64","modified":1588070052191},{"_id":"themes/ayer/layout/_partial/post/justifiedGallery.ejs","hash":"4a21fd3d7335ffcd0661036fee81a927c125e2e2","modified":1588070052198},{"_id":"themes/ayer/layout/_partial/post/search.ejs","hash":"2c9d19d1685e834aa2020998da2a2d259ce9b9ff","modified":1588070052197},{"_id":"themes/ayer/layout/_partial/post/nav.ejs","hash":"e59198918e92ef92156aeefbf6023584ac1cae64","modified":1588070052194},{"_id":"themes/ayer/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1588070052197},{"_id":"themes/ayer/layout/_partial/post/minivaline.ejs","hash":"4c0558f3f27c1f6d9daeed13d31eb8678d3a551e","modified":1588070052196},{"_id":"themes/ayer/layout/_partial/post/title.ejs","hash":"3b076a65b9847cd6e0d424f5c2874046ef51d4d9","modified":1588070052193},{"_id":"themes/ayer/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1588070052196},{"_id":"themes/ayer/layout/_partial/post/share.ejs","hash":"0a364766931f48df60b7c92fec8fde1067a93e00","modified":1588070052198},{"_id":"themes/ayer/layout/_partial/post/topping.ejs","hash":"fea793e132f627a8148059a9aee8bc33550506d1","modified":1588070052192},{"_id":"themes/ayer/layout/_partial/post/valine.ejs","hash":"6250f3c87a4bf3d437291128c7e60608cf807e10","modified":1588070052195},{"_id":"themes/ayer/layout/_partial/post/busuanzi.ejs","hash":"4a6b6300284876a2008f2b13067d2c77cd41e26e","modified":1588070052199},{"_id":"themes/ayer/layout/_partial/post/tocbot.ejs","hash":"9898b0dd9237e21908ba40292a8a9f947bed44d2","modified":1588070052195},{"_id":"themes/ayer/layout/_partial/post/word.ejs","hash":"4b8e8455709debd73fba6bf3aad63378c4156dc1","modified":1588070052201},{"_id":"themes/ayer/source/images/cover6.jpg","hash":"a5b8a5dddff2607fee5fccf5fdef3b214a8468cc","modified":1587036002321},{"_id":"themes/ayer/source-src/css/_partial/apple.styl","hash":"e06dce604cc58ec39d677e4e59910c2725684901","modified":1587522295417},{"_id":"themes/ayer/source-src/css/_partial/archive.styl","hash":"3e61c25e6ae9a25196c6d904731cf6821c950341","modified":1587522295417},{"_id":"themes/ayer/source-src/css/_partial/ayer.styl","hash":"da46e69f7ecc779ec137deb601c3524524fd758d","modified":1587522295409},{"_id":"themes/ayer/source-src/css/_partial/articles.styl","hash":"39a0bc6c5cf85f0527d6ee81f6feebce8550c1dd","modified":1587522295412},{"_id":"themes/ayer/source-src/css/_partial/albums.styl","hash":"0659d5f7469f24a415354ff767d949926465d515","modified":1587522295428},{"_id":"themes/ayer/source-src/css/_partial/article.styl","hash":"5ed306bbd1dbec2f70fb08711a3212566ae637ac","modified":1587522295379},{"_id":"themes/ayer/source-src/css/_partial/gallery.styl","hash":"7bdc2c9fb4971dbd7511c5cbb69bd611f20db591","modified":1587522295422},{"_id":"themes/ayer/source-src/css/_partial/float.styl","hash":"d888df89a172e4c8119cb8740fc1eae1a9539157","modified":1587522295407},{"_id":"themes/ayer/source-src/css/_partial/categories.styl","hash":"67f4824419c497e54469094f239978dd888a9706","modified":1587522295418},{"_id":"themes/ayer/source-src/css/_partial/footer.styl","hash":"1a4576c38ef19834a4d0a8ac887e7b55d21f2f13","modified":1587522295414},{"_id":"themes/ayer/source-src/css/_partial/gitalk.styl","hash":"3706eef2e0541493f1679a30241d279e29dfdc17","modified":1587522295408},{"_id":"themes/ayer/source-src/css/_partial/layout.styl","hash":"b1bcdd213115c2fa8331d4e0f620dc7bcad64dc5","modified":1587522295373},{"_id":"themes/ayer/source-src/css/_partial/justifiedGallery.styl","hash":"f2f43ae9831c3df017b35c68caa94f5eb911f697","modified":1587522295402},{"_id":"themes/ayer/source-src/css/_partial/mobile.styl","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1587522295425},{"_id":"themes/ayer/source-src/css/_partial/highlight.styl","hash":"e894b3a1aff76e84137e1e6db59a75afd0a53f52","modified":1587522295411},{"_id":"themes/ayer/source-src/css/_partial/reward.styl","hash":"9d06ee41617f5979fa378c6ebfacf394e58f9628","modified":1587522295415},{"_id":"themes/ayer/source-src/css/_partial/search.styl","hash":"f81076fc6d4a83be3371bcb79ab54d88e2605714","modified":1587522295413},{"_id":"themes/ayer/source-src/css/_partial/navbar.styl","hash":"bc477e4905ab88da711e0049ac3866dd1f88f80a","modified":1587522295410},{"_id":"themes/ayer/source-src/css/_partial/pace.styl","hash":"bde7bf3577c66a3ac829410d52fa2144212282ef","modified":1587522295413},{"_id":"themes/ayer/source-src/css/_partial/tag.styl","hash":"87df748185edfba222c3f9a4b9ef8eee0e8e5ad3","modified":1587522295372},{"_id":"themes/ayer/source-src/css/_partial/share.styl","hash":"9ce2dd3ffdc4e170dfcb975a7158f913bc40b8d8","modified":1587522295416},{"_id":"themes/ayer/source-src/css/_partial/sidebar.styl","hash":"600c70f1de82da5223af290d47a583f9c379d188","modified":1587522295415},{"_id":"themes/ayer/source-src/css/_partial/lists.styl","hash":"da8a82a48852411c10e279dfee43038d46f4f273","modified":1587522295406},{"_id":"themes/ayer/source-src/css/_partial/totop.styl","hash":"9e9d8dc167ed2c332e4520cbf41244c34c1ebf64","modified":1587522295431},{"_id":"themes/ayer/source-src/css/_partial/tocbot.styl","hash":"b4b6f02d4028ad602cf860d1c530d2e38bd4be75","modified":1587522295419},{"_id":"source/_posts/Introduction-to-digital-video-technology/av1_browser_analyzer.png","hash":"29ebdec75ff886024a02ae0cbe0db5e159ae8019","modified":1582851545008},{"_id":"source/_posts/Introduction-to-digital-video-technology/drm_general_flow.jpeg","hash":"a8d75e72a0ca070f6253cb79382c10016471a8d2","modified":1582851545034},{"_id":"source/_posts/Introduction-to-digital-video-technology/intra_prediction_intel_video_pro_analyzer.png","hash":"e028144adbcec28cd4652aaeff0b8e507a2c33e4","modified":1582851545051},{"_id":"source/_posts/Introduction-to-digital-video-technology/macro_blocks_ffmpeg.png","hash":"1f2ea7469a74654c9e6026acca3c3416b48cf64c","modified":1582851544999},{"_id":"source/_posts/Introduction-to-digital-video-technology/motion_vectors_ffmpeg.png","hash":"e044216494293ca01c5d271f9a7e4ce6b113a9be","modified":1582851545007},{"_id":"source/_posts/Introduction-to-digital-video-technology/s/cut_smaller_video.sh","hash":"7209314953eff6e66bb19ad60da2d010ca5bce21","modified":1582851547950},{"_id":"source/_posts/Introduction-to-digital-video-technology/s/download_video.sh","hash":"302121cfa6140dc1f8dcb610d6f202dd7e80c023","modified":1582851547951},{"_id":"source/_posts/Introduction-to-digital-video-technology/s/mediainfo","hash":"c129cedd045017d1ef1a746b9eb0d646ab73fa88","modified":1582851547949},{"_id":"source/_posts/Introduction-to-digital-video-technology/s/vmaf","hash":"d87321346812c2f9e05c330f5c6929b09ba3673c","modified":1582851547950},{"_id":"source/_posts/Introduction-to-digital-video-technology/s/ffmpeg","hash":"c326e5668d1d5617f856fa22ff6cd6e5344e6d5a","modified":1582851547949},{"_id":"source/_posts/Introduction-to-digital-video-technology/s/start_jupyter.sh","hash":"fb21df5b45ae6d18bf3daa87e02688f58784408c","modified":1582851547952},{"_id":"source/_posts/compile-libyuv-for-Android/3.jpg","hash":"0c21fd454fb7d2379a5dce491e0babc0bb0465a7","modified":1585630721425},{"_id":"source/_posts/ffmpeg-compiled-in-macOS-Catalina-runs-crash/3.jpg","hash":"1a847ec7d650f298d9045bdb05dd5db4a8862972","modified":1586327667093},{"_id":"source/_posts/how-to-calculate-the-SSIM-in-FFMpeg/1.jpg","hash":"3148dd3acaea6b6992f64817c2895c6911a60085","modified":1581757432625},{"_id":"source/_posts/Introduction-to-digital-video-technology/intel-video-pro-analyzer.png","hash":"09c6044b14c236636e0ff04940de018f9bf8ebb5","modified":1582851545059},{"_id":"source/_posts/Introduction-to-digital-video-technology/py/dct_experiences.ipynb","hash":"a4d7f5029b42288b82c4c792aa338c339871be03","modified":1582855982034},{"_id":"source/_posts/Introduction-to-digital-video-technology/py/frame_difference_vs_motion_estimation_plus_residual.ipynb","hash":"a591d77d49b429d685b4e285f8d6e9d0f37863b8","modified":1582855982048},{"_id":"source/_posts/Introduction-to-digital-video-technology/py/uniform_quantization_experience.ipynb","hash":"7a5b656e299dfe3136397797983bd87bb1c3cc0c","modified":1582855982065},{"_id":"source/_posts/analysize-SRT-protocol-live-stream-with-wireshark/4.jpg","hash":"f9b4620e1001d82df170cc58fc9bbb2f43ee1fcb","modified":1586419572305},{"_id":"source/_posts/introduction-to-image-pyramid/3.jpg","hash":"a90f77b2f3a3e31739631742ebc36d4cf39b5bff","modified":1584538748443},{"_id":"themes/ayer/source/images/cover1.jpg","hash":"e019fbed6158ae3e4ec3d255b41bddc1afcbfa90","modified":1587036002338},{"_id":"themes/ayer/source/images/cover3.jpg","hash":"7b4e7c9ce19ce5d5c1588b2567e88ecbb04370af","modified":1587036002340},{"_id":"source/_posts/Introduction-to-digital-video-technology/avc_vs_hevc.png","hash":"aa36e2760c140a115588746027decb4b734a6163","modified":1582851545013},{"_id":"source/_posts/Introduction-to-digital-video-technology/inter_prediction_intel_video_pro_analyzer.png","hash":"b8f6e61a887c1a17fde767e059ac423e3a78208d","modified":1582851545011},{"_id":"source/_posts/how-to-calculate-the-SSIM-in-FFMpeg/2.jpg","hash":"f4dd94bdef42546d9dc851a146e9ff1031643eff","modified":1581757446339},{"_id":"source/_posts/thinking-about-QA-my-years-for-work-as-a-QA/6.jpg","hash":"8dda9488742ca6b97604b9e10b54baa47c7360b2","modified":1580878551198},{"_id":"source/_posts/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/2.jpg","hash":"08797bfe9f1d6737d2624c213c5f28eca8239b85","modified":1588071478637},{"_id":"source/_posts/introduction-to-image-pyramid/4.jpg","hash":"e0234aaf3742a0c367e52b26c90edbf04bdeef8e","modified":1584538748444},{"_id":"source/_posts/introduction-to-image-pyramid/1.jpg","hash":"fd9dee12a9bd39cb4e6c4c2e9a197f6b9db56bab","modified":1584538748441},{"_id":"source/_posts/introduction-to-image-pyramid/5.jpg","hash":"774099ad65e984fc7d13fa79a06582469976a6d6","modified":1584538748445},{"_id":"source/_posts/introduction-to-image-pyramid/8.jpg","hash":"017c9cc2065909c4d26533bebcaf2eb27ec36420","modified":1584579275661},{"_id":"source/_posts/where-of-what-is-past-is-prologue/1.jpg","hash":"388d22f269bfbcada00b3b19db722d0eee9265db","modified":1580886534461},{"_id":"source/_posts/Introduction-to-digital-video-technology/py/dct_better_explained.ipynb","hash":"a907fb857413f0dc9324eb37cbd0735c886618ba","modified":1582855982019},{"_id":"source/_posts/Introduction-to-digital-video-technology/py/filters_are_easy.ipynb","hash":"8a6ff77a39d505b258961acd2e15ca0b678d595c","modified":1582855982035},{"_id":"themes/ayer/source/images/cover4.jpg","hash":"77040e609d2f1c60e00a8e47cb95b25bf7ff5b9c","modified":1587036002335},{"_id":"source/_posts/analysize-SRT-protocol-live-stream-with-wireshark/2.jpg","hash":"91762a890442e64060a120ee9e1c5d8ebd616549","modified":1586417761586},{"_id":"source/_posts/thinking-about-QA-my-years-for-work-as-a-QA/2.png","hash":"170f3da8439e0b4ff869a72343adc69f11212ff7","modified":1580878157789},{"_id":"source/_posts/analysize-SRT-protocol-live-stream-with-wireshark/5.jpg","hash":"459096e4e8ed55fd3f90113081ba60c49a003590","modified":1586419503143},{"_id":"source/_posts/Introduction-to-digital-video-technology/py/image_as_3d_array.ipynb","hash":"54c66aa8b872d84e603bff4993cd698fd07fe639","modified":1582855982057},{"_id":"source/_posts/introduction-to-image-pyramid/2.jpg","hash":"613b10d2cc3d1c86a439ff265e002913d3dd6b93","modified":1584538748442},{"_id":"themes/ayer/source/images/cover5.jpg","hash":"1a2d6182d94b015a45fe4bf8e8a239dfcc0ef776","modified":1587036002334},{"_id":"source/_posts/introduction-to-image-pyramid/image_pyramid_blend.ipynb","hash":"d4247df5e28ff57b36e925e422c7c9082c0b79d1","modified":1584579002989},{"_id":"source/_posts/Introduction-to-digital-video-technology/py/image_transform_frequency_domain.ipynb","hash":"1612bfaf0bcfce785034892722e0138dc5bd0f47","modified":1582855982062},{"_id":"source/_posts/Introduction-to-digital-video-technology/v/small_bunny_1080p_30fps.mp4","hash":"db719d7f38cfb505446620ec6076436956ffe065","modified":1582851550348},{"_id":"source/_posts/Introduction-to-digital-video-technology/v/small_bunny_1080p_60fps.mp4","hash":"5a0ff37edce6f468a0e4a00bc68039df91b97441","modified":1582851550352},{"_id":"public/search.xml","hash":"fbcf8ef0493e41c1e0d8961eef4c0117bf7da814","modified":1588141546150},{"_id":"public/sitemap.xml","hash":"225e87253cb0ad21ebb97747f32db4a9502e74c7","modified":1588141546150},{"_id":"public/aboutme/index.html","hash":"09b6223422f8da120e703c96e4e5738f1a710da8","modified":1588141546150},{"_id":"public/categories/index.html","hash":"f58117bc015b3061df1c337e70e1c61af306eb27","modified":1588141546150},{"_id":"public/tags/index.html","hash":"8d1df8c4bcf4e58295b3510c4b60ce41a01f9d37","modified":1588141546150},{"_id":"public/archives/page/2/index.html","hash":"2f4bb51ecdd18579333dd429c73a80cf8100e6d4","modified":1588141546150},{"_id":"public/archives/2020/page/2/index.html","hash":"e0f6c72b49ddd46efb503bf48686b29222777cc5","modified":1588141546150},{"_id":"public/archives/2020/02/index.html","hash":"0f1ac88fe1726dc8dbfb41e6cb3b101f48e5d2b8","modified":1588141546150},{"_id":"public/archives/2020/03/index.html","hash":"8fe7da7aac1f1ac6ce2adf7fc66d0c8dfc9e9d50","modified":1588141546150},{"_id":"public/archives/2020/04/index.html","hash":"5934c0ab385d2c5b5933ba3771d601920caee129","modified":1588141546150},{"_id":"public/tags/Wireshark/index.html","hash":"43b5798ae30703ef4d4f5a5e9de60557c456eb5a","modified":1588141546150},{"_id":"public/tags/SRT/index.html","hash":"5b7b1b55fb3f03e83860c1aed4229cc31a520b89","modified":1588141546150},{"_id":"public/tags/网络协议分析/index.html","hash":"bf2e54764335d7d6c479e15cb05af08ee46fe23e","modified":1588141546150},{"_id":"public/tags/python/index.html","hash":"6e92c2effc830f6ff3ae41871f819f7a79fe52be","modified":1588141546150},{"_id":"public/tags/matplotlib/index.html","hash":"e3ac04c139c64d5a797d1d68486a5dcc33af9f0d","modified":1588141546150},{"_id":"public/tags/non-interactive-rendering/index.html","hash":"1869b49e791e7b70c6b231d64ac6ac9ab97c68f9","modified":1588141546150},{"_id":"public/tags/libyuv/index.html","hash":"084fa5b8a33db1bb2c5c3025b6281ca20245cc83","modified":1588141546150},{"_id":"public/tags/FFMpeg/index.html","hash":"266b27521c0559afc27222e071cf6d2788f5b45b","modified":1588141546150},{"_id":"public/tags/编译/index.html","hash":"51c625fecf3bd1bdfd0d5ae6e8a8dc21a0177983","modified":1588141546150},{"_id":"public/tags/macOS-Catalina/index.html","hash":"32f60aa8d0273661f3fe6376519004cbe99de9ec","modified":1588141546150},{"_id":"public/tags/数字视频/index.html","hash":"581a3b5bdcce0ede88995c2bfe7c02f53aee318c","modified":1588141546150},{"_id":"public/tags/digital-video/index.html","hash":"441594082181e64b74b496d9518d2990949253ab","modified":1588141546150},{"_id":"public/tags/视频基础知识/index.html","hash":"dea324826dc5516114652e56ce0c93600bf0f34a","modified":1588141546150},{"_id":"public/tags/hexo/index.html","hash":"dc4db47e5c2fb2ef2ff93610f8572ae0cb957d7e","modified":1588141546150},{"_id":"public/tags/SSIM/index.html","hash":"655891e48f459d326ec3903055af206a772db0cf","modified":1588141546150},{"_id":"public/tags/MS-SSIM/index.html","hash":"564f22362ec315b2d5820eac04f0b8903840996f","modified":1588141546150},{"_id":"public/tags/图像金字塔/index.html","hash":"e7d8d1134b3e249ed24e36c4d0afa797f4b4f823","modified":1588141546150},{"_id":"public/tags/高斯金字塔/index.html","hash":"3ca5f50e7261dcf2f2e14de3bde973b84d3f972d","modified":1588141546150},{"_id":"public/tags/拉普拉斯金字塔/index.html","hash":"8b782a258dac15351f30dc90104a86ae0dfab2f8","modified":1588141546150},{"_id":"public/tags/image-pyramid/index.html","hash":"8cb7b940c25662963179ef3b1de13446d689a392","modified":1588141546150},{"_id":"public/tags/测试工作/index.html","hash":"cc9f6a8cd539dae36484071b7a29882179150c14","modified":1588141546150},{"_id":"public/tags/github/index.html","hash":"8c292c0f386e7ef86a07c3e6f12daef3f011278d","modified":1588141546150},{"_id":"public/tags/建站/index.html","hash":"afc20c9a628cefeb569bc61d4e697e700bbe0762","modified":1588141546150},{"_id":"public/categories/视频技术/index.html","hash":"c39f3f27a1ba1bb55a666a33a3317ee623ef8482","modified":1588141546150},{"_id":"public/categories/matplotlib/index.html","hash":"6844cc85176111d04efa6bdd5b7a16637f55d840","modified":1588141546150},{"_id":"public/categories/Android/index.html","hash":"59a8f03736ce4139ff7e7313e3bdffe598ebcf2d","modified":1588141546150},{"_id":"public/categories/技术/index.html","hash":"e749cfcbf24300e1adc972dbe3b5c32000e014df","modified":1588141546150},{"_id":"public/categories/视觉技术/index.html","hash":"8a8d91e6ae99f1ea24ac1e07a9a2032aff227575","modified":1588141546150},{"_id":"public/categories/总结/index.html","hash":"d6d86d80774f7d6e3c030770f6989cc04ddc73c2","modified":1588141546150},{"_id":"public/2020/04/28/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/index.html","hash":"75293b27e4028d0b6503d515e78648020919c5ca","modified":1588141546150},{"_id":"public/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/index.html","hash":"6a56cef2e4b8d248e03f615a862a75482dd91b7e","modified":1588141546150},{"_id":"public/2020/04/08/ffmpeg-compiled-in-macOS-Catalina-runs-crash/index.html","hash":"c2ec412e1feb06b0d82d0f3df6fe894ce2938554","modified":1588141546150},{"_id":"public/2020/03/30/compile-libyuv-for-Android/index.html","hash":"3c9aef1dff200c8d1886863dc7a01aea63c631f9","modified":1588141546150},{"_id":"public/2020/03/18/introduction-to-image-pyramid/index.html","hash":"d739e65ed0bfbe06018855a356816edb2474f1a9","modified":1588141546150},{"_id":"public/2020/03/03/digital-video-concept/index.html","hash":"28fbc10ebc1e7a5a2d716697919739c6558271fc","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/index.html","hash":"e81d0a6cba8129fa89b653c3941ade1dd2bd8277","modified":1588141546150},{"_id":"public/2020/02/25/how-to-calculate-the-MS-SSIM/index.html","hash":"83f9d49f72dd6a4cb0d3e5b596c2bca413a16a4f","modified":1588141546150},{"_id":"public/2020/02/18/the-proof-of-the-SSIM-in-FFMpeg/index.html","hash":"b1a335336f97d7050564afa3ea62729e67e59c00","modified":1588141546150},{"_id":"public/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/index.html","hash":"e8e6043cc6831c7b36ce86eddc6b282ead1e26b9","modified":1588141546150},{"_id":"public/2020/02/05/handle-the-bug-of-hexo-asset-image-plugin/index.html","hash":"c436cecc3cf6e4619865cd54d81db17894c0d0fe","modified":1588141546150},{"_id":"public/2020/02/05/use-hexo-and-github-for-blog/index.html","hash":"6f8edb412a74ac1f302d4778e097c95f0c9b403f","modified":1588141546150},{"_id":"public/2020/02/05/where-of-what-is-past-is-prologue/index.html","hash":"df5b4e8135e47edb1d8737a239f9a814d477b51a","modified":1588141546150},{"_id":"public/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/index.html","hash":"d1ac4f8bd50d96e0f48a6ba5ccd5fd0999dcfc88","modified":1588141546150},{"_id":"public/archives/index.html","hash":"e516af9cd1be5641fbb201f563bb58cb0e1b7b61","modified":1588141546150},{"_id":"public/archives/2020/index.html","hash":"4fb7aa48d0ea2d4110348644e62415604aa991af","modified":1588141546150},{"_id":"public/index.html","hash":"97b02192a0a711ea5d75ec6c78ee7b12f7eb4521","modified":1588141546150},{"_id":"public/page/2/index.html","hash":"1d75c12e988ba856d07faa820d9fec8e3377076e","modified":1588141546150},{"_id":"public/favicon.ico","hash":"473ba682e828a7e34f24fae320e77b6bed4260c7","modified":1588141546150},{"_id":"public/images/404.jpg","hash":"4f36a8d378712427cded03f5166949f5e0ba754c","modified":1588141546150},{"_id":"public/images/ayer.png","hash":"0466c05244273f645d239cd27513bfa3c50308aa","modified":1588141546150},{"_id":"public/images/ayer-side.svg","hash":"ad004ce7a873de0f91774f3d5923e010396a07bd","modified":1588141546150},{"_id":"public/images/ayer.svg","hash":"379c3307f97c364718a1dbc1e52fb14de12eb11a","modified":1588141546150},{"_id":"public/images/forkme.png","hash":"99c3e21a169421e4f249befb428396c729863a75","modified":1588141546150},{"_id":"public/images/cover7.jpg","hash":"573bff6899d2d9c5bcba0dc9c60cd1ec9eb8b029","modified":1588141546150},{"_id":"public/images/sponsor.jpg","hash":"b3efa167f50cad85404c83f21dec2be570ed21dc","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/py/frame_difference_vs_motion_estimation_plus_residual.ipynb","hash":"a591d77d49b429d685b4e285f8d6e9d0f37863b8","modified":1588141546150},{"_id":"public/2020/02/25/how-to-calculate-the-MS-SSIM/test_msssim.cpp","hash":"7694c9543e918a9a1c51bc17a9d8414e46c0d16d","modified":1588141546150},{"_id":"public/2020/02/05/where-of-what-is-past-is-prologue/2.jpg","hash":"e8687566ed849f2b2396925329cff398a8bdf4a6","modified":1588141546150},{"_id":"public/2020/04/28/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/1.png","hash":"a80a86773f19344f4f14db7f53e926cb90530ad1","modified":1588141546150},{"_id":"public/2020/04/28/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/3.jpg","hash":"398689e0cfb25dc9e8e791199c9513d7ceeb6639","modified":1588141546150},{"_id":"public/2020/03/30/compile-libyuv-for-Android/1.jpg","hash":"8299203f204adab5b5ff7d70374affd9be80c46a","modified":1588141546150},{"_id":"public/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/3.jpg","hash":"d93f46f5955f3aec8c7552d8722e899d81e54c0e","modified":1588141546150},{"_id":"public/2020/02/05/use-hexo-and-github-for-blog/1.jpg","hash":"bbc9d73ff51283fc2c72f0c5ad3b6289e53cc127","modified":1588141546150},{"_id":"public/2020/02/05/use-hexo-and-github-for-blog/3.jpg","hash":"0de3ba5c10fd5d590f7754150e4722a2cb2caaa0","modified":1588141546150},{"_id":"public/2020/04/08/ffmpeg-compiled-in-macOS-Catalina-runs-crash/2.jpg","hash":"c7788bcbfabe7eaa08452114aa17c40f026db977","modified":1588141546150},{"_id":"public/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/4.jpg","hash":"479ce99609fba3a54e7deb327f4e63aa107a1da6","modified":1588141546150},{"_id":"public/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/5.jpg","hash":"0cb84d2a2ffa4468e1b60245072408edd4bfda0b","modified":1588141546150},{"_id":"public/2020/03/18/introduction-to-image-pyramid/apple.jpg","hash":"a9f2c40030d8ddbd136aa544af6b92deec785237","modified":1588141546150},{"_id":"public/2020/03/18/introduction-to-image-pyramid/orange.jpg","hash":"ca78aa4ba0c8dec15e16cac0713f387cee3760dd","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/H.264_block_diagram_with_quality_score.jpg","hash":"3967abd6bc5c1d712229fde39d8b04e15aa30f2b","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/PAR.png","hash":"c70e69dc198ec3839ef3733009871c4878376bf4","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/adaptive_streaming.png","hash":"0a538e6289d9a999f23921984dac5d06907d0b9c","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/arithimetic_range.png","hash":"6fbb622dbade48a18f5be4da09c21b654daf40bf","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/applicat.jpg","hash":"c9586003d8d0aa8b1a3bbacf94e5fcc83052cfd9","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/codec_history_timeline.png","hash":"c3f9b31c0bc3d74f7bb6fcea99cdda6eb9f0416f","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/chroma_subsampling_examples.jpg","hash":"8f80a735370ad4c089747c13b2de3afb70cbc88d","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/comparison_delta_vs_motion_estimation.png","hash":"743f28976298f2a2d5fe53d656029799b7ad4c86","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/dct_basis.png","hash":"79574d6b73bcdf6800f15ebb7f6194ab4486d2e5","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/dct_coefficient_image.png","hash":"759c7f0a7967d03372327e637fc13835841969c2","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/dct_coefficient_zeroed.png","hash":"0fa16d99d840777d8d82630a03d0410c1d7b6aac","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/difference_frames.png","hash":"8cd2a73205a5150a41dfa4c10d0453b389ce00a1","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/DAR.png","hash":"3e6dbaf9603920192e6f4e4166b71a7ea41b4fce","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/dct_coefficient_values.png","hash":"12bfbc207a3e65e3eaab9b24c4d2492581390fca","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/drm.png","hash":"ca04b55cf66ff0266eaf1f0ba9946e50c3126de8","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/dctfrequ.jpg","hash":"323574aa5d932b997e47307087d05c4bae18d6ce","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/eyes.jpg","hash":"31f674344d4de9760f05bbc0dbb3d97fdd7bee40","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/general_architecture.png","hash":"d286cfd4cd5f0879ff5c6dd6a1444e0d9688b067","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/gray_image.png","hash":"829439d9bbbab14c5aede4eb458dbd747d3d7f54","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/h264_bitstream_macro_diagram.png","hash":"c85786d4ed6998920e0324d53766420cc0c80e3c","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/image_3d_matrix_rgb.png","hash":"d5046d5897becc2bd67ba800d52c8d2c169a8827","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/h264_intra_predictions.png","hash":"0b2b59a570c7b78e89cee9178e01e8b81a71a26b","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/jpeg_quantization_table.png","hash":"f6bc13d2e38c759d86e6aea19ca79f21d808be49","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/luminance_vs_color.png","hash":"374ba02bbc65847b75404480eb3b3b33f0d6dd02","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/mediainfo_details_1.png","hash":"d66a66c45e16fe57d11d37c206aaacc25f2adfb0","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/kernel_convolution.jpg","hash":"d4738259da8f98a64103db35dd929567d8c56cc7","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/frame_types.png","hash":"ca9e44729981dc90350522310046f6b6877a058b","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/minimal.png","hash":"d12660c567bbf861f680e46edeb213e8b1d61202","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/minimal_yuv420_bin.png","hash":"a43ddd4ca136adfb1187274804bfe1aa5c673776","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/mediainfo_details_2.png","hash":"c1b08d567ed8af2e4f792b9014a60509b5f96ce9","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/minimal_yuv420_hex.png","hash":"df61b32017844a26e43365632bf4d7c6902dd879","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/minimal_yuv420_hex_string.png","hash":"415c9605f5c5e4b0daa069f3e3c865dd10a21968","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/motion_estimation.png","hash":"6733715226f45172f1229531155aae5c72270199","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/nal_units.png","hash":"36f5941829dd5e16aff37aa12475497c8a436e6e","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/nes-color-palette.png","hash":"61e45f08a6e876d95a2a31dcbdd9e30f63d94255","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/original_frames.png","hash":"b2a66ad2340b7882d544d635710a5f1d5bb9960c","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/original_frames_motion_estimation.png","hash":"f3431bd6393801153e0a7d927c814e1f74efc75d","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/original_vs_quantized.png","hash":"3103989178d4629076da51e79c6c6b4984a5ee25","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/picture_partitioning.png","hash":"88ffdc679b2a42c6e7f8606f174e26c7ad73b5e6","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/pixel_matrice.png","hash":"1d38579358def020ef49af0ae5c74e5c44b83456","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/progressive_download.png","hash":"c73a91a8862321979b705c51b87a35d124037e41","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/quantize.png","hash":"e1c7283c23436e7f2765585b10b1976dfd60a0cb","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/range.png","hash":"ab5441e9cd2fa2498fd393e66fc407295921a1eb","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/range_show.png","hash":"741709b132a51ee1f661f7944f71f5a6b88f596f","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/re-quantize.png","hash":"caa1fb8e5fb0ea8e855f2178ca54b46bade356b4","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/repetition_in_time.png","hash":"cbae59485c7d35e34348588a5a96700fa69b650b","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/resolution.png","hash":"967825f91f47079229579dc26ffa056a51d00539","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/s/cut_smaller_video.sh","hash":"7209314953eff6e66bb19ad60da2d010ca5bce21","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/repetitions_in_space.png","hash":"cb54eecc040b86413c51dcd063f0c7f1ca03b494","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/s/download_video.sh","hash":"302121cfa6140dc1f8dcb610d6f202dd7e80c023","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/s/ffmpeg","hash":"c326e5668d1d5617f856fa22ff6cd6e5344e6d5a","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/s/mediainfo","hash":"c129cedd045017d1ef1a746b9eb0d646ab73fa88","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/s/start_jupyter.sh","hash":"fb21df5b45ae6d18bf3daa87e02688f58784408c","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/s/vmaf","hash":"d87321346812c2f9e05c330f5c6929b09ba3673c","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/second_subrange.png","hash":"ba2ad2798da6ea952c64d39006ed9bfebc11b808","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/slice_header.png","hash":"c9b5aae2f792bb37eea778cac1bfe0cac9157577","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/slice_nal_idr_bin.png","hash":"82e19fd4025aaa1c33ba7001f2858ede5e7cf5fa","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/smw_background.png","hash":"73a9455bbf9dd322369932d9942b292fca6c0d9b","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_1.png","hash":"89be621f40ed5462fc8cbced436dde40fac00445","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_2.png","hash":"ab72447c3277e5c7bdf8dfeadd35baa75e456416","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_2_diff.png","hash":"ea51fa687ae1f10dd4d80d642f6888003fba9967","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_3.png","hash":"e114d32c4ca716b37ac4ff57e06d3054aee5c09d","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_4.png","hash":"25bb55f7c91003f820015efcf89c5c9b489b378b","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/smw_bg.png","hash":"f68548bfec81428d2cd9cd639797f8a1154d42c7","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/smw_bg_block.png","hash":"edd0e7cfc866811d641f391f0c004ac2ff1ca4e6","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/smw_bg_prediction.png","hash":"3cb2d48ab72207f48bc6adde832239459765df66","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/smw_residual.png","hash":"3b2fcec030d91d76033340a145c3416eeac89ee4","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/solid_background.png","hash":"4915697969371ff7fd763684b3034a6bb0b22055","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/solid_background_1.png","hash":"4915697969371ff7fd763684b3034a6bb0b22055","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/solid_background_3.png","hash":"4915697969371ff7fd763684b3034a6bb0b22055","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/solid_background_ball_1.png","hash":"e528a2d79f0db4026f4283cddec0b274fc5e9b5b","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/solid_background_ball_2.png","hash":"526949c9ad33f66de5e0f6f76fe60d10a4089534","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/super_mario_head.jpg","hash":"dca3d9c22a35ef65fd866063f1eb1263a4811457","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/super_mario_head.png","hash":"77be19b64b991529b264b3db7e8d12d6f02f9f05","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/solid_background_2.png","hash":"4915697969371ff7fd763684b3034a6bb0b22055","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/solid_background_4.png","hash":"4915697969371ff7fd763684b3034a6bb0b22055","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/super_mario_world.png","hash":"8b527d98f56a629de2df806a5c4672438c668d82","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/thor_codec_block_diagram.png","hash":"0c763f00a42adb88c79fe3a9923c447dd7a72e6a","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/token_protection.png","hash":"3ef9e9355cc9f8a2e6ba8d79393ce5ef1071ee2b","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/vbr.png","hash":"445dec1c0287fa1db386297b535d413309b853bf","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/video.png","hash":"3e16bc901961acd938292d32231dd983c49387f9","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/ycbcr_420_merge.png","hash":"d138dbdf8bfd3babbf3004f22d2af5c7d974953f","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/ycbcr_subsampling_resolution.png","hash":"cd3f42a1a7564b30a0b2fac823cb3463a2b85e11","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/z_char_8x8.png","hash":"d6063b7999a10e00028130f40aec3025f7fa1648","modified":1588141546150},{"_id":"public/images/alipay.jpg","hash":"8f5409e29764fca573f1d274003910aa3c919de1","modified":1588141546150},{"_id":"public/images/cover2.jpg","hash":"f61dd08c95327468c5f6bc5175eff68d00f05b46","modified":1588141546150},{"_id":"public/images/wechat.jpg","hash":"93a362574a8498e75dca469b7bceb0b321fda387","modified":1588141546150},{"_id":"public/2020/04/08/ffmpeg-compiled-in-macOS-Catalina-runs-crash/3.jpg","hash":"1a847ec7d650f298d9045bdb05dd5db4a8862972","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/py/dct_experiences.ipynb","hash":"a4d7f5029b42288b82c4c792aa338c339871be03","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/py/uniform_quantization_experience.ipynb","hash":"7a5b656e299dfe3136397797983bd87bb1c3cc0c","modified":1588141546150},{"_id":"public/2020/02/25/how-to-calculate-the-MS-SSIM/1.jpg","hash":"0dd41bada2063671f8e0ec2d10c84520c97c2706","modified":1588141546150},{"_id":"public/2020/03/30/compile-libyuv-for-Android/2.png","hash":"8045a119da321b14843d8a469acffff22da3e511","modified":1588141546150},{"_id":"public/2020/02/05/use-hexo-and-github-for-blog/2.jpg","hash":"08c05a019f978e2f7749152381f28005061b7d95","modified":1588141546150},{"_id":"public/2020/04/08/ffmpeg-compiled-in-macOS-Catalina-runs-crash/1.jpg","hash":"3d2506cb29fedab642a010307ce3bceb1984cd2c","modified":1588141546150},{"_id":"public/2020/04/08/ffmpeg-compiled-in-macOS-Catalina-runs-crash/4.jpg","hash":"d7ae1c86a3f8379c1b48c397781ff2eb3c4d2ff1","modified":1588141546150},{"_id":"public/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/1.png","hash":"4c73d790314f16422a88751f059ae1004bb29628","modified":1588141546150},{"_id":"public/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/3.jpg","hash":"afbae05a6a3e4ef72fb66124b5a0a0d92fb975aa","modified":1588141546150},{"_id":"public/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/1.jpg","hash":"85557498ad4dd75c93d818e99b79643e91881d52","modified":1588141546150},{"_id":"public/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/3.jpg","hash":"c055cf4f4a7e682a89364e4da1a5e1cbb11d08e8","modified":1588141546150},{"_id":"public/2020/03/18/introduction-to-image-pyramid/6.jpg","hash":"89321083d322bf7c1066c45b49be052b7f859d6a","modified":1588141546150},{"_id":"public/2020/03/18/introduction-to-image-pyramid/7.jpg","hash":"9758790ead0cb8c74c251e9ee8c47d31b98b52d2","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/drm_decoder_flow.jpeg","hash":"d87b37c56a865d003f4518348ef4c8bd0cefffc5","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/interlaced_vs_progressive.png","hash":"263b22737aee3ca5ef237483c1a1ea559e2beef3","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/new_pixel_geometry.jpg","hash":"f2dd319c1be29d425def1155342e4cb739e2801e","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/paritions_view_intel_video_pro_analyzer.png","hash":"220cf827dcbf6a50e5638dce3efcc6791c5a2010","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/real_world_motion_compensation.png","hash":"4c561ecca6437e116fe5be4c9acee24dcb17ca88","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/rgb_channels_intensity.png","hash":"5bb27ab974e41e6183e14d31464c955f2d4a4629","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/ycbcr.png","hash":"c433af6bb0a99a137e96a5c2e571a55c4219569a","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/yuv_histogram.png","hash":"26f1cf761872a9d0cb9daebcd2accbaf2b5d877a","modified":1588141546150},{"_id":"public/css/custom.css","hash":"78b13bf5b98278ff65cdb0a731f2a1c98b2324a5","modified":1588141546150},{"_id":"public/css/clipboard.css","hash":"7990b92ffeda1b06b94b50140d9c95dac21bd418","modified":1588141546150},{"_id":"public/dist/main.js","hash":"eac86de3ed854247e50b04d6908816b1837e08cf","modified":1588141546150},{"_id":"public/js/busuanzi-2.3.pure.min.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1588141546150},{"_id":"public/js/clickLove.js","hash":"9e8e79d69ad8338761272f86fe5cad0ad5e503cc","modified":1588141546150},{"_id":"public/js/tocbot.min.js","hash":"bae97e8a24a05a99335f8e725641c8ca9c50502a","modified":1588141546150},{"_id":"public/js/search.js","hash":"118be0e0918532ac1225f62e1a0a6f0673e0b173","modified":1588141546150},{"_id":"public/js/lazyload.min.js","hash":"b801b3946fb9b72e03512c0663458e140e1fa77b","modified":1588141546150},{"_id":"public/404.html","hash":"b81cb3d508c7bb24213583810167199a64883814","modified":1588141546150},{"_id":"public/google441c42e6b4b19747.html","hash":"4d49e478280b2e928e2c5ba3aca77b72843882e8","modified":1588141546150},{"_id":"public/dist/main.css","hash":"3db86b8f33531dba19614251a886becfcbdb7972","modified":1588141546150},{"_id":"public/js/jquery-2.0.3.min.js","hash":"800edb7787c30f4982bf38f2cb8f4f6fb61340e9","modified":1588141546150},{"_id":"public/images/cover6.jpg","hash":"a5b8a5dddff2607fee5fccf5fdef3b214a8468cc","modified":1588141546150},{"_id":"public/2020/03/30/compile-libyuv-for-Android/3.jpg","hash":"0c21fd454fb7d2379a5dce491e0babc0bb0465a7","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/inter_prediction_intel_video_pro_analyzer.png","hash":"b8f6e61a887c1a17fde767e059ac423e3a78208d","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/py/dct_better_explained.ipynb","hash":"a907fb857413f0dc9324eb37cbd0735c886618ba","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/av1_browser_analyzer.png","hash":"29ebdec75ff886024a02ae0cbe0db5e159ae8019","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/drm_general_flow.jpeg","hash":"a8d75e72a0ca070f6253cb79382c10016471a8d2","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/intra_prediction_intel_video_pro_analyzer.png","hash":"e028144adbcec28cd4652aaeff0b8e507a2c33e4","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/macro_blocks_ffmpeg.png","hash":"1f2ea7469a74654c9e6026acca3c3416b48cf64c","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/motion_vectors_ffmpeg.png","hash":"e044216494293ca01c5d271f9a7e4ce6b113a9be","modified":1588141546150},{"_id":"public/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/4.jpg","hash":"f9b4620e1001d82df170cc58fc9bbb2f43ee1fcb","modified":1588141546150},{"_id":"public/2020/03/18/introduction-to-image-pyramid/3.jpg","hash":"a90f77b2f3a3e31739631742ebc36d4cf39b5bff","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/avc_vs_hevc.png","hash":"aa36e2760c140a115588746027decb4b734a6163","modified":1588141546150},{"_id":"public/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/2.jpg","hash":"f4dd94bdef42546d9dc851a146e9ff1031643eff","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/py/filters_are_easy.ipynb","hash":"8a6ff77a39d505b258961acd2e15ca0b678d595c","modified":1588141546150},{"_id":"public/images/cover3.jpg","hash":"7b4e7c9ce19ce5d5c1588b2567e88ecbb04370af","modified":1588141546150},{"_id":"public/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/1.jpg","hash":"3148dd3acaea6b6992f64817c2895c6911a60085","modified":1588141546150},{"_id":"public/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/6.jpg","hash":"8dda9488742ca6b97604b9e10b54baa47c7360b2","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/intel-video-pro-analyzer.png","hash":"09c6044b14c236636e0ff04940de018f9bf8ebb5","modified":1588141546150},{"_id":"public/2020/04/28/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/2.jpg","hash":"08797bfe9f1d6737d2624c213c5f28eca8239b85","modified":1588141546150},{"_id":"public/images/cover1.jpg","hash":"e019fbed6158ae3e4ec3d255b41bddc1afcbfa90","modified":1588141546150},{"_id":"public/2020/03/18/introduction-to-image-pyramid/4.jpg","hash":"e0234aaf3742a0c367e52b26c90edbf04bdeef8e","modified":1588141546150},{"_id":"public/2020/03/18/introduction-to-image-pyramid/5.jpg","hash":"774099ad65e984fc7d13fa79a06582469976a6d6","modified":1588141546150},{"_id":"public/2020/03/18/introduction-to-image-pyramid/8.jpg","hash":"017c9cc2065909c4d26533bebcaf2eb27ec36420","modified":1588141546150},{"_id":"public/2020/03/18/introduction-to-image-pyramid/1.jpg","hash":"fd9dee12a9bd39cb4e6c4c2e9a197f6b9db56bab","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/py/image_as_3d_array.ipynb","hash":"54c66aa8b872d84e603bff4993cd698fd07fe639","modified":1588141546150},{"_id":"public/images/cover4.jpg","hash":"77040e609d2f1c60e00a8e47cb95b25bf7ff5b9c","modified":1588141546150},{"_id":"public/2020/02/05/where-of-what-is-past-is-prologue/1.jpg","hash":"388d22f269bfbcada00b3b19db722d0eee9265db","modified":1588141546150},{"_id":"public/2020/03/18/introduction-to-image-pyramid/2.jpg","hash":"613b10d2cc3d1c86a439ff265e002913d3dd6b93","modified":1588141546150},{"_id":"public/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/2.jpg","hash":"91762a890442e64060a120ee9e1c5d8ebd616549","modified":1588141546150},{"_id":"public/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/2.png","hash":"170f3da8439e0b4ff869a72343adc69f11212ff7","modified":1588141546150},{"_id":"public/images/cover5.jpg","hash":"1a2d6182d94b015a45fe4bf8e8a239dfcc0ef776","modified":1588141546150},{"_id":"public/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/5.jpg","hash":"459096e4e8ed55fd3f90113081ba60c49a003590","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/py/image_transform_frequency_domain.ipynb","hash":"1612bfaf0bcfce785034892722e0138dc5bd0f47","modified":1588141546150},{"_id":"public/2020/03/18/introduction-to-image-pyramid/image_pyramid_blend.ipynb","hash":"d4247df5e28ff57b36e925e422c7c9082c0b79d1","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/v/small_bunny_1080p_30fps.mp4","hash":"db719d7f38cfb505446620ec6076436956ffe065","modified":1588141546150},{"_id":"public/2020/02/28/Introduction-to-digital-video-technology/v/small_bunny_1080p_60fps.mp4","hash":"5a0ff37edce6f468a0e4a00bc68039df91b97441","modified":1588141546150}],"Category":[{"name":"视频技术","_id":"ck9kyh06s00041pdb1h5f23cr"},{"name":"matplotlib","_id":"ck9kyh06z00091pdb59w28ahl"},{"name":"Android","_id":"ck9kyh074000e1pdbht4t64md"},{"name":"技术","_id":"ck9kyh077000k1pdb7fcharb6"},{"name":"视觉技术","_id":"ck9kyh07e000s1pdb6ytl8cot"},{"name":"总结","_id":"ck9kyh07j00101pdb7yw5f8el"}],"Data":[],"Page":[{"title":"About me","reward":false,"date":"2020-02-06T11:07:17.000Z","copyright":false,"_content":"\n待补充\n\n\n","source":"aboutme/index.md","raw":"---\ntitle: About me\nreward: false\ndate: 2020-02-06 19:07:17\ncopyright: false\n---\n\n待补充\n\n\n","updated":"2020-04-09T11:46:13.295Z","path":"aboutme/index.html","comments":1,"layout":"page","_id":"ck9kyh06m00011pdbfkh7f7ab","content":"<p>待补充</p>\n","site":{"data":{}},"excerpt":"","more":"<p>待补充</p>\n"},{"title":"categories","date":"2020-02-05T03:53:35.000Z","type":"categories","layout":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2020-02-05 11:53:35\ntype: \"categories\"\nlayout: \"categories\"\ncomments: false\n---\n","updated":"2020-02-05T09:25:36.311Z","path":"categories/index.html","_id":"ck9kyh06r00031pdbh585ea9n","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2020-02-05T03:53:58.000Z","type":"tags","layout":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2020-02-05 11:53:58\ntype: \"tags\"\nlayout: \"tags\"\ncomments: false\n---\n","updated":"2020-02-05T09:25:36.312Z","path":"tags/index.html","_id":"ck9kyh0ck002k1pdb514l8ih1","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"使用Wireshark分析SRT直播流","reward":false,"top":false,"date":"2020-04-09T04:33:36.000Z","_content":"\n[SRT(Secure Reliable Transport)](https://www.haivision.com/products/srt-secure-reliable-transport/)是一种基于[UDT(UDP-based Data Transfer)](https://tools.ietf.org/html/draft-gg-udt-03)的、安全的、可靠的、开源的数据传输协议&技术。SRT在UDP基础之上实现了：智能数据重传机制和AES256加密技术，这使得其成为一种安全、可靠、低延迟的传输技术。利用SRT，可以实现在不可预测的网络环境下（例如互联网）高效、安全的传输数据。[SRT](https://github.com/Haivision/srt/)还做了特殊优化以适合视频实时流数据的传输。根据[SRT Alliance](https://www.srtalliance.org/srt-alliance-announces-the-addition-of-the-srt-low-latency-protocol-to-open-broadcaster-softwares-obs-studio/)在2019-04-04的介绍，目前如下的应用已经集成并支持SRT：[OBS Studio](https://obsproject.com/)，[VideoLAN's VLC](https://www.videolan.org/vlc/)，[FFMpeg](http://ffmpeg.org/)，[Wireshark](https://www.wireshark.org/)。\n\n![](1.png)\n<!--more-->\n\n本文只介绍：**如何利用FFMpeg生成SRT数据流并利用Wireshark对该SRT数据进行抓包分析**。关于SRT的详细内容，可以参考[SRT Protocol Technical Overview Draft](https://github.com/Haivision/srt/files/2489142/SRT_Protocol_TechnicalOverview_DRAFT_2018-10-17.pdf)。\n\n## 前期准备\n\n1. 按照[说明](https://github.com/Haivision/srt/blob/master/README.md)安装SRT\n2. 利用`./configure --enable-libsrt`重新编译FFMpeg，让ffmpeg工具集支持SRT协议。重新configure的过程如果遇到`ERROR: srt >= 1.3.0 not found using pkg-config`的错误，可以查看`ffbuild/config.log`的相关信息，一般需要把srt和srt所依赖的openssl的**pkgconfig**路径增加到`PKG_CONFIG_PATH`环境变量中即可。\n3. 升级Wireshark到3.0之后的版本，并且设置Wireshark取消Wireshark对UDT协议的支持，具体做法为：点击菜单栏中的`Analyze`->`Enabled Protocols`，然后从弹出的支持协议中找到UDT，并取消UDT前面的选择标记。\n\n    ![](2.jpg)\n\n4. 安装VLC播放器，用于播放SRT协议的视频流。\n\n## 生成SRT直播流\n可以利用`ffmpeg`和`srt-live-transmit`（该工具在安装srt的时候会默认安装）来生成SRT直播流。主要思路是首先利用`ffmpeg`生成UDP的直播流，然后利用`srt-live-transmit`把UDP的直播流转换成SRT的直播流，更详细的方式可以参考[srt-live-transmit的使用说明](https://github.com/Haivision/srt/blob/master/docs/srt-live-transmit.md)。\n\n```shell\n# 生成UDP视频流\n$ ffmpeg -f lavfi -re -i smptebars=duration=300:size=1280x720:rate=30 \\\n-f lavfi -re -i sine=frequency=1000:duration=60:sample_rate=44100 \\\n-pix_fmt yuv420p -c:v libx264 -b:v 1000k -g 30 -keyint_min 120 \\\n-profile:v baseline -preset veryfast -f mpegts \\\n\"udp://127.0.0.1:5000?pkt_size=1316\"\n\n# 生成SRT视频流\n$ srt-live-transmit -s:10 udp://:5000 srt://:5001\n\n# 使用ffplay播放SRT视频流\n$ ffplay \"srt://127.0.0.1:5001\"\n```\n\n具体播放效果如下所示：\n\n![](3.jpg)\n\n## 使用Wireshark分析SRT\n为了可以用Wireshark抓到SRT数据包，需要使用VLC播放器来打开刚才创建的SRT视频流，具体如下所示：\n\n![](4.jpg)\n\n打开Wireshark，选择Lookback（因为要捕获的SRT地址为127.0.0.1），然后在捕获的数据窗口选择srt协议过滤，稍等片刻就可以看到捕获的SRT数据包，具体如下图所示：\n\n![](5.jpg)\n\n接下来就可以利用Wireshark来分析SRT协议的处理流程，例如上图中的Handshake数据包。尤其是在学习的过程中，配合[SRT的协议文档](https://github.com/Haivision/srt/files/2489142/SRT_Protocol_TechnicalOverview_DRAFT_2018-10-17.pdf)以及Wireshark的抓包分析，能够加深对SRT协议的理解，达到事半功倍的效果。\n\n","source":"_posts/analysize-SRT-protocol-live-stream-with-wireshark.md","raw":"---\ntitle: 使用Wireshark分析SRT直播流\nreward: false\ntop: false\ndate: 2020-04-09 12:33:36\ncategories: \n  - 视频技术\ntags:\n  - Wireshark\n  - SRT\n  - 网络协议分析\n---\n\n[SRT(Secure Reliable Transport)](https://www.haivision.com/products/srt-secure-reliable-transport/)是一种基于[UDT(UDP-based Data Transfer)](https://tools.ietf.org/html/draft-gg-udt-03)的、安全的、可靠的、开源的数据传输协议&技术。SRT在UDP基础之上实现了：智能数据重传机制和AES256加密技术，这使得其成为一种安全、可靠、低延迟的传输技术。利用SRT，可以实现在不可预测的网络环境下（例如互联网）高效、安全的传输数据。[SRT](https://github.com/Haivision/srt/)还做了特殊优化以适合视频实时流数据的传输。根据[SRT Alliance](https://www.srtalliance.org/srt-alliance-announces-the-addition-of-the-srt-low-latency-protocol-to-open-broadcaster-softwares-obs-studio/)在2019-04-04的介绍，目前如下的应用已经集成并支持SRT：[OBS Studio](https://obsproject.com/)，[VideoLAN's VLC](https://www.videolan.org/vlc/)，[FFMpeg](http://ffmpeg.org/)，[Wireshark](https://www.wireshark.org/)。\n\n![](1.png)\n<!--more-->\n\n本文只介绍：**如何利用FFMpeg生成SRT数据流并利用Wireshark对该SRT数据进行抓包分析**。关于SRT的详细内容，可以参考[SRT Protocol Technical Overview Draft](https://github.com/Haivision/srt/files/2489142/SRT_Protocol_TechnicalOverview_DRAFT_2018-10-17.pdf)。\n\n## 前期准备\n\n1. 按照[说明](https://github.com/Haivision/srt/blob/master/README.md)安装SRT\n2. 利用`./configure --enable-libsrt`重新编译FFMpeg，让ffmpeg工具集支持SRT协议。重新configure的过程如果遇到`ERROR: srt >= 1.3.0 not found using pkg-config`的错误，可以查看`ffbuild/config.log`的相关信息，一般需要把srt和srt所依赖的openssl的**pkgconfig**路径增加到`PKG_CONFIG_PATH`环境变量中即可。\n3. 升级Wireshark到3.0之后的版本，并且设置Wireshark取消Wireshark对UDT协议的支持，具体做法为：点击菜单栏中的`Analyze`->`Enabled Protocols`，然后从弹出的支持协议中找到UDT，并取消UDT前面的选择标记。\n\n    ![](2.jpg)\n\n4. 安装VLC播放器，用于播放SRT协议的视频流。\n\n## 生成SRT直播流\n可以利用`ffmpeg`和`srt-live-transmit`（该工具在安装srt的时候会默认安装）来生成SRT直播流。主要思路是首先利用`ffmpeg`生成UDP的直播流，然后利用`srt-live-transmit`把UDP的直播流转换成SRT的直播流，更详细的方式可以参考[srt-live-transmit的使用说明](https://github.com/Haivision/srt/blob/master/docs/srt-live-transmit.md)。\n\n```shell\n# 生成UDP视频流\n$ ffmpeg -f lavfi -re -i smptebars=duration=300:size=1280x720:rate=30 \\\n-f lavfi -re -i sine=frequency=1000:duration=60:sample_rate=44100 \\\n-pix_fmt yuv420p -c:v libx264 -b:v 1000k -g 30 -keyint_min 120 \\\n-profile:v baseline -preset veryfast -f mpegts \\\n\"udp://127.0.0.1:5000?pkt_size=1316\"\n\n# 生成SRT视频流\n$ srt-live-transmit -s:10 udp://:5000 srt://:5001\n\n# 使用ffplay播放SRT视频流\n$ ffplay \"srt://127.0.0.1:5001\"\n```\n\n具体播放效果如下所示：\n\n![](3.jpg)\n\n## 使用Wireshark分析SRT\n为了可以用Wireshark抓到SRT数据包，需要使用VLC播放器来打开刚才创建的SRT视频流，具体如下所示：\n\n![](4.jpg)\n\n打开Wireshark，选择Lookback（因为要捕获的SRT地址为127.0.0.1），然后在捕获的数据窗口选择srt协议过滤，稍等片刻就可以看到捕获的SRT数据包，具体如下图所示：\n\n![](5.jpg)\n\n接下来就可以利用Wireshark来分析SRT协议的处理流程，例如上图中的Handshake数据包。尤其是在学习的过程中，配合[SRT的协议文档](https://github.com/Haivision/srt/files/2489142/SRT_Protocol_TechnicalOverview_DRAFT_2018-10-17.pdf)以及Wireshark的抓包分析，能够加深对SRT协议的理解，达到事半功倍的效果。\n\n","slug":"analysize-SRT-protocol-live-stream-with-wireshark","published":1,"updated":"2020-04-09T09:04:14.371Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9kyh06400001pdbg9p27a1d","content":"<p><a href=\"https://www.haivision.com/products/srt-secure-reliable-transport/\" target=\"_blank\" rel=\"noopener\">SRT(Secure Reliable Transport)</a>是一种基于<a href=\"https://tools.ietf.org/html/draft-gg-udt-03\" target=\"_blank\" rel=\"noopener\">UDT(UDP-based Data Transfer)</a>的、安全的、可靠的、开源的数据传输协议&amp;技术。SRT在UDP基础之上实现了：智能数据重传机制和AES256加密技术，这使得其成为一种安全、可靠、低延迟的传输技术。利用SRT，可以实现在不可预测的网络环境下（例如互联网）高效、安全的传输数据。<a href=\"https://github.com/Haivision/srt/\" target=\"_blank\" rel=\"noopener\">SRT</a>还做了特殊优化以适合视频实时流数据的传输。根据<a href=\"https://www.srtalliance.org/srt-alliance-announces-the-addition-of-the-srt-low-latency-protocol-to-open-broadcaster-softwares-obs-studio/\" target=\"_blank\" rel=\"noopener\">SRT Alliance</a>在2019-04-04的介绍，目前如下的应用已经集成并支持SRT：<a href=\"https://obsproject.com/\" target=\"_blank\" rel=\"noopener\">OBS Studio</a>，<a href=\"https://www.videolan.org/vlc/\" target=\"_blank\" rel=\"noopener\">VideoLAN’s VLC</a>，<a href=\"http://ffmpeg.org/\" target=\"_blank\" rel=\"noopener\">FFMpeg</a>，<a href=\"https://www.wireshark.org/\" target=\"_blank\" rel=\"noopener\">Wireshark</a>。</p>\n<p><img src=\"/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/1.png\" alt></p>\n<a id=\"more\"></a>\n\n<p>本文只介绍：<strong>如何利用FFMpeg生成SRT数据流并利用Wireshark对该SRT数据进行抓包分析</strong>。关于SRT的详细内容，可以参考<a href=\"https://github.com/Haivision/srt/files/2489142/SRT_Protocol_TechnicalOverview_DRAFT_2018-10-17.pdf\" target=\"_blank\" rel=\"noopener\">SRT Protocol Technical Overview Draft</a>。</p>\n<h2 id=\"前期准备\"><a href=\"#前期准备\" class=\"headerlink\" title=\"前期准备\"></a>前期准备</h2><ol>\n<li><p>按照<a href=\"https://github.com/Haivision/srt/blob/master/README.md\" target=\"_blank\" rel=\"noopener\">说明</a>安装SRT</p>\n</li>\n<li><p>利用<code>./configure --enable-libsrt</code>重新编译FFMpeg，让ffmpeg工具集支持SRT协议。重新configure的过程如果遇到<code>ERROR: srt &gt;= 1.3.0 not found using pkg-config</code>的错误，可以查看<code>ffbuild/config.log</code>的相关信息，一般需要把srt和srt所依赖的openssl的<strong>pkgconfig</strong>路径增加到<code>PKG_CONFIG_PATH</code>环境变量中即可。</p>\n</li>\n<li><p>升级Wireshark到3.0之后的版本，并且设置Wireshark取消Wireshark对UDT协议的支持，具体做法为：点击菜单栏中的<code>Analyze</code>-&gt;<code>Enabled Protocols</code>，然后从弹出的支持协议中找到UDT，并取消UDT前面的选择标记。</p>\n<p> <img src=\"/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/2.jpg\" alt></p>\n</li>\n<li><p>安装VLC播放器，用于播放SRT协议的视频流。</p>\n</li>\n</ol>\n<h2 id=\"生成SRT直播流\"><a href=\"#生成SRT直播流\" class=\"headerlink\" title=\"生成SRT直播流\"></a>生成SRT直播流</h2><p>可以利用<code>ffmpeg</code>和<code>srt-live-transmit</code>（该工具在安装srt的时候会默认安装）来生成SRT直播流。主要思路是首先利用<code>ffmpeg</code>生成UDP的直播流，然后利用<code>srt-live-transmit</code>把UDP的直播流转换成SRT的直播流，更详细的方式可以参考<a href=\"https://github.com/Haivision/srt/blob/master/docs/srt-live-transmit.md\" target=\"_blank\" rel=\"noopener\">srt-live-transmit的使用说明</a>。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 生成UDP视频流</span></span><br><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> ffmpeg -f lavfi -re -i smptebars=duration=300:size=1280x720:rate=30 \\</span></span><br><span class=\"line\">-f lavfi -re -i sine=frequency=1000:duration=60:sample_rate=44100 \\</span><br><span class=\"line\">-pix_fmt yuv420p -c:v libx264 -b:v 1000k -g 30 -keyint_min 120 \\</span><br><span class=\"line\">-profile:v baseline -preset veryfast -f mpegts \\</span><br><span class=\"line\">\"udp://127.0.0.1:5000?pkt_size=1316\"</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 生成SRT视频流</span></span><br><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> srt-live-transmit -s:10 udp://:5000 srt://:5001</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 使用ffplay播放SRT视频流</span></span><br><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> ffplay <span class=\"string\">\"srt://127.0.0.1:5001\"</span></span></span><br></pre></td></tr></table></figure>\n\n<p>具体播放效果如下所示：</p>\n<p><img src=\"/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/3.jpg\" alt></p>\n<h2 id=\"使用Wireshark分析SRT\"><a href=\"#使用Wireshark分析SRT\" class=\"headerlink\" title=\"使用Wireshark分析SRT\"></a>使用Wireshark分析SRT</h2><p>为了可以用Wireshark抓到SRT数据包，需要使用VLC播放器来打开刚才创建的SRT视频流，具体如下所示：</p>\n<p><img src=\"/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/4.jpg\" alt></p>\n<p>打开Wireshark，选择Lookback（因为要捕获的SRT地址为127.0.0.1），然后在捕获的数据窗口选择srt协议过滤，稍等片刻就可以看到捕获的SRT数据包，具体如下图所示：</p>\n<p><img src=\"/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/5.jpg\" alt></p>\n<p>接下来就可以利用Wireshark来分析SRT协议的处理流程，例如上图中的Handshake数据包。尤其是在学习的过程中，配合<a href=\"https://github.com/Haivision/srt/files/2489142/SRT_Protocol_TechnicalOverview_DRAFT_2018-10-17.pdf\" target=\"_blank\" rel=\"noopener\">SRT的协议文档</a>以及Wireshark的抓包分析，能够加深对SRT协议的理解，达到事半功倍的效果。</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"https://www.haivision.com/products/srt-secure-reliable-transport/\" target=\"_blank\" rel=\"noopener\">SRT(Secure Reliable Transport)</a>是一种基于<a href=\"https://tools.ietf.org/html/draft-gg-udt-03\" target=\"_blank\" rel=\"noopener\">UDT(UDP-based Data Transfer)</a>的、安全的、可靠的、开源的数据传输协议&amp;技术。SRT在UDP基础之上实现了：智能数据重传机制和AES256加密技术，这使得其成为一种安全、可靠、低延迟的传输技术。利用SRT，可以实现在不可预测的网络环境下（例如互联网）高效、安全的传输数据。<a href=\"https://github.com/Haivision/srt/\" target=\"_blank\" rel=\"noopener\">SRT</a>还做了特殊优化以适合视频实时流数据的传输。根据<a href=\"https://www.srtalliance.org/srt-alliance-announces-the-addition-of-the-srt-low-latency-protocol-to-open-broadcaster-softwares-obs-studio/\" target=\"_blank\" rel=\"noopener\">SRT Alliance</a>在2019-04-04的介绍，目前如下的应用已经集成并支持SRT：<a href=\"https://obsproject.com/\" target=\"_blank\" rel=\"noopener\">OBS Studio</a>，<a href=\"https://www.videolan.org/vlc/\" target=\"_blank\" rel=\"noopener\">VideoLAN’s VLC</a>，<a href=\"http://ffmpeg.org/\" target=\"_blank\" rel=\"noopener\">FFMpeg</a>，<a href=\"https://www.wireshark.org/\" target=\"_blank\" rel=\"noopener\">Wireshark</a>。</p>\n<p><img src=\"/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/1.png\" alt></p>","more":"<p>本文只介绍：<strong>如何利用FFMpeg生成SRT数据流并利用Wireshark对该SRT数据进行抓包分析</strong>。关于SRT的详细内容，可以参考<a href=\"https://github.com/Haivision/srt/files/2489142/SRT_Protocol_TechnicalOverview_DRAFT_2018-10-17.pdf\" target=\"_blank\" rel=\"noopener\">SRT Protocol Technical Overview Draft</a>。</p>\n<h2 id=\"前期准备\"><a href=\"#前期准备\" class=\"headerlink\" title=\"前期准备\"></a>前期准备</h2><ol>\n<li><p>按照<a href=\"https://github.com/Haivision/srt/blob/master/README.md\" target=\"_blank\" rel=\"noopener\">说明</a>安装SRT</p>\n</li>\n<li><p>利用<code>./configure --enable-libsrt</code>重新编译FFMpeg，让ffmpeg工具集支持SRT协议。重新configure的过程如果遇到<code>ERROR: srt &gt;= 1.3.0 not found using pkg-config</code>的错误，可以查看<code>ffbuild/config.log</code>的相关信息，一般需要把srt和srt所依赖的openssl的<strong>pkgconfig</strong>路径增加到<code>PKG_CONFIG_PATH</code>环境变量中即可。</p>\n</li>\n<li><p>升级Wireshark到3.0之后的版本，并且设置Wireshark取消Wireshark对UDT协议的支持，具体做法为：点击菜单栏中的<code>Analyze</code>-&gt;<code>Enabled Protocols</code>，然后从弹出的支持协议中找到UDT，并取消UDT前面的选择标记。</p>\n<p> <img src=\"/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/2.jpg\" alt></p>\n</li>\n<li><p>安装VLC播放器，用于播放SRT协议的视频流。</p>\n</li>\n</ol>\n<h2 id=\"生成SRT直播流\"><a href=\"#生成SRT直播流\" class=\"headerlink\" title=\"生成SRT直播流\"></a>生成SRT直播流</h2><p>可以利用<code>ffmpeg</code>和<code>srt-live-transmit</code>（该工具在安装srt的时候会默认安装）来生成SRT直播流。主要思路是首先利用<code>ffmpeg</code>生成UDP的直播流，然后利用<code>srt-live-transmit</code>把UDP的直播流转换成SRT的直播流，更详细的方式可以参考<a href=\"https://github.com/Haivision/srt/blob/master/docs/srt-live-transmit.md\" target=\"_blank\" rel=\"noopener\">srt-live-transmit的使用说明</a>。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 生成UDP视频流</span></span><br><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> ffmpeg -f lavfi -re -i smptebars=duration=300:size=1280x720:rate=30 \\</span></span><br><span class=\"line\">-f lavfi -re -i sine=frequency=1000:duration=60:sample_rate=44100 \\</span><br><span class=\"line\">-pix_fmt yuv420p -c:v libx264 -b:v 1000k -g 30 -keyint_min 120 \\</span><br><span class=\"line\">-profile:v baseline -preset veryfast -f mpegts \\</span><br><span class=\"line\">\"udp://127.0.0.1:5000?pkt_size=1316\"</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 生成SRT视频流</span></span><br><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> srt-live-transmit -s:10 udp://:5000 srt://:5001</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 使用ffplay播放SRT视频流</span></span><br><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> ffplay <span class=\"string\">\"srt://127.0.0.1:5001\"</span></span></span><br></pre></td></tr></table></figure>\n\n<p>具体播放效果如下所示：</p>\n<p><img src=\"/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/3.jpg\" alt></p>\n<h2 id=\"使用Wireshark分析SRT\"><a href=\"#使用Wireshark分析SRT\" class=\"headerlink\" title=\"使用Wireshark分析SRT\"></a>使用Wireshark分析SRT</h2><p>为了可以用Wireshark抓到SRT数据包，需要使用VLC播放器来打开刚才创建的SRT视频流，具体如下所示：</p>\n<p><img src=\"/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/4.jpg\" alt></p>\n<p>打开Wireshark，选择Lookback（因为要捕获的SRT地址为127.0.0.1），然后在捕获的数据窗口选择srt协议过滤，稍等片刻就可以看到捕获的SRT数据包，具体如下图所示：</p>\n<p><img src=\"/2020/04/09/analysize-SRT-protocol-live-stream-with-wireshark/5.jpg\" alt></p>\n<p>接下来就可以利用Wireshark来分析SRT协议的处理流程，例如上图中的Handshake数据包。尤其是在学习的过程中，配合<a href=\"https://github.com/Haivision/srt/files/2489142/SRT_Protocol_TechnicalOverview_DRAFT_2018-10-17.pdf\" target=\"_blank\" rel=\"noopener\">SRT的协议文档</a>以及Wireshark的抓包分析，能够加深对SRT协议的理解，达到事半功倍的效果。</p>"},{"title":"matplotlib的backends以及非交互式绘图","reward":false,"top":false,"date":"2020-04-28T10:40:01.000Z","_content":"\n在分析视频的psnr时，需要用到`matplotlib`来绘制视频每一帧的psnr。在Mac调试的时候可以正常生成如下所示的psnr趋势图：\n\n![](1.png)\n\n但是将应用部署到linux机器的时候，却提示**ModuleNotFoundError: No module named '_tkinter'**的错误。\n\n<!--more-->\n\n具体如下图所示：\n\n![](2.jpg)\n\n请教了同事后采用如下的方式解决了问题：\n\n```\nimport matplotlib \nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n```\n\n![](3.jpg)\n\n虽然解决了问题，但是当时没有明白：\n* 问题究竟是如何解决的？\n* 为什么这样就可以解决？\n* 解决方案和之前的写法的差异在哪里？\n\n因此查阅了matplotlib的文档，然后就有了该文接下来的内容。为了保证材料的原汁原味，如下的内容均摘录自matplotlib的开发文档。\n\n## matplotlib中backend的概念\nA lot of documentation on the website and in the mailing lists refers to the **\"backend\"** and many new users are confused by this term. matplotlib targets many different use cases and output formats. Some people use matplotlib interactively from the python shell and have plotting windows pop up when they type commands. Some people run Jupyter notebooks and draw inline plots for quick data analysis. Others embed matplotlib into graphical user interfaces like wxpython or pygtk to build rich applications. Some people use matplotlib in batch scripts to generate postscript images from numerical simulations, and still others run web application servers to dynamically serve up graphs.\n\n**To support all of these use cases, matplotlib can target different outputs, and each of these capabilities is called a backend; the \"frontend\" is the user facing code, i.e., the plotting code, whereas the \"backend\" does all the hard work behind-the-scenes to make the figure.** \n\nThere are two types of backends: \n* user interface backends (for use in pygtk, wxpython, tkinter, qt4, or macosx; also referred to as `\"interactive backends\"`). The interactive backends: GTK3Agg, GTK3Cairo, MacOSX, nbAgg, Qt4Agg, Qt4Cairo, Qt5Agg, Qt5Cairo, TkAgg, TkCairo, WebAgg, WX, WXAgg, WXCairo.\n* hardcopy backends to make image files (PNG, SVG, PDF, PS; also referred to as `\"non-interactive backends\"`). The non-interactive backends: agg, cairo, pdf, pgf, ps, svg, template.\n\n\n## matplotlib中backend的默认配置\nBy default, **Matplotlib should automatically select a default backend which allows both interactive work and plotting from scripts**, with output to the screen and/or to a file, so at least initially you will not need to worry about the backend. **The most common exception is if your Python distribution comes without tkinter and you have no other GUI toolkit installed; this happens on certain Linux distributions**, where you need to install a Linux package named python-tk (or similar).\n\nIf, however, you want to write graphical user interfaces, or a web application server, or need a better understanding of what is going on, read on. To make things a little more customizable for graphical user interfaces, matplotlib separates the concept of the renderer (the thing that actually does the drawing) from the canvas (the place where the drawing goes). The canonical renderer for user interfaces is Agg which uses the Anti-Grain Geometry C++ library to make a raster (pixel) image of the figure; it is used by the Qt5Agg, Qt4Agg, GTK3Agg, wxAgg, TkAgg, and macosx backends. An alternative renderer is based on the Cairo library, used by Qt5Cairo, Qt4Cairo, etc.\n\n## interactive模式的概念\nUse of an interactive backend (see What is a backend?) permits--but does not by itself require or ensure--plotting to the screen. Whether and when plotting to the screen occurs, and whether a script or shell session continues after a plot is drawn on the screen, depends on the functions and methods that are called, and on a state variable that determines whether matplotlib is in \"interactive mode\". The default Boolean value is set by the matplotlibrc file, and may be customized like any other configuration parameter (see Customizing Matplotlib with style sheets and rcParams). It may also be set via matplotlib.interactive(), and its value may be queried via matplotlib.is_interactive(). Turning interactive mode on and off in the middle of a stream of plotting commands, whether in a script or in a shell, is rarely needed and potentially confusing, so in the following we will assume all plotting is done with interactive mode either on or off.\n\nInteractive mode may also be turned on via matplotlib.pyplot.ion(), and turned off via matplotlib.pyplot.ioff().\n\n**In interactive mode, pyplot functions automatically draw to the screen.** When plotting interactively, if using object method calls in addition to pyplot functions, then call draw() whenever you want to refresh the plot.\n\nUse non-interactive mode in scripts in which you want to generate one or more figures and display them before ending or generating a new set of figures. In that case, use show() to display the figure(s) and to block execution until you have manually destroyed them.\n\n## 如何选择backend\nThere are three ways to configure your backend:\n* The `rcParams[\"backend\"]` (default: 'agg') parameter in your matplotlibrc file\n* The `MPLBACKEND` environment variable\n* The function `matplotlib.use()`\n\nA more detailed description is given below.\n\nIf multiple of these are configurations are present, the last one from the list takes precedence; e.g. calling `matplotlib.use()` will override the setting in your matplotlibrc.\n\nIf no backend is explicitly set, Matplotlib automatically detects a usable backend based on what is available on your system and on whether a GUI event loop is already running. On Linux, if the environment variable `DISPLAY` is unset, the \"event loop\" is identified as \"headless\", which causes a fallback to a noninteractive backend (agg).\n\nHere is a detailed description of the configuration methods:\n1. Setting `rcParams[\"backend\"]` (default: 'agg') in your matplotlibrc file:\n  ```\n  backend : qt5agg   # use pyqt5 with antigrain (agg) rendering\n  ```\n  See also Customizing Matplotlib with style sheets and rcParams.\n\n2. Setting the `MPLBACKEND` environment variable:\n\n  You can set the environment variable either for your current shell or for a single script.\n\n  On Unix:\n\n  ```\n  > export MPLBACKEND=qt5agg\n  > python simple_plot.py\n\n  > MPLBACKEND=qt5agg python simple_plot.py\n  ```\n\n  On Windows, only the former is possible:\n\n  ```\n  > set MPLBACKEND=qt5agg\n  > python simple_plot.py\n  ```\n\n  Setting this environment variable will override the backend parameter in any matplotlibrc, even if there is a matplotlibrc in your current working directory. Therefore, setting MPLBACKEND globally, e.g. in your .bashrc or .profile, is discouraged as it might lead to counter-intuitive behavior.\n\n3. If your script depends on a specific backend you can use the function `matplotlib.use()`:\n\n  ```\n  import matplotlib\n  matplotlib.use('qt5agg')\n  ```\n  This should be done before any figure is created; otherwise Matplotlib may fail to switch the backend and raise an ImportError.\n\n  Using use will require changes in your code if users want to use a different backend. Therefore, you should avoid explicitly calling use unless absolutely necessary.\n\n## matplotlib.use('Agg')的作用\n在介绍完整整体的matplotlib中的backends以及matplotlib中的绘图模式之后，就能明白为什么文章开始出现的迁移过程中会出现类似的错误。\n\n首先根据[matplotlib中backend的默认配置](#matplotlib中backend的默认配置)中的介绍可以知道，matplotlib会有一个默认的backend配置，并且该默认的backend会同时支持交互式绘图模式以及利用脚本绘图的模式。并且，交互式绘图还需要利用GUI工具来生成一个屏幕绘图的窗口。而迁移的Linux机器上缺乏tkinter这样的GUI工具，因此在`import matplotlib.pyplot as plt`的时候会产生错误。\n\n而利用`matplotlib.use('Agg')`可以切换matplotlib的backend，将其backend从默认的交互式模式切换为非交互式模式，此时，生成的图形会以图片的形式保存起来，而无需tkinter这样的GUI工具的支持，因此可以解决本文最开始出现的问题。\n\n","source":"_posts/Matplotlib-s-backends-and-non-interactive-backends-for-rendering.md","raw":"---\ntitle: matplotlib的backends以及非交互式绘图\nreward: false\ntop: false\ndate: 2020-04-28 18:40:01\ncategories: \n  - matplotlib\ntags:\n  - python\n  - matplotlib\n  - non-interactive rendering \n---\n\n在分析视频的psnr时，需要用到`matplotlib`来绘制视频每一帧的psnr。在Mac调试的时候可以正常生成如下所示的psnr趋势图：\n\n![](1.png)\n\n但是将应用部署到linux机器的时候，却提示**ModuleNotFoundError: No module named '_tkinter'**的错误。\n\n<!--more-->\n\n具体如下图所示：\n\n![](2.jpg)\n\n请教了同事后采用如下的方式解决了问题：\n\n```\nimport matplotlib \nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n```\n\n![](3.jpg)\n\n虽然解决了问题，但是当时没有明白：\n* 问题究竟是如何解决的？\n* 为什么这样就可以解决？\n* 解决方案和之前的写法的差异在哪里？\n\n因此查阅了matplotlib的文档，然后就有了该文接下来的内容。为了保证材料的原汁原味，如下的内容均摘录自matplotlib的开发文档。\n\n## matplotlib中backend的概念\nA lot of documentation on the website and in the mailing lists refers to the **\"backend\"** and many new users are confused by this term. matplotlib targets many different use cases and output formats. Some people use matplotlib interactively from the python shell and have plotting windows pop up when they type commands. Some people run Jupyter notebooks and draw inline plots for quick data analysis. Others embed matplotlib into graphical user interfaces like wxpython or pygtk to build rich applications. Some people use matplotlib in batch scripts to generate postscript images from numerical simulations, and still others run web application servers to dynamically serve up graphs.\n\n**To support all of these use cases, matplotlib can target different outputs, and each of these capabilities is called a backend; the \"frontend\" is the user facing code, i.e., the plotting code, whereas the \"backend\" does all the hard work behind-the-scenes to make the figure.** \n\nThere are two types of backends: \n* user interface backends (for use in pygtk, wxpython, tkinter, qt4, or macosx; also referred to as `\"interactive backends\"`). The interactive backends: GTK3Agg, GTK3Cairo, MacOSX, nbAgg, Qt4Agg, Qt4Cairo, Qt5Agg, Qt5Cairo, TkAgg, TkCairo, WebAgg, WX, WXAgg, WXCairo.\n* hardcopy backends to make image files (PNG, SVG, PDF, PS; also referred to as `\"non-interactive backends\"`). The non-interactive backends: agg, cairo, pdf, pgf, ps, svg, template.\n\n\n## matplotlib中backend的默认配置\nBy default, **Matplotlib should automatically select a default backend which allows both interactive work and plotting from scripts**, with output to the screen and/or to a file, so at least initially you will not need to worry about the backend. **The most common exception is if your Python distribution comes without tkinter and you have no other GUI toolkit installed; this happens on certain Linux distributions**, where you need to install a Linux package named python-tk (or similar).\n\nIf, however, you want to write graphical user interfaces, or a web application server, or need a better understanding of what is going on, read on. To make things a little more customizable for graphical user interfaces, matplotlib separates the concept of the renderer (the thing that actually does the drawing) from the canvas (the place where the drawing goes). The canonical renderer for user interfaces is Agg which uses the Anti-Grain Geometry C++ library to make a raster (pixel) image of the figure; it is used by the Qt5Agg, Qt4Agg, GTK3Agg, wxAgg, TkAgg, and macosx backends. An alternative renderer is based on the Cairo library, used by Qt5Cairo, Qt4Cairo, etc.\n\n## interactive模式的概念\nUse of an interactive backend (see What is a backend?) permits--but does not by itself require or ensure--plotting to the screen. Whether and when plotting to the screen occurs, and whether a script or shell session continues after a plot is drawn on the screen, depends on the functions and methods that are called, and on a state variable that determines whether matplotlib is in \"interactive mode\". The default Boolean value is set by the matplotlibrc file, and may be customized like any other configuration parameter (see Customizing Matplotlib with style sheets and rcParams). It may also be set via matplotlib.interactive(), and its value may be queried via matplotlib.is_interactive(). Turning interactive mode on and off in the middle of a stream of plotting commands, whether in a script or in a shell, is rarely needed and potentially confusing, so in the following we will assume all plotting is done with interactive mode either on or off.\n\nInteractive mode may also be turned on via matplotlib.pyplot.ion(), and turned off via matplotlib.pyplot.ioff().\n\n**In interactive mode, pyplot functions automatically draw to the screen.** When plotting interactively, if using object method calls in addition to pyplot functions, then call draw() whenever you want to refresh the plot.\n\nUse non-interactive mode in scripts in which you want to generate one or more figures and display them before ending or generating a new set of figures. In that case, use show() to display the figure(s) and to block execution until you have manually destroyed them.\n\n## 如何选择backend\nThere are three ways to configure your backend:\n* The `rcParams[\"backend\"]` (default: 'agg') parameter in your matplotlibrc file\n* The `MPLBACKEND` environment variable\n* The function `matplotlib.use()`\n\nA more detailed description is given below.\n\nIf multiple of these are configurations are present, the last one from the list takes precedence; e.g. calling `matplotlib.use()` will override the setting in your matplotlibrc.\n\nIf no backend is explicitly set, Matplotlib automatically detects a usable backend based on what is available on your system and on whether a GUI event loop is already running. On Linux, if the environment variable `DISPLAY` is unset, the \"event loop\" is identified as \"headless\", which causes a fallback to a noninteractive backend (agg).\n\nHere is a detailed description of the configuration methods:\n1. Setting `rcParams[\"backend\"]` (default: 'agg') in your matplotlibrc file:\n  ```\n  backend : qt5agg   # use pyqt5 with antigrain (agg) rendering\n  ```\n  See also Customizing Matplotlib with style sheets and rcParams.\n\n2. Setting the `MPLBACKEND` environment variable:\n\n  You can set the environment variable either for your current shell or for a single script.\n\n  On Unix:\n\n  ```\n  > export MPLBACKEND=qt5agg\n  > python simple_plot.py\n\n  > MPLBACKEND=qt5agg python simple_plot.py\n  ```\n\n  On Windows, only the former is possible:\n\n  ```\n  > set MPLBACKEND=qt5agg\n  > python simple_plot.py\n  ```\n\n  Setting this environment variable will override the backend parameter in any matplotlibrc, even if there is a matplotlibrc in your current working directory. Therefore, setting MPLBACKEND globally, e.g. in your .bashrc or .profile, is discouraged as it might lead to counter-intuitive behavior.\n\n3. If your script depends on a specific backend you can use the function `matplotlib.use()`:\n\n  ```\n  import matplotlib\n  matplotlib.use('qt5agg')\n  ```\n  This should be done before any figure is created; otherwise Matplotlib may fail to switch the backend and raise an ImportError.\n\n  Using use will require changes in your code if users want to use a different backend. Therefore, you should avoid explicitly calling use unless absolutely necessary.\n\n## matplotlib.use('Agg')的作用\n在介绍完整整体的matplotlib中的backends以及matplotlib中的绘图模式之后，就能明白为什么文章开始出现的迁移过程中会出现类似的错误。\n\n首先根据[matplotlib中backend的默认配置](#matplotlib中backend的默认配置)中的介绍可以知道，matplotlib会有一个默认的backend配置，并且该默认的backend会同时支持交互式绘图模式以及利用脚本绘图的模式。并且，交互式绘图还需要利用GUI工具来生成一个屏幕绘图的窗口。而迁移的Linux机器上缺乏tkinter这样的GUI工具，因此在`import matplotlib.pyplot as plt`的时候会产生错误。\n\n而利用`matplotlib.use('Agg')`可以切换matplotlib的backend，将其backend从默认的交互式模式切换为非交互式模式，此时，生成的图形会以图片的形式保存起来，而无需tkinter这样的GUI工具的支持，因此可以解决本文最开始出现的问题。\n\n","slug":"Matplotlib-s-backends-and-non-interactive-backends-for-rendering","published":1,"updated":"2020-04-29T06:25:07.132Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9kyh06p00021pdb2ye40dks","content":"<p>在分析视频的psnr时，需要用到<code>matplotlib</code>来绘制视频每一帧的psnr。在Mac调试的时候可以正常生成如下所示的psnr趋势图：</p>\n<p><img src=\"/2020/04/28/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/1.png\" alt></p>\n<p>但是将应用部署到linux机器的时候，却提示<strong>ModuleNotFoundError: No module named ‘_tkinter’</strong>的错误。</p>\n<a id=\"more\"></a>\n\n<p>具体如下图所示：</p>\n<p><img src=\"/2020/04/28/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/2.jpg\" alt></p>\n<p>请教了同事后采用如下的方式解决了问题：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import matplotlib </span><br><span class=\"line\">matplotlib.use(&#39;Agg&#39;)</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"/2020/04/28/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/3.jpg\" alt></p>\n<p>虽然解决了问题，但是当时没有明白：</p>\n<ul>\n<li>问题究竟是如何解决的？</li>\n<li>为什么这样就可以解决？</li>\n<li>解决方案和之前的写法的差异在哪里？</li>\n</ul>\n<p>因此查阅了matplotlib的文档，然后就有了该文接下来的内容。为了保证材料的原汁原味，如下的内容均摘录自matplotlib的开发文档。</p>\n<h2 id=\"matplotlib中backend的概念\"><a href=\"#matplotlib中backend的概念\" class=\"headerlink\" title=\"matplotlib中backend的概念\"></a>matplotlib中backend的概念</h2><p>A lot of documentation on the website and in the mailing lists refers to the <strong>“backend”</strong> and many new users are confused by this term. matplotlib targets many different use cases and output formats. Some people use matplotlib interactively from the python shell and have plotting windows pop up when they type commands. Some people run Jupyter notebooks and draw inline plots for quick data analysis. Others embed matplotlib into graphical user interfaces like wxpython or pygtk to build rich applications. Some people use matplotlib in batch scripts to generate postscript images from numerical simulations, and still others run web application servers to dynamically serve up graphs.</p>\n<p><strong>To support all of these use cases, matplotlib can target different outputs, and each of these capabilities is called a backend; the “frontend” is the user facing code, i.e., the plotting code, whereas the “backend” does all the hard work behind-the-scenes to make the figure.</strong> </p>\n<p>There are two types of backends: </p>\n<ul>\n<li>user interface backends (for use in pygtk, wxpython, tkinter, qt4, or macosx; also referred to as <code>&quot;interactive backends&quot;</code>). The interactive backends: GTK3Agg, GTK3Cairo, MacOSX, nbAgg, Qt4Agg, Qt4Cairo, Qt5Agg, Qt5Cairo, TkAgg, TkCairo, WebAgg, WX, WXAgg, WXCairo.</li>\n<li>hardcopy backends to make image files (PNG, SVG, PDF, PS; also referred to as <code>&quot;non-interactive backends&quot;</code>). The non-interactive backends: agg, cairo, pdf, pgf, ps, svg, template.</li>\n</ul>\n<h2 id=\"matplotlib中backend的默认配置\"><a href=\"#matplotlib中backend的默认配置\" class=\"headerlink\" title=\"matplotlib中backend的默认配置\"></a>matplotlib中backend的默认配置</h2><p>By default, <strong>Matplotlib should automatically select a default backend which allows both interactive work and plotting from scripts</strong>, with output to the screen and/or to a file, so at least initially you will not need to worry about the backend. <strong>The most common exception is if your Python distribution comes without tkinter and you have no other GUI toolkit installed; this happens on certain Linux distributions</strong>, where you need to install a Linux package named python-tk (or similar).</p>\n<p>If, however, you want to write graphical user interfaces, or a web application server, or need a better understanding of what is going on, read on. To make things a little more customizable for graphical user interfaces, matplotlib separates the concept of the renderer (the thing that actually does the drawing) from the canvas (the place where the drawing goes). The canonical renderer for user interfaces is Agg which uses the Anti-Grain Geometry C++ library to make a raster (pixel) image of the figure; it is used by the Qt5Agg, Qt4Agg, GTK3Agg, wxAgg, TkAgg, and macosx backends. An alternative renderer is based on the Cairo library, used by Qt5Cairo, Qt4Cairo, etc.</p>\n<h2 id=\"interactive模式的概念\"><a href=\"#interactive模式的概念\" class=\"headerlink\" title=\"interactive模式的概念\"></a>interactive模式的概念</h2><p>Use of an interactive backend (see What is a backend?) permits–but does not by itself require or ensure–plotting to the screen. Whether and when plotting to the screen occurs, and whether a script or shell session continues after a plot is drawn on the screen, depends on the functions and methods that are called, and on a state variable that determines whether matplotlib is in “interactive mode”. The default Boolean value is set by the matplotlibrc file, and may be customized like any other configuration parameter (see Customizing Matplotlib with style sheets and rcParams). It may also be set via matplotlib.interactive(), and its value may be queried via matplotlib.is_interactive(). Turning interactive mode on and off in the middle of a stream of plotting commands, whether in a script or in a shell, is rarely needed and potentially confusing, so in the following we will assume all plotting is done with interactive mode either on or off.</p>\n<p>Interactive mode may also be turned on via matplotlib.pyplot.ion(), and turned off via matplotlib.pyplot.ioff().</p>\n<p><strong>In interactive mode, pyplot functions automatically draw to the screen.</strong> When plotting interactively, if using object method calls in addition to pyplot functions, then call draw() whenever you want to refresh the plot.</p>\n<p>Use non-interactive mode in scripts in which you want to generate one or more figures and display them before ending or generating a new set of figures. In that case, use show() to display the figure(s) and to block execution until you have manually destroyed them.</p>\n<h2 id=\"如何选择backend\"><a href=\"#如何选择backend\" class=\"headerlink\" title=\"如何选择backend\"></a>如何选择backend</h2><p>There are three ways to configure your backend:</p>\n<ul>\n<li>The <code>rcParams[&quot;backend&quot;]</code> (default: ‘agg’) parameter in your matplotlibrc file</li>\n<li>The <code>MPLBACKEND</code> environment variable</li>\n<li>The function <code>matplotlib.use()</code></li>\n</ul>\n<p>A more detailed description is given below.</p>\n<p>If multiple of these are configurations are present, the last one from the list takes precedence; e.g. calling <code>matplotlib.use()</code> will override the setting in your matplotlibrc.</p>\n<p>If no backend is explicitly set, Matplotlib automatically detects a usable backend based on what is available on your system and on whether a GUI event loop is already running. On Linux, if the environment variable <code>DISPLAY</code> is unset, the “event loop” is identified as “headless”, which causes a fallback to a noninteractive backend (agg).</p>\n<p>Here is a detailed description of the configuration methods:</p>\n<ol>\n<li><p>Setting <code>rcParams[&quot;backend&quot;]</code> (default: ‘agg’) in your matplotlibrc file:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">backend : qt5agg   # use pyqt5 with antigrain (agg) rendering</span><br></pre></td></tr></table></figure>\n<p>See also Customizing Matplotlib with style sheets and rcParams.</p>\n</li>\n<li><p>Setting the <code>MPLBACKEND</code> environment variable:</p>\n<p>You can set the environment variable either for your current shell or for a single script.</p>\n<p>On Unix:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; export MPLBACKEND&#x3D;qt5agg</span><br><span class=\"line\">&gt; python simple_plot.py</span><br><span class=\"line\"></span><br><span class=\"line\">&gt; MPLBACKEND&#x3D;qt5agg python simple_plot.py</span><br></pre></td></tr></table></figure>\n\n<p>On Windows, only the former is possible:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; set MPLBACKEND&#x3D;qt5agg</span><br><span class=\"line\">&gt; python simple_plot.py</span><br></pre></td></tr></table></figure>\n\n<p>Setting this environment variable will override the backend parameter in any matplotlibrc, even if there is a matplotlibrc in your current working directory. Therefore, setting MPLBACKEND globally, e.g. in your .bashrc or .profile, is discouraged as it might lead to counter-intuitive behavior.</p>\n</li>\n<li><p>If your script depends on a specific backend you can use the function <code>matplotlib.use()</code>:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import matplotlib</span><br><span class=\"line\">matplotlib.use(&#39;qt5agg&#39;)</span><br></pre></td></tr></table></figure>\n<p>This should be done before any figure is created; otherwise Matplotlib may fail to switch the backend and raise an ImportError.</p>\n<p>Using use will require changes in your code if users want to use a different backend. Therefore, you should avoid explicitly calling use unless absolutely necessary.</p>\n</li>\n</ol>\n<h2 id=\"matplotlib-use-‘Agg’-的作用\"><a href=\"#matplotlib-use-‘Agg’-的作用\" class=\"headerlink\" title=\"matplotlib.use(‘Agg’)的作用\"></a>matplotlib.use(‘Agg’)的作用</h2><p>在介绍完整整体的matplotlib中的backends以及matplotlib中的绘图模式之后，就能明白为什么文章开始出现的迁移过程中会出现类似的错误。</p>\n<p>首先根据<a href=\"#matplotlib中backend的默认配置\">matplotlib中backend的默认配置</a>中的介绍可以知道，matplotlib会有一个默认的backend配置，并且该默认的backend会同时支持交互式绘图模式以及利用脚本绘图的模式。并且，交互式绘图还需要利用GUI工具来生成一个屏幕绘图的窗口。而迁移的Linux机器上缺乏tkinter这样的GUI工具，因此在<code>import matplotlib.pyplot as plt</code>的时候会产生错误。</p>\n<p>而利用<code>matplotlib.use(&#39;Agg&#39;)</code>可以切换matplotlib的backend，将其backend从默认的交互式模式切换为非交互式模式，此时，生成的图形会以图片的形式保存起来，而无需tkinter这样的GUI工具的支持，因此可以解决本文最开始出现的问题。</p>\n","site":{"data":{}},"excerpt":"<p>在分析视频的psnr时，需要用到<code>matplotlib</code>来绘制视频每一帧的psnr。在Mac调试的时候可以正常生成如下所示的psnr趋势图：</p>\n<p><img src=\"/2020/04/28/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/1.png\" alt></p>\n<p>但是将应用部署到linux机器的时候，却提示<strong>ModuleNotFoundError: No module named ‘_tkinter’</strong>的错误。</p>","more":"<p>具体如下图所示：</p>\n<p><img src=\"/2020/04/28/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/2.jpg\" alt></p>\n<p>请教了同事后采用如下的方式解决了问题：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import matplotlib </span><br><span class=\"line\">matplotlib.use(&#39;Agg&#39;)</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"/2020/04/28/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/3.jpg\" alt></p>\n<p>虽然解决了问题，但是当时没有明白：</p>\n<ul>\n<li>问题究竟是如何解决的？</li>\n<li>为什么这样就可以解决？</li>\n<li>解决方案和之前的写法的差异在哪里？</li>\n</ul>\n<p>因此查阅了matplotlib的文档，然后就有了该文接下来的内容。为了保证材料的原汁原味，如下的内容均摘录自matplotlib的开发文档。</p>\n<h2 id=\"matplotlib中backend的概念\"><a href=\"#matplotlib中backend的概念\" class=\"headerlink\" title=\"matplotlib中backend的概念\"></a>matplotlib中backend的概念</h2><p>A lot of documentation on the website and in the mailing lists refers to the <strong>“backend”</strong> and many new users are confused by this term. matplotlib targets many different use cases and output formats. Some people use matplotlib interactively from the python shell and have plotting windows pop up when they type commands. Some people run Jupyter notebooks and draw inline plots for quick data analysis. Others embed matplotlib into graphical user interfaces like wxpython or pygtk to build rich applications. Some people use matplotlib in batch scripts to generate postscript images from numerical simulations, and still others run web application servers to dynamically serve up graphs.</p>\n<p><strong>To support all of these use cases, matplotlib can target different outputs, and each of these capabilities is called a backend; the “frontend” is the user facing code, i.e., the plotting code, whereas the “backend” does all the hard work behind-the-scenes to make the figure.</strong> </p>\n<p>There are two types of backends: </p>\n<ul>\n<li>user interface backends (for use in pygtk, wxpython, tkinter, qt4, or macosx; also referred to as <code>&quot;interactive backends&quot;</code>). The interactive backends: GTK3Agg, GTK3Cairo, MacOSX, nbAgg, Qt4Agg, Qt4Cairo, Qt5Agg, Qt5Cairo, TkAgg, TkCairo, WebAgg, WX, WXAgg, WXCairo.</li>\n<li>hardcopy backends to make image files (PNG, SVG, PDF, PS; also referred to as <code>&quot;non-interactive backends&quot;</code>). The non-interactive backends: agg, cairo, pdf, pgf, ps, svg, template.</li>\n</ul>\n<h2 id=\"matplotlib中backend的默认配置\"><a href=\"#matplotlib中backend的默认配置\" class=\"headerlink\" title=\"matplotlib中backend的默认配置\"></a>matplotlib中backend的默认配置</h2><p>By default, <strong>Matplotlib should automatically select a default backend which allows both interactive work and plotting from scripts</strong>, with output to the screen and/or to a file, so at least initially you will not need to worry about the backend. <strong>The most common exception is if your Python distribution comes without tkinter and you have no other GUI toolkit installed; this happens on certain Linux distributions</strong>, where you need to install a Linux package named python-tk (or similar).</p>\n<p>If, however, you want to write graphical user interfaces, or a web application server, or need a better understanding of what is going on, read on. To make things a little more customizable for graphical user interfaces, matplotlib separates the concept of the renderer (the thing that actually does the drawing) from the canvas (the place where the drawing goes). The canonical renderer for user interfaces is Agg which uses the Anti-Grain Geometry C++ library to make a raster (pixel) image of the figure; it is used by the Qt5Agg, Qt4Agg, GTK3Agg, wxAgg, TkAgg, and macosx backends. An alternative renderer is based on the Cairo library, used by Qt5Cairo, Qt4Cairo, etc.</p>\n<h2 id=\"interactive模式的概念\"><a href=\"#interactive模式的概念\" class=\"headerlink\" title=\"interactive模式的概念\"></a>interactive模式的概念</h2><p>Use of an interactive backend (see What is a backend?) permits–but does not by itself require or ensure–plotting to the screen. Whether and when plotting to the screen occurs, and whether a script or shell session continues after a plot is drawn on the screen, depends on the functions and methods that are called, and on a state variable that determines whether matplotlib is in “interactive mode”. The default Boolean value is set by the matplotlibrc file, and may be customized like any other configuration parameter (see Customizing Matplotlib with style sheets and rcParams). It may also be set via matplotlib.interactive(), and its value may be queried via matplotlib.is_interactive(). Turning interactive mode on and off in the middle of a stream of plotting commands, whether in a script or in a shell, is rarely needed and potentially confusing, so in the following we will assume all plotting is done with interactive mode either on or off.</p>\n<p>Interactive mode may also be turned on via matplotlib.pyplot.ion(), and turned off via matplotlib.pyplot.ioff().</p>\n<p><strong>In interactive mode, pyplot functions automatically draw to the screen.</strong> When plotting interactively, if using object method calls in addition to pyplot functions, then call draw() whenever you want to refresh the plot.</p>\n<p>Use non-interactive mode in scripts in which you want to generate one or more figures and display them before ending or generating a new set of figures. In that case, use show() to display the figure(s) and to block execution until you have manually destroyed them.</p>\n<h2 id=\"如何选择backend\"><a href=\"#如何选择backend\" class=\"headerlink\" title=\"如何选择backend\"></a>如何选择backend</h2><p>There are three ways to configure your backend:</p>\n<ul>\n<li>The <code>rcParams[&quot;backend&quot;]</code> (default: ‘agg’) parameter in your matplotlibrc file</li>\n<li>The <code>MPLBACKEND</code> environment variable</li>\n<li>The function <code>matplotlib.use()</code></li>\n</ul>\n<p>A more detailed description is given below.</p>\n<p>If multiple of these are configurations are present, the last one from the list takes precedence; e.g. calling <code>matplotlib.use()</code> will override the setting in your matplotlibrc.</p>\n<p>If no backend is explicitly set, Matplotlib automatically detects a usable backend based on what is available on your system and on whether a GUI event loop is already running. On Linux, if the environment variable <code>DISPLAY</code> is unset, the “event loop” is identified as “headless”, which causes a fallback to a noninteractive backend (agg).</p>\n<p>Here is a detailed description of the configuration methods:</p>\n<ol>\n<li><p>Setting <code>rcParams[&quot;backend&quot;]</code> (default: ‘agg’) in your matplotlibrc file:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">backend : qt5agg   # use pyqt5 with antigrain (agg) rendering</span><br></pre></td></tr></table></figure>\n<p>See also Customizing Matplotlib with style sheets and rcParams.</p>\n</li>\n<li><p>Setting the <code>MPLBACKEND</code> environment variable:</p>\n<p>You can set the environment variable either for your current shell or for a single script.</p>\n<p>On Unix:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; export MPLBACKEND&#x3D;qt5agg</span><br><span class=\"line\">&gt; python simple_plot.py</span><br><span class=\"line\"></span><br><span class=\"line\">&gt; MPLBACKEND&#x3D;qt5agg python simple_plot.py</span><br></pre></td></tr></table></figure>\n\n<p>On Windows, only the former is possible:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; set MPLBACKEND&#x3D;qt5agg</span><br><span class=\"line\">&gt; python simple_plot.py</span><br></pre></td></tr></table></figure>\n\n<p>Setting this environment variable will override the backend parameter in any matplotlibrc, even if there is a matplotlibrc in your current working directory. Therefore, setting MPLBACKEND globally, e.g. in your .bashrc or .profile, is discouraged as it might lead to counter-intuitive behavior.</p>\n</li>\n<li><p>If your script depends on a specific backend you can use the function <code>matplotlib.use()</code>:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import matplotlib</span><br><span class=\"line\">matplotlib.use(&#39;qt5agg&#39;)</span><br></pre></td></tr></table></figure>\n<p>This should be done before any figure is created; otherwise Matplotlib may fail to switch the backend and raise an ImportError.</p>\n<p>Using use will require changes in your code if users want to use a different backend. Therefore, you should avoid explicitly calling use unless absolutely necessary.</p>\n</li>\n</ol>\n<h2 id=\"matplotlib-use-‘Agg’-的作用\"><a href=\"#matplotlib-use-‘Agg’-的作用\" class=\"headerlink\" title=\"matplotlib.use(‘Agg’)的作用\"></a>matplotlib.use(‘Agg’)的作用</h2><p>在介绍完整整体的matplotlib中的backends以及matplotlib中的绘图模式之后，就能明白为什么文章开始出现的迁移过程中会出现类似的错误。</p>\n<p>首先根据<a href=\"#matplotlib中backend的默认配置\">matplotlib中backend的默认配置</a>中的介绍可以知道，matplotlib会有一个默认的backend配置，并且该默认的backend会同时支持交互式绘图模式以及利用脚本绘图的模式。并且，交互式绘图还需要利用GUI工具来生成一个屏幕绘图的窗口。而迁移的Linux机器上缺乏tkinter这样的GUI工具，因此在<code>import matplotlib.pyplot as plt</code>的时候会产生错误。</p>\n<p>而利用<code>matplotlib.use(&#39;Agg&#39;)</code>可以切换matplotlib的backend，将其backend从默认的交互式模式切换为非交互式模式，此时，生成的图形会以图片的形式保存起来，而无需tkinter这样的GUI工具的支持，因此可以解决本文最开始出现的问题。</p>"},{"title":"Android环境下编译libyuv","reward":false,"top":false,"date":"2020-03-30T03:50:33.000Z","_content":"\nlibyuv是Google开源的实现各种YUV格式与RGB格式之间相互转换、旋转、缩放的库。关于libyuv的具体介绍可以参考[官网介绍](https://chromium.googlesource.com/libyuv/libyuv)。\n\n<!--more-->\n\n## Android Studio编译libyuv\n在自己编译libyuv的过程中，遇到了很多坑。在解决这些坑的过程中发现，从网上查询的很多资料并没有对问题的解决提供帮助。因此，在自己趟过了所有的坑之后，就想把编译libyuv的过程整理了出来，一方面是做一个梳理和总结，另一方面也希望帮助更多有类似需求的人。\n\n接下来，开始介绍在Android平台上，利用NKD编译包含libjpeg库的libyuv库。这个主要通过[libyuv/Android.mk](https://chromium.googlesource.com/libyuv/libyuv/+/refs/heads/master/Android.mk)中的如下代码实现：\n\n```\ninclude $(CLEAR_VARS)\n\nLOCAL_WHOLE_STATIC_LIBRARIES := libyuv_static\nLOCAL_MODULE := libyuv\nifneq ($(LIBYUV_DISABLE_JPEG), \"yes\")\nLOCAL_SHARED_LIBRARIES := jpeg\nendif\n\ninclude $(BUILD_SHARED_LIBRARY)\n```\n\n## 需要下载的库\n* [libyuv](https://chromium.googlesource.com/libyuv/libyuv)或者[libyuv](https//github.com/lemenkov/libyuv.git)\n* [libjpeg-turbo](https://github.com/libjpeg-turbo/libjpeg-turbo.git)\n\n## 编译依赖\n### NDK\n因为NDK-r17之后的版本，不支持gcc编译so，需要使用clang来编译so库。因此编译时，需要确定好自己的NDK版本。\n\n### CMAKE, NASM, GCC等\n该部分编译依赖为编译libjpeg-turbo库的编译依赖，具体可以参见libjpeg-turbo的[BUILD.md](https://github.com/libjpeg-turbo/libjpeg-turbo/blob/master/BUILDING.md)\n\n## 编译libjpeg-turbo\n1. 下载编译脚本，编译脚本位于[libjpeg-turbo](https://github.com/wangwei1237/libyuv-with-jpeg/tree/master/libjpeg-turbo)目录下，共计有三个脚本：\n    1. config.sh：配置编译参数，例如ANDROID_NDK_ROOT等，需要根据自己的环境变量进行替换。\n    2. build_jpeg.sh：编译某个CPU架构的so库，该脚本中的变量一般不需要修改。\n    3. build_jpeg_all.sh：编译所有CPU架构的so库\n\n2. 下载[libjpeg-turbo](https://github.com/libjpeg-turbo/libjpeg-turbo)源码：`git clone https://github.com/libjpeg-turbo/libjpeg-turbo.git`。\n\n3. 将第*1*步中下载到的三个文件复制到libjepg-turob源码的根目录下：`cp -r libjpeg-turbo/* JPEG_SRC_ROOT_PATH/`。\n\n4. 修改*config.sh*中的`ANDROID_NDK_ROOT`为自己的NDK路径，修改`ANDROID_NDK_VERSION`为对应的数字版本。例如*r16b*版本的为`ANDROID_NDK_VERSION=16`。\n\n5. 运行`sh build_jpeg_all.sh`编译libjpeg-turbo，运行之后会在当前目录的*libs*目录下生成各种CPU架构对应的so库。如下图所示：\n\n    ![图1. libjpeg-turbo编译产物](1.jpg)\n\n## 编译libyuv\n1. 将之前下载的libyuv的源码导入到Android Studio中的指定目录：PROJECT/app/jni/libyuv,该目录自己定义就可以。\n\n2. 将上一步生成的so文件按照ABI的格式复制到libyuv/libs目录下，如下图所示：\n\n    ![图2](2.png)\n\n3. 修改libyuv目录下的Android.mk文件，[libyuv/Android.mk](https://github.com/wangwei1237/libyuv-with-jpeg/blob/master/myapp/jni/libyuv/Android.mk)文件中的第4~12行的内容即可。\n\n    ```\n    ########################################################\n    ## {{BEGIN 增加如下的代码\n    include $(CLEAR_VARS)\n    LOCAL_MODULE := jpeg\n    LOCAL_SRC_FILES := libs/$(TARGET_ARCH_ABI)/libjpeg.so\n    LOCAL_EXPORT_C_INCLUDES := $(LOCAL_PATH)/include\n    include $(PREBUILT_SHARED_LIBRARY)\n    ## END}}\n    ########################################################\n    ```\n\n4. 按照[仓库](https://github.com/wangwei1237/libyuv-with-jpeg)所示的[myapp/jni](https://github.com/wangwei1237/libyuv-with-jpeg/tree/master/myapp/jni)目录下的*Android.mk*和*Application.mk*文件修改自己代码中的对应文件即可，主要是用于编译libyuv使用。\n\n5. 修改自己app目录下的[build.gradle](https://github.com/wangwei1237/libyuv-with-jpeg/blob/master/myapp/build.gradle)，在android{}中增加如下编译任务\n\n    ```\n    externalNativeBuild {\n        ndkBuild {\n            path file('jni/Android.mk')\n        }\n    }\n    ```\n\n6. 点击Android Studio的编译按钮编译即可，编译后生成的编译产物如下图所示：\n\n    ![图3. libyuv的编译产物](3.jpg)","source":"_posts/compile-libyuv-for-Android.md","raw":"---\ntitle: Android环境下编译libyuv\nreward: false\ntop: false\ndate: 2020-03-30 11:50:33\ncategories: Android\ntags: \n  - libyuv\n---\n\nlibyuv是Google开源的实现各种YUV格式与RGB格式之间相互转换、旋转、缩放的库。关于libyuv的具体介绍可以参考[官网介绍](https://chromium.googlesource.com/libyuv/libyuv)。\n\n<!--more-->\n\n## Android Studio编译libyuv\n在自己编译libyuv的过程中，遇到了很多坑。在解决这些坑的过程中发现，从网上查询的很多资料并没有对问题的解决提供帮助。因此，在自己趟过了所有的坑之后，就想把编译libyuv的过程整理了出来，一方面是做一个梳理和总结，另一方面也希望帮助更多有类似需求的人。\n\n接下来，开始介绍在Android平台上，利用NKD编译包含libjpeg库的libyuv库。这个主要通过[libyuv/Android.mk](https://chromium.googlesource.com/libyuv/libyuv/+/refs/heads/master/Android.mk)中的如下代码实现：\n\n```\ninclude $(CLEAR_VARS)\n\nLOCAL_WHOLE_STATIC_LIBRARIES := libyuv_static\nLOCAL_MODULE := libyuv\nifneq ($(LIBYUV_DISABLE_JPEG), \"yes\")\nLOCAL_SHARED_LIBRARIES := jpeg\nendif\n\ninclude $(BUILD_SHARED_LIBRARY)\n```\n\n## 需要下载的库\n* [libyuv](https://chromium.googlesource.com/libyuv/libyuv)或者[libyuv](https//github.com/lemenkov/libyuv.git)\n* [libjpeg-turbo](https://github.com/libjpeg-turbo/libjpeg-turbo.git)\n\n## 编译依赖\n### NDK\n因为NDK-r17之后的版本，不支持gcc编译so，需要使用clang来编译so库。因此编译时，需要确定好自己的NDK版本。\n\n### CMAKE, NASM, GCC等\n该部分编译依赖为编译libjpeg-turbo库的编译依赖，具体可以参见libjpeg-turbo的[BUILD.md](https://github.com/libjpeg-turbo/libjpeg-turbo/blob/master/BUILDING.md)\n\n## 编译libjpeg-turbo\n1. 下载编译脚本，编译脚本位于[libjpeg-turbo](https://github.com/wangwei1237/libyuv-with-jpeg/tree/master/libjpeg-turbo)目录下，共计有三个脚本：\n    1. config.sh：配置编译参数，例如ANDROID_NDK_ROOT等，需要根据自己的环境变量进行替换。\n    2. build_jpeg.sh：编译某个CPU架构的so库，该脚本中的变量一般不需要修改。\n    3. build_jpeg_all.sh：编译所有CPU架构的so库\n\n2. 下载[libjpeg-turbo](https://github.com/libjpeg-turbo/libjpeg-turbo)源码：`git clone https://github.com/libjpeg-turbo/libjpeg-turbo.git`。\n\n3. 将第*1*步中下载到的三个文件复制到libjepg-turob源码的根目录下：`cp -r libjpeg-turbo/* JPEG_SRC_ROOT_PATH/`。\n\n4. 修改*config.sh*中的`ANDROID_NDK_ROOT`为自己的NDK路径，修改`ANDROID_NDK_VERSION`为对应的数字版本。例如*r16b*版本的为`ANDROID_NDK_VERSION=16`。\n\n5. 运行`sh build_jpeg_all.sh`编译libjpeg-turbo，运行之后会在当前目录的*libs*目录下生成各种CPU架构对应的so库。如下图所示：\n\n    ![图1. libjpeg-turbo编译产物](1.jpg)\n\n## 编译libyuv\n1. 将之前下载的libyuv的源码导入到Android Studio中的指定目录：PROJECT/app/jni/libyuv,该目录自己定义就可以。\n\n2. 将上一步生成的so文件按照ABI的格式复制到libyuv/libs目录下，如下图所示：\n\n    ![图2](2.png)\n\n3. 修改libyuv目录下的Android.mk文件，[libyuv/Android.mk](https://github.com/wangwei1237/libyuv-with-jpeg/blob/master/myapp/jni/libyuv/Android.mk)文件中的第4~12行的内容即可。\n\n    ```\n    ########################################################\n    ## {{BEGIN 增加如下的代码\n    include $(CLEAR_VARS)\n    LOCAL_MODULE := jpeg\n    LOCAL_SRC_FILES := libs/$(TARGET_ARCH_ABI)/libjpeg.so\n    LOCAL_EXPORT_C_INCLUDES := $(LOCAL_PATH)/include\n    include $(PREBUILT_SHARED_LIBRARY)\n    ## END}}\n    ########################################################\n    ```\n\n4. 按照[仓库](https://github.com/wangwei1237/libyuv-with-jpeg)所示的[myapp/jni](https://github.com/wangwei1237/libyuv-with-jpeg/tree/master/myapp/jni)目录下的*Android.mk*和*Application.mk*文件修改自己代码中的对应文件即可，主要是用于编译libyuv使用。\n\n5. 修改自己app目录下的[build.gradle](https://github.com/wangwei1237/libyuv-with-jpeg/blob/master/myapp/build.gradle)，在android{}中增加如下编译任务\n\n    ```\n    externalNativeBuild {\n        ndkBuild {\n            path file('jni/Android.mk')\n        }\n    }\n    ```\n\n6. 点击Android Studio的编译按钮编译即可，编译后生成的编译产物如下图所示：\n\n    ![图3. libyuv的编译产物](3.jpg)","slug":"compile-libyuv-for-Android","published":1,"updated":"2020-03-31T05:40:04.165Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9kyh06v00061pdbfxoe5yu0","content":"<p>libyuv是Google开源的实现各种YUV格式与RGB格式之间相互转换、旋转、缩放的库。关于libyuv的具体介绍可以参考<a href=\"https://chromium.googlesource.com/libyuv/libyuv\" target=\"_blank\" rel=\"noopener\">官网介绍</a>。</p>\n<a id=\"more\"></a>\n\n<h2 id=\"Android-Studio编译libyuv\"><a href=\"#Android-Studio编译libyuv\" class=\"headerlink\" title=\"Android Studio编译libyuv\"></a>Android Studio编译libyuv</h2><p>在自己编译libyuv的过程中，遇到了很多坑。在解决这些坑的过程中发现，从网上查询的很多资料并没有对问题的解决提供帮助。因此，在自己趟过了所有的坑之后，就想把编译libyuv的过程整理了出来，一方面是做一个梳理和总结，另一方面也希望帮助更多有类似需求的人。</p>\n<p>接下来，开始介绍在Android平台上，利用NKD编译包含libjpeg库的libyuv库。这个主要通过<a href=\"https://chromium.googlesource.com/libyuv/libyuv/+/refs/heads/master/Android.mk\" target=\"_blank\" rel=\"noopener\">libyuv/Android.mk</a>中的如下代码实现：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include $(CLEAR_VARS)</span><br><span class=\"line\"></span><br><span class=\"line\">LOCAL_WHOLE_STATIC_LIBRARIES :&#x3D; libyuv_static</span><br><span class=\"line\">LOCAL_MODULE :&#x3D; libyuv</span><br><span class=\"line\">ifneq ($(LIBYUV_DISABLE_JPEG), &quot;yes&quot;)</span><br><span class=\"line\">LOCAL_SHARED_LIBRARIES :&#x3D; jpeg</span><br><span class=\"line\">endif</span><br><span class=\"line\"></span><br><span class=\"line\">include $(BUILD_SHARED_LIBRARY)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"需要下载的库\"><a href=\"#需要下载的库\" class=\"headerlink\" title=\"需要下载的库\"></a>需要下载的库</h2><ul>\n<li><a href=\"https://chromium.googlesource.com/libyuv/libyuv\" target=\"_blank\" rel=\"noopener\">libyuv</a>或者<a href=\"https//github.com/lemenkov/libyuv.git\">libyuv</a></li>\n<li><a href=\"https://github.com/libjpeg-turbo/libjpeg-turbo.git\" target=\"_blank\" rel=\"noopener\">libjpeg-turbo</a></li>\n</ul>\n<h2 id=\"编译依赖\"><a href=\"#编译依赖\" class=\"headerlink\" title=\"编译依赖\"></a>编译依赖</h2><h3 id=\"NDK\"><a href=\"#NDK\" class=\"headerlink\" title=\"NDK\"></a>NDK</h3><p>因为NDK-r17之后的版本，不支持gcc编译so，需要使用clang来编译so库。因此编译时，需要确定好自己的NDK版本。</p>\n<h3 id=\"CMAKE-NASM-GCC等\"><a href=\"#CMAKE-NASM-GCC等\" class=\"headerlink\" title=\"CMAKE, NASM, GCC等\"></a>CMAKE, NASM, GCC等</h3><p>该部分编译依赖为编译libjpeg-turbo库的编译依赖，具体可以参见libjpeg-turbo的<a href=\"https://github.com/libjpeg-turbo/libjpeg-turbo/blob/master/BUILDING.md\" target=\"_blank\" rel=\"noopener\">BUILD.md</a></p>\n<h2 id=\"编译libjpeg-turbo\"><a href=\"#编译libjpeg-turbo\" class=\"headerlink\" title=\"编译libjpeg-turbo\"></a>编译libjpeg-turbo</h2><ol>\n<li><p>下载编译脚本，编译脚本位于<a href=\"https://github.com/wangwei1237/libyuv-with-jpeg/tree/master/libjpeg-turbo\" target=\"_blank\" rel=\"noopener\">libjpeg-turbo</a>目录下，共计有三个脚本：</p>\n<ol>\n<li>config.sh：配置编译参数，例如ANDROID_NDK_ROOT等，需要根据自己的环境变量进行替换。</li>\n<li>build_jpeg.sh：编译某个CPU架构的so库，该脚本中的变量一般不需要修改。</li>\n<li>build_jpeg_all.sh：编译所有CPU架构的so库</li>\n</ol>\n</li>\n<li><p>下载<a href=\"https://github.com/libjpeg-turbo/libjpeg-turbo\" target=\"_blank\" rel=\"noopener\">libjpeg-turbo</a>源码：<code>git clone https://github.com/libjpeg-turbo/libjpeg-turbo.git</code>。</p>\n</li>\n<li><p>将第<em>1</em>步中下载到的三个文件复制到libjepg-turob源码的根目录下：<code>cp -r libjpeg-turbo/* JPEG_SRC_ROOT_PATH/</code>。</p>\n</li>\n<li><p>修改<em>config.sh</em>中的<code>ANDROID_NDK_ROOT</code>为自己的NDK路径，修改<code>ANDROID_NDK_VERSION</code>为对应的数字版本。例如<em>r16b</em>版本的为<code>ANDROID_NDK_VERSION=16</code>。</p>\n</li>\n<li><p>运行<code>sh build_jpeg_all.sh</code>编译libjpeg-turbo，运行之后会在当前目录的<em>libs</em>目录下生成各种CPU架构对应的so库。如下图所示：</p>\n<p> <img src=\"/2020/03/30/compile-libyuv-for-Android/1.jpg\" alt=\"图1. libjpeg-turbo编译产物\"></p>\n</li>\n</ol>\n<h2 id=\"编译libyuv\"><a href=\"#编译libyuv\" class=\"headerlink\" title=\"编译libyuv\"></a>编译libyuv</h2><ol>\n<li><p>将之前下载的libyuv的源码导入到Android Studio中的指定目录：PROJECT/app/jni/libyuv,该目录自己定义就可以。</p>\n</li>\n<li><p>将上一步生成的so文件按照ABI的格式复制到libyuv/libs目录下，如下图所示：</p>\n<p> <img src=\"/2020/03/30/compile-libyuv-for-Android/2.png\" alt=\"图2\"></p>\n</li>\n<li><p>修改libyuv目录下的Android.mk文件，<a href=\"https://github.com/wangwei1237/libyuv-with-jpeg/blob/master/myapp/jni/libyuv/Android.mk\" target=\"_blank\" rel=\"noopener\">libyuv/Android.mk</a>文件中的第4~12行的内容即可。</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">########################################################</span><br><span class=\"line\">## &#123;&#123;BEGIN 增加如下的代码</span><br><span class=\"line\">include $(CLEAR_VARS)</span><br><span class=\"line\">LOCAL_MODULE :&#x3D; jpeg</span><br><span class=\"line\">LOCAL_SRC_FILES :&#x3D; libs&#x2F;$(TARGET_ARCH_ABI)&#x2F;libjpeg.so</span><br><span class=\"line\">LOCAL_EXPORT_C_INCLUDES :&#x3D; $(LOCAL_PATH)&#x2F;include</span><br><span class=\"line\">include $(PREBUILT_SHARED_LIBRARY)</span><br><span class=\"line\">## END&#125;&#125;</span><br><span class=\"line\">########################################################</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>按照<a href=\"https://github.com/wangwei1237/libyuv-with-jpeg\" target=\"_blank\" rel=\"noopener\">仓库</a>所示的<a href=\"https://github.com/wangwei1237/libyuv-with-jpeg/tree/master/myapp/jni\" target=\"_blank\" rel=\"noopener\">myapp/jni</a>目录下的<em>Android.mk</em>和<em>Application.mk</em>文件修改自己代码中的对应文件即可，主要是用于编译libyuv使用。</p>\n</li>\n<li><p>修改自己app目录下的<a href=\"https://github.com/wangwei1237/libyuv-with-jpeg/blob/master/myapp/build.gradle\" target=\"_blank\" rel=\"noopener\">build.gradle</a>，在android{}中增加如下编译任务</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">externalNativeBuild &#123;</span><br><span class=\"line\">    ndkBuild &#123;</span><br><span class=\"line\">        path file(&#39;jni&#x2F;Android.mk&#39;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>点击Android Studio的编译按钮编译即可，编译后生成的编译产物如下图所示：</p>\n<p> <img src=\"/2020/03/30/compile-libyuv-for-Android/3.jpg\" alt=\"图3. libyuv的编译产物\"></p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>libyuv是Google开源的实现各种YUV格式与RGB格式之间相互转换、旋转、缩放的库。关于libyuv的具体介绍可以参考<a href=\"https://chromium.googlesource.com/libyuv/libyuv\" target=\"_blank\" rel=\"noopener\">官网介绍</a>。</p>","more":"<h2 id=\"Android-Studio编译libyuv\"><a href=\"#Android-Studio编译libyuv\" class=\"headerlink\" title=\"Android Studio编译libyuv\"></a>Android Studio编译libyuv</h2><p>在自己编译libyuv的过程中，遇到了很多坑。在解决这些坑的过程中发现，从网上查询的很多资料并没有对问题的解决提供帮助。因此，在自己趟过了所有的坑之后，就想把编译libyuv的过程整理了出来，一方面是做一个梳理和总结，另一方面也希望帮助更多有类似需求的人。</p>\n<p>接下来，开始介绍在Android平台上，利用NKD编译包含libjpeg库的libyuv库。这个主要通过<a href=\"https://chromium.googlesource.com/libyuv/libyuv/+/refs/heads/master/Android.mk\" target=\"_blank\" rel=\"noopener\">libyuv/Android.mk</a>中的如下代码实现：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include $(CLEAR_VARS)</span><br><span class=\"line\"></span><br><span class=\"line\">LOCAL_WHOLE_STATIC_LIBRARIES :&#x3D; libyuv_static</span><br><span class=\"line\">LOCAL_MODULE :&#x3D; libyuv</span><br><span class=\"line\">ifneq ($(LIBYUV_DISABLE_JPEG), &quot;yes&quot;)</span><br><span class=\"line\">LOCAL_SHARED_LIBRARIES :&#x3D; jpeg</span><br><span class=\"line\">endif</span><br><span class=\"line\"></span><br><span class=\"line\">include $(BUILD_SHARED_LIBRARY)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"需要下载的库\"><a href=\"#需要下载的库\" class=\"headerlink\" title=\"需要下载的库\"></a>需要下载的库</h2><ul>\n<li><a href=\"https://chromium.googlesource.com/libyuv/libyuv\" target=\"_blank\" rel=\"noopener\">libyuv</a>或者<a href=\"https//github.com/lemenkov/libyuv.git\">libyuv</a></li>\n<li><a href=\"https://github.com/libjpeg-turbo/libjpeg-turbo.git\" target=\"_blank\" rel=\"noopener\">libjpeg-turbo</a></li>\n</ul>\n<h2 id=\"编译依赖\"><a href=\"#编译依赖\" class=\"headerlink\" title=\"编译依赖\"></a>编译依赖</h2><h3 id=\"NDK\"><a href=\"#NDK\" class=\"headerlink\" title=\"NDK\"></a>NDK</h3><p>因为NDK-r17之后的版本，不支持gcc编译so，需要使用clang来编译so库。因此编译时，需要确定好自己的NDK版本。</p>\n<h3 id=\"CMAKE-NASM-GCC等\"><a href=\"#CMAKE-NASM-GCC等\" class=\"headerlink\" title=\"CMAKE, NASM, GCC等\"></a>CMAKE, NASM, GCC等</h3><p>该部分编译依赖为编译libjpeg-turbo库的编译依赖，具体可以参见libjpeg-turbo的<a href=\"https://github.com/libjpeg-turbo/libjpeg-turbo/blob/master/BUILDING.md\" target=\"_blank\" rel=\"noopener\">BUILD.md</a></p>\n<h2 id=\"编译libjpeg-turbo\"><a href=\"#编译libjpeg-turbo\" class=\"headerlink\" title=\"编译libjpeg-turbo\"></a>编译libjpeg-turbo</h2><ol>\n<li><p>下载编译脚本，编译脚本位于<a href=\"https://github.com/wangwei1237/libyuv-with-jpeg/tree/master/libjpeg-turbo\" target=\"_blank\" rel=\"noopener\">libjpeg-turbo</a>目录下，共计有三个脚本：</p>\n<ol>\n<li>config.sh：配置编译参数，例如ANDROID_NDK_ROOT等，需要根据自己的环境变量进行替换。</li>\n<li>build_jpeg.sh：编译某个CPU架构的so库，该脚本中的变量一般不需要修改。</li>\n<li>build_jpeg_all.sh：编译所有CPU架构的so库</li>\n</ol>\n</li>\n<li><p>下载<a href=\"https://github.com/libjpeg-turbo/libjpeg-turbo\" target=\"_blank\" rel=\"noopener\">libjpeg-turbo</a>源码：<code>git clone https://github.com/libjpeg-turbo/libjpeg-turbo.git</code>。</p>\n</li>\n<li><p>将第<em>1</em>步中下载到的三个文件复制到libjepg-turob源码的根目录下：<code>cp -r libjpeg-turbo/* JPEG_SRC_ROOT_PATH/</code>。</p>\n</li>\n<li><p>修改<em>config.sh</em>中的<code>ANDROID_NDK_ROOT</code>为自己的NDK路径，修改<code>ANDROID_NDK_VERSION</code>为对应的数字版本。例如<em>r16b</em>版本的为<code>ANDROID_NDK_VERSION=16</code>。</p>\n</li>\n<li><p>运行<code>sh build_jpeg_all.sh</code>编译libjpeg-turbo，运行之后会在当前目录的<em>libs</em>目录下生成各种CPU架构对应的so库。如下图所示：</p>\n<p> <img src=\"/2020/03/30/compile-libyuv-for-Android/1.jpg\" alt=\"图1. libjpeg-turbo编译产物\"></p>\n</li>\n</ol>\n<h2 id=\"编译libyuv\"><a href=\"#编译libyuv\" class=\"headerlink\" title=\"编译libyuv\"></a>编译libyuv</h2><ol>\n<li><p>将之前下载的libyuv的源码导入到Android Studio中的指定目录：PROJECT/app/jni/libyuv,该目录自己定义就可以。</p>\n</li>\n<li><p>将上一步生成的so文件按照ABI的格式复制到libyuv/libs目录下，如下图所示：</p>\n<p> <img src=\"/2020/03/30/compile-libyuv-for-Android/2.png\" alt=\"图2\"></p>\n</li>\n<li><p>修改libyuv目录下的Android.mk文件，<a href=\"https://github.com/wangwei1237/libyuv-with-jpeg/blob/master/myapp/jni/libyuv/Android.mk\" target=\"_blank\" rel=\"noopener\">libyuv/Android.mk</a>文件中的第4~12行的内容即可。</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">########################################################</span><br><span class=\"line\">## &#123;&#123;BEGIN 增加如下的代码</span><br><span class=\"line\">include $(CLEAR_VARS)</span><br><span class=\"line\">LOCAL_MODULE :&#x3D; jpeg</span><br><span class=\"line\">LOCAL_SRC_FILES :&#x3D; libs&#x2F;$(TARGET_ARCH_ABI)&#x2F;libjpeg.so</span><br><span class=\"line\">LOCAL_EXPORT_C_INCLUDES :&#x3D; $(LOCAL_PATH)&#x2F;include</span><br><span class=\"line\">include $(PREBUILT_SHARED_LIBRARY)</span><br><span class=\"line\">## END&#125;&#125;</span><br><span class=\"line\">########################################################</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>按照<a href=\"https://github.com/wangwei1237/libyuv-with-jpeg\" target=\"_blank\" rel=\"noopener\">仓库</a>所示的<a href=\"https://github.com/wangwei1237/libyuv-with-jpeg/tree/master/myapp/jni\" target=\"_blank\" rel=\"noopener\">myapp/jni</a>目录下的<em>Android.mk</em>和<em>Application.mk</em>文件修改自己代码中的对应文件即可，主要是用于编译libyuv使用。</p>\n</li>\n<li><p>修改自己app目录下的<a href=\"https://github.com/wangwei1237/libyuv-with-jpeg/blob/master/myapp/build.gradle\" target=\"_blank\" rel=\"noopener\">build.gradle</a>，在android{}中增加如下编译任务</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">externalNativeBuild &#123;</span><br><span class=\"line\">    ndkBuild &#123;</span><br><span class=\"line\">        path file(&#39;jni&#x2F;Android.mk&#39;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>点击Android Studio的编译按钮编译即可，编译后生成的编译产物如下图所示：</p>\n<p> <img src=\"/2020/03/30/compile-libyuv-for-Android/3.jpg\" alt=\"图3. libyuv的编译产物\"></p>\n</li>\n</ol>"},{"title":"macOS Catalina编译的FFMpeg运行crash","reward":false,"top":false,"date":"2020-04-08T01:23:19.000Z","_content":"\n因为近期正在调研&了解[SRT协议](https://github.com/Haivision/srt)的相关内容，为了方便，需要打开FFMpeg的`--enable-libsrt`功能并重新编译FFMpeg，从而保证FFMpeg支持SRT协议。但是重新编译之后却发现启动ffmpeg工具就会被内核杀死，具体如下所示。\n\n![](1.jpg)\n<!--more-->\n\n利用*lldb*调试该*ffmpeg*发现直接提示`error: Malformed Mach-o file`的错误，具体如下：\n\n![](2.jpg)\n\n编译异常的Mac版本和Xcode版本信息分别如下：\n\n![](3.jpg)\n\n查阅了[网上的资料](https://trac.ffmpeg.org/ticket/8073)，也查看了[brew编译FFMpeg的指令](https://github.com/Homebrew/homebrew-core/blob/master/Formula/ffmpeg.rb)，原因可能是：**Xcode11下clang默认开启-fstack-check**。\n\n同时，在如下图所示的Xcode 10.3版本的Mac上则可编译成功：\n\n![](4.jpg)\n\n因此根据如上信息：`./configure`时需要增加`--host-cflags=-fno-stack-check`配置。\n\n但是增加该配置之后，编译出的ffmpeg依然无法正常运行。\n\n**暂时先将这个问题记录下来，等有时间了再彻底追查一下。**\n","source":"_posts/ffmpeg-compiled-in-macOS-Catalina-runs-crash.md","raw":"---\ntitle: macOS Catalina编译的FFMpeg运行crash\nreward: false\ntop: false\ndate: 2020-04-08 09:23:19\ncategories: \n  - 视频技术\ntags:\n  - FFMpeg\n  - 编译\n  - macOS Catalina\n---\n\n因为近期正在调研&了解[SRT协议](https://github.com/Haivision/srt)的相关内容，为了方便，需要打开FFMpeg的`--enable-libsrt`功能并重新编译FFMpeg，从而保证FFMpeg支持SRT协议。但是重新编译之后却发现启动ffmpeg工具就会被内核杀死，具体如下所示。\n\n![](1.jpg)\n<!--more-->\n\n利用*lldb*调试该*ffmpeg*发现直接提示`error: Malformed Mach-o file`的错误，具体如下：\n\n![](2.jpg)\n\n编译异常的Mac版本和Xcode版本信息分别如下：\n\n![](3.jpg)\n\n查阅了[网上的资料](https://trac.ffmpeg.org/ticket/8073)，也查看了[brew编译FFMpeg的指令](https://github.com/Homebrew/homebrew-core/blob/master/Formula/ffmpeg.rb)，原因可能是：**Xcode11下clang默认开启-fstack-check**。\n\n同时，在如下图所示的Xcode 10.3版本的Mac上则可编译成功：\n\n![](4.jpg)\n\n因此根据如上信息：`./configure`时需要增加`--host-cflags=-fno-stack-check`配置。\n\n但是增加该配置之后，编译出的ffmpeg依然无法正常运行。\n\n**暂时先将这个问题记录下来，等有时间了再彻底追查一下。**\n","slug":"ffmpeg-compiled-in-macOS-Catalina-runs-crash","published":1,"updated":"2020-04-08T06:56:58.487Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9kyh06x00071pdbatp45mr2","content":"<p>因为近期正在调研&amp;了解<a href=\"https://github.com/Haivision/srt\" target=\"_blank\" rel=\"noopener\">SRT协议</a>的相关内容，为了方便，需要打开FFMpeg的<code>--enable-libsrt</code>功能并重新编译FFMpeg，从而保证FFMpeg支持SRT协议。但是重新编译之后却发现启动ffmpeg工具就会被内核杀死，具体如下所示。</p>\n<p><img src=\"/2020/04/08/ffmpeg-compiled-in-macOS-Catalina-runs-crash/1.jpg\" alt></p>\n<a id=\"more\"></a>\n\n<p>利用<em>lldb</em>调试该<em>ffmpeg</em>发现直接提示<code>error: Malformed Mach-o file</code>的错误，具体如下：</p>\n<p><img src=\"/2020/04/08/ffmpeg-compiled-in-macOS-Catalina-runs-crash/2.jpg\" alt></p>\n<p>编译异常的Mac版本和Xcode版本信息分别如下：</p>\n<p><img src=\"/2020/04/08/ffmpeg-compiled-in-macOS-Catalina-runs-crash/3.jpg\" alt></p>\n<p>查阅了<a href=\"https://trac.ffmpeg.org/ticket/8073\" target=\"_blank\" rel=\"noopener\">网上的资料</a>，也查看了<a href=\"https://github.com/Homebrew/homebrew-core/blob/master/Formula/ffmpeg.rb\" target=\"_blank\" rel=\"noopener\">brew编译FFMpeg的指令</a>，原因可能是：<strong>Xcode11下clang默认开启-fstack-check</strong>。</p>\n<p>同时，在如下图所示的Xcode 10.3版本的Mac上则可编译成功：</p>\n<p><img src=\"/2020/04/08/ffmpeg-compiled-in-macOS-Catalina-runs-crash/4.jpg\" alt></p>\n<p>因此根据如上信息：<code>./configure</code>时需要增加<code>--host-cflags=-fno-stack-check</code>配置。</p>\n<p>但是增加该配置之后，编译出的ffmpeg依然无法正常运行。</p>\n<p><strong>暂时先将这个问题记录下来，等有时间了再彻底追查一下。</strong></p>\n","site":{"data":{}},"excerpt":"<p>因为近期正在调研&amp;了解<a href=\"https://github.com/Haivision/srt\" target=\"_blank\" rel=\"noopener\">SRT协议</a>的相关内容，为了方便，需要打开FFMpeg的<code>--enable-libsrt</code>功能并重新编译FFMpeg，从而保证FFMpeg支持SRT协议。但是重新编译之后却发现启动ffmpeg工具就会被内核杀死，具体如下所示。</p>\n<p><img src=\"/2020/04/08/ffmpeg-compiled-in-macOS-Catalina-runs-crash/1.jpg\" alt></p>","more":"<p>利用<em>lldb</em>调试该<em>ffmpeg</em>发现直接提示<code>error: Malformed Mach-o file</code>的错误，具体如下：</p>\n<p><img src=\"/2020/04/08/ffmpeg-compiled-in-macOS-Catalina-runs-crash/2.jpg\" alt></p>\n<p>编译异常的Mac版本和Xcode版本信息分别如下：</p>\n<p><img src=\"/2020/04/08/ffmpeg-compiled-in-macOS-Catalina-runs-crash/3.jpg\" alt></p>\n<p>查阅了<a href=\"https://trac.ffmpeg.org/ticket/8073\" target=\"_blank\" rel=\"noopener\">网上的资料</a>，也查看了<a href=\"https://github.com/Homebrew/homebrew-core/blob/master/Formula/ffmpeg.rb\" target=\"_blank\" rel=\"noopener\">brew编译FFMpeg的指令</a>，原因可能是：<strong>Xcode11下clang默认开启-fstack-check</strong>。</p>\n<p>同时，在如下图所示的Xcode 10.3版本的Mac上则可编译成功：</p>\n<p><img src=\"/2020/04/08/ffmpeg-compiled-in-macOS-Catalina-runs-crash/4.jpg\" alt></p>\n<p>因此根据如上信息：<code>./configure</code>时需要增加<code>--host-cflags=-fno-stack-check</code>配置。</p>\n<p>但是增加该配置之后，编译出的ffmpeg依然无法正常运行。</p>\n<p><strong>暂时先将这个问题记录下来，等有时间了再彻底追查一下。</strong></p>"},{"title":"数字视频基本概念","reward":false,"date":"2020-03-03T02:20:19.000Z","_content":"\n在学习和研究视频技术的过程中，联合几个同事一起翻译了[Digital Video Concepts, Methods, and Metrics: Quality, Compression, Performance, and Power Trade-off Analysis](https://link.springer.com/book/10.1007/978-1-4302-6713-3)，形成了[**数字视频概念，方法和测量指标：质量，压缩，性能和电量等的权衡分析（中文版）**](/digital-video-concept)。该翻译版本仅供学习交流之用。希望对想学习和了解数字视频相关技术的同学，有所帮助。\n\n在翻译的过程中，尽可能的保留了原书的内容，并且对原书中的个别内容进行了修正和补充。\n\n<!--more-->\n\n![](/digital-video-concept/images/cover_0.jpg)\n\n该书在翻译的过程中采用gitbook方式编写，可以利用gitbook将该项目打包成静态网页格式或者PDF\n格式，具体可以参见：[gitbook教程](https://einverne.github.io/gitbook-tutorial/output/static.html)。\n\n项目主页地址：[github仓库](https://github.com/wangwei1237/digital_video_concepts.git)\n\n在线预览地址：[预览地址1](https://wangwei1237.github.io/digital-video-concept), [预览地址2](https://wangwei1237.gitbook.io/digital_video_concepts)\n\n在阅读过程中有任何意见或建议，欢迎在github上提交issue，我们也会快速跟进。\n\n","source":"_posts/digital-video-concept.md","raw":"---\ntitle: 数字视频基本概念\nreward: false\ndate: 2020-03-03 10:20:19\ncategories: 视频技术\ntags:\n  - 数字视频\n  - digital video\n  - 视频基础知识\n---\n\n在学习和研究视频技术的过程中，联合几个同事一起翻译了[Digital Video Concepts, Methods, and Metrics: Quality, Compression, Performance, and Power Trade-off Analysis](https://link.springer.com/book/10.1007/978-1-4302-6713-3)，形成了[**数字视频概念，方法和测量指标：质量，压缩，性能和电量等的权衡分析（中文版）**](/digital-video-concept)。该翻译版本仅供学习交流之用。希望对想学习和了解数字视频相关技术的同学，有所帮助。\n\n在翻译的过程中，尽可能的保留了原书的内容，并且对原书中的个别内容进行了修正和补充。\n\n<!--more-->\n\n![](/digital-video-concept/images/cover_0.jpg)\n\n该书在翻译的过程中采用gitbook方式编写，可以利用gitbook将该项目打包成静态网页格式或者PDF\n格式，具体可以参见：[gitbook教程](https://einverne.github.io/gitbook-tutorial/output/static.html)。\n\n项目主页地址：[github仓库](https://github.com/wangwei1237/digital_video_concepts.git)\n\n在线预览地址：[预览地址1](https://wangwei1237.github.io/digital-video-concept), [预览地址2](https://wangwei1237.gitbook.io/digital_video_concepts)\n\n在阅读过程中有任何意见或建议，欢迎在github上提交issue，我们也会快速跟进。\n\n","slug":"digital-video-concept","published":1,"updated":"2020-03-30T09:30:58.261Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9kyh06z00081pdbbfh95nrm","content":"<p>在学习和研究视频技术的过程中，联合几个同事一起翻译了<a href=\"https://link.springer.com/book/10.1007/978-1-4302-6713-3\" target=\"_blank\" rel=\"noopener\">Digital Video Concepts, Methods, and Metrics: Quality, Compression, Performance, and Power Trade-off Analysis</a>，形成了<a href=\"/digital-video-concept\"><strong>数字视频概念，方法和测量指标：质量，压缩，性能和电量等的权衡分析（中文版）</strong></a>。该翻译版本仅供学习交流之用。希望对想学习和了解数字视频相关技术的同学，有所帮助。</p>\n<p>在翻译的过程中，尽可能的保留了原书的内容，并且对原书中的个别内容进行了修正和补充。</p>\n<a id=\"more\"></a>\n\n<p><img src=\"/digital-video-concept/images/cover_0.jpg\" alt></p>\n<p>该书在翻译的过程中采用gitbook方式编写，可以利用gitbook将该项目打包成静态网页格式或者PDF<br>格式，具体可以参见：<a href=\"https://einverne.github.io/gitbook-tutorial/output/static.html\" target=\"_blank\" rel=\"noopener\">gitbook教程</a>。</p>\n<p>项目主页地址：<a href=\"https://github.com/wangwei1237/digital_video_concepts.git\" target=\"_blank\" rel=\"noopener\">github仓库</a></p>\n<p>在线预览地址：<a href=\"https://wangwei1237.github.io/digital-video-concept\">预览地址1</a>, <a href=\"https://wangwei1237.gitbook.io/digital_video_concepts\" target=\"_blank\" rel=\"noopener\">预览地址2</a></p>\n<p>在阅读过程中有任何意见或建议，欢迎在github上提交issue，我们也会快速跟进。</p>\n","site":{"data":{}},"excerpt":"<p>在学习和研究视频技术的过程中，联合几个同事一起翻译了<a href=\"https://link.springer.com/book/10.1007/978-1-4302-6713-3\" target=\"_blank\" rel=\"noopener\">Digital Video Concepts, Methods, and Metrics: Quality, Compression, Performance, and Power Trade-off Analysis</a>，形成了<a href=\"/digital-video-concept\"><strong>数字视频概念，方法和测量指标：质量，压缩，性能和电量等的权衡分析（中文版）</strong></a>。该翻译版本仅供学习交流之用。希望对想学习和了解数字视频相关技术的同学，有所帮助。</p>\n<p>在翻译的过程中，尽可能的保留了原书的内容，并且对原书中的个别内容进行了修正和补充。</p>","more":"<p><img src=\"/digital-video-concept/images/cover_0.jpg\" alt></p>\n<p>该书在翻译的过程中采用gitbook方式编写，可以利用gitbook将该项目打包成静态网页格式或者PDF<br>格式，具体可以参见：<a href=\"https://einverne.github.io/gitbook-tutorial/output/static.html\" target=\"_blank\" rel=\"noopener\">gitbook教程</a>。</p>\n<p>项目主页地址：<a href=\"https://github.com/wangwei1237/digital_video_concepts.git\" target=\"_blank\" rel=\"noopener\">github仓库</a></p>\n<p>在线预览地址：<a href=\"https://wangwei1237.github.io/digital-video-concept\">预览地址1</a>, <a href=\"https://wangwei1237.gitbook.io/digital_video_concepts\" target=\"_blank\" rel=\"noopener\">预览地址2</a></p>\n<p>在阅读过程中有任何意见或建议，欢迎在github上提交issue，我们也会快速跟进。</p>"},{"title":"解决hexo-asset-image的图片地址错误问题","reward":false,"date":"2020-02-05T13:05:57.000Z","_content":"\n由于`hexo-asset-image`插件存在bug，会导致博文中引用图片时无法生成正确的链接地址，进而导致图片无法访问的现象。\n具体解决方案为将文件`node_modules/hexo-asset-image/index.js`替换为如下的内容：\n\n<!--more-->\n\n```javascript\n'use strict';\nvar cheerio = require('cheerio');\n\n// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-string\nfunction getPosition(str, m, i) {\n  return str.split(m, i).join(m).length;\n}\n\nvar version = String(hexo.version).split('.');\nhexo.extend.filter.register('after_post_render', function(data){\n  var config = hexo.config;\n  if(config.post_asset_folder){\n    \tvar link = data.permalink;\n\tif(version.length > 0 && Number(version[0]) == 3)\n\t   var beginPos = getPosition(link, '/', 1) + 1;\n\telse\n\t   var beginPos = getPosition(link, '/', 3) + 1;\n\t// In hexo 3.1.1, the permalink of \"about\" page is like \".../about/index.html\".\n\tvar endPos = link.lastIndexOf('/') + 1;\n    link = link.substring(beginPos, endPos);\n\n    var toprocess = ['excerpt', 'more', 'content'];\n    for(var i = 0; i < toprocess.length; i++){\n      var key = toprocess[i];\n \n      var $ = cheerio.load(data[key], {\n        ignoreWhitespace: false,\n        xmlMode: false,\n        lowerCaseTags: false,\n        decodeEntities: false\n      });\n\n      $('img').each(function(){\n\t\tif ($(this).attr('src')){\n\t\t\t// For windows style path, we replace '\\' to '/'.\n\t\t\tvar src = $(this).attr('src').replace('\\\\', '/');\n\t\t\tif(!/http[s]*.*|\\/\\/.*/.test(src) &&\n\t\t\t   !/^\\s*\\//.test(src)) {\n\t\t\t  // For \"about\" page, the first part of \"src\" can't be removed.\n\t\t\t  // In addition, to support multi-level local directory.\n\t\t\t  var linkArray = link.split('/').filter(function(elem){\n\t\t\t\treturn elem != '';\n\t\t\t  });\n\t\t\t  var srcArray = src.split('/').filter(function(elem){\n\t\t\t\treturn elem != '' && elem != '.';\n\t\t\t  });\n\t\t\t  if(srcArray.length > 1)\n\t\t\t\tsrcArray.shift();\n\t\t\t  src = srcArray.join('/');\n\t\t\t  $(this).attr('src', config.root + link + src);\n\t\t\t  console.info&&console.info(\"update link as:-->\"+config.root + link + src);\n\t\t\t}\n\t\t}else{\n\t\t\tconsole.info&&console.info(\"no src attr, skipped...\");\n\t\t\tconsole.info&&console.info($(this));\n\t\t}\n      });\n      data[key] = $.html();\n    }\n  }\n});\n```","source":"_posts/handle-the-bug-of-hexo-asset-image-plugin.md","raw":"---\ntitle: 解决hexo-asset-image的图片地址错误问题\nreward: false\ndate: 2020-02-05 21:05:57\ncategories: 技术\ntags: hexo\n---\n\n由于`hexo-asset-image`插件存在bug，会导致博文中引用图片时无法生成正确的链接地址，进而导致图片无法访问的现象。\n具体解决方案为将文件`node_modules/hexo-asset-image/index.js`替换为如下的内容：\n\n<!--more-->\n\n```javascript\n'use strict';\nvar cheerio = require('cheerio');\n\n// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-string\nfunction getPosition(str, m, i) {\n  return str.split(m, i).join(m).length;\n}\n\nvar version = String(hexo.version).split('.');\nhexo.extend.filter.register('after_post_render', function(data){\n  var config = hexo.config;\n  if(config.post_asset_folder){\n    \tvar link = data.permalink;\n\tif(version.length > 0 && Number(version[0]) == 3)\n\t   var beginPos = getPosition(link, '/', 1) + 1;\n\telse\n\t   var beginPos = getPosition(link, '/', 3) + 1;\n\t// In hexo 3.1.1, the permalink of \"about\" page is like \".../about/index.html\".\n\tvar endPos = link.lastIndexOf('/') + 1;\n    link = link.substring(beginPos, endPos);\n\n    var toprocess = ['excerpt', 'more', 'content'];\n    for(var i = 0; i < toprocess.length; i++){\n      var key = toprocess[i];\n \n      var $ = cheerio.load(data[key], {\n        ignoreWhitespace: false,\n        xmlMode: false,\n        lowerCaseTags: false,\n        decodeEntities: false\n      });\n\n      $('img').each(function(){\n\t\tif ($(this).attr('src')){\n\t\t\t// For windows style path, we replace '\\' to '/'.\n\t\t\tvar src = $(this).attr('src').replace('\\\\', '/');\n\t\t\tif(!/http[s]*.*|\\/\\/.*/.test(src) &&\n\t\t\t   !/^\\s*\\//.test(src)) {\n\t\t\t  // For \"about\" page, the first part of \"src\" can't be removed.\n\t\t\t  // In addition, to support multi-level local directory.\n\t\t\t  var linkArray = link.split('/').filter(function(elem){\n\t\t\t\treturn elem != '';\n\t\t\t  });\n\t\t\t  var srcArray = src.split('/').filter(function(elem){\n\t\t\t\treturn elem != '' && elem != '.';\n\t\t\t  });\n\t\t\t  if(srcArray.length > 1)\n\t\t\t\tsrcArray.shift();\n\t\t\t  src = srcArray.join('/');\n\t\t\t  $(this).attr('src', config.root + link + src);\n\t\t\t  console.info&&console.info(\"update link as:-->\"+config.root + link + src);\n\t\t\t}\n\t\t}else{\n\t\t\tconsole.info&&console.info(\"no src attr, skipped...\");\n\t\t\tconsole.info&&console.info($(this));\n\t\t}\n      });\n      data[key] = $.html();\n    }\n  }\n});\n```","slug":"handle-the-bug-of-hexo-asset-image-plugin","published":1,"updated":"2020-02-05T13:19:39.931Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9kyh071000b1pdb1nb3281z","content":"<p>由于<code>hexo-asset-image</code>插件存在bug，会导致博文中引用图片时无法生成正确的链接地址，进而导致图片无法访问的现象。<br>具体解决方案为将文件<code>node_modules/hexo-asset-image/index.js</code>替换为如下的内容：</p>\n<a id=\"more\"></a>\n\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">'use strict'</span>;</span><br><span class=\"line\"><span class=\"keyword\">var</span> cheerio = <span class=\"built_in\">require</span>(<span class=\"string\">'cheerio'</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-string</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">getPosition</span>(<span class=\"params\">str, m, i</span>) </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> str.split(m, i).join(m).length;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> version = <span class=\"built_in\">String</span>(hexo.version).split(<span class=\"string\">'.'</span>);</span><br><span class=\"line\">hexo.extend.filter.register(<span class=\"string\">'after_post_render'</span>, <span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">data</span>)</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> config = hexo.config;</span><br><span class=\"line\">  <span class=\"keyword\">if</span>(config.post_asset_folder)&#123;</span><br><span class=\"line\">    \t<span class=\"keyword\">var</span> link = data.permalink;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span>(version.length &gt; <span class=\"number\">0</span> &amp;&amp; <span class=\"built_in\">Number</span>(version[<span class=\"number\">0</span>]) == <span class=\"number\">3</span>)</span><br><span class=\"line\">\t   <span class=\"keyword\">var</span> beginPos = getPosition(link, <span class=\"string\">'/'</span>, <span class=\"number\">1</span>) + <span class=\"number\">1</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">else</span></span><br><span class=\"line\">\t   <span class=\"keyword\">var</span> beginPos = getPosition(link, <span class=\"string\">'/'</span>, <span class=\"number\">3</span>) + <span class=\"number\">1</span>;</span><br><span class=\"line\">\t<span class=\"comment\">// In hexo 3.1.1, the permalink of \"about\" page is like \".../about/index.html\".</span></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> endPos = link.lastIndexOf(<span class=\"string\">'/'</span>) + <span class=\"number\">1</span>;</span><br><span class=\"line\">    link = link.substring(beginPos, endPos);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">var</span> toprocess = [<span class=\"string\">'excerpt'</span>, <span class=\"string\">'more'</span>, <span class=\"string\">'content'</span>];</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">var</span> i = <span class=\"number\">0</span>; i &lt; toprocess.length; i++)&#123;</span><br><span class=\"line\">      <span class=\"keyword\">var</span> key = toprocess[i];</span><br><span class=\"line\"> </span><br><span class=\"line\">      <span class=\"keyword\">var</span> $ = cheerio.load(data[key], &#123;</span><br><span class=\"line\">        ignoreWhitespace: <span class=\"literal\">false</span>,</span><br><span class=\"line\">        xmlMode: <span class=\"literal\">false</span>,</span><br><span class=\"line\">        lowerCaseTags: <span class=\"literal\">false</span>,</span><br><span class=\"line\">        decodeEntities: <span class=\"literal\">false</span></span><br><span class=\"line\">      &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">      $(<span class=\"string\">'img'</span>).each(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> ($(<span class=\"keyword\">this</span>).attr(<span class=\"string\">'src'</span>))&#123;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// For windows style path, we replace '\\' to '/'.</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">var</span> src = $(<span class=\"keyword\">this</span>).attr(<span class=\"string\">'src'</span>).replace(<span class=\"string\">'\\\\'</span>, <span class=\"string\">'/'</span>);</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span>(!<span class=\"regexp\">/http[s]*.*|\\/\\/.*/</span>.test(src) &amp;&amp;</span><br><span class=\"line\">\t\t\t   !<span class=\"regexp\">/^\\s*\\//</span>.test(src)) &#123;</span><br><span class=\"line\">\t\t\t  <span class=\"comment\">// For \"about\" page, the first part of \"src\" can't be removed.</span></span><br><span class=\"line\">\t\t\t  <span class=\"comment\">// In addition, to support multi-level local directory.</span></span><br><span class=\"line\">\t\t\t  <span class=\"keyword\">var</span> linkArray = link.split(<span class=\"string\">'/'</span>).filter(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">elem</span>)</span>&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> elem != <span class=\"string\">''</span>;</span><br><span class=\"line\">\t\t\t  &#125;);</span><br><span class=\"line\">\t\t\t  <span class=\"keyword\">var</span> srcArray = src.split(<span class=\"string\">'/'</span>).filter(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">elem</span>)</span>&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> elem != <span class=\"string\">''</span> &amp;&amp; elem != <span class=\"string\">'.'</span>;</span><br><span class=\"line\">\t\t\t  &#125;);</span><br><span class=\"line\">\t\t\t  <span class=\"keyword\">if</span>(srcArray.length &gt; <span class=\"number\">1</span>)</span><br><span class=\"line\">\t\t\t\tsrcArray.shift();</span><br><span class=\"line\">\t\t\t  src = srcArray.join(<span class=\"string\">'/'</span>);</span><br><span class=\"line\">\t\t\t  $(<span class=\"keyword\">this</span>).attr(<span class=\"string\">'src'</span>, config.root + link + src);</span><br><span class=\"line\">\t\t\t  <span class=\"built_in\">console</span>.info&amp;&amp;<span class=\"built_in\">console</span>.info(<span class=\"string\">\"update link as:--&gt;\"</span>+config.root + link + src);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">console</span>.info&amp;&amp;<span class=\"built_in\">console</span>.info(<span class=\"string\">\"no src attr, skipped...\"</span>);</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">console</span>.info&amp;&amp;<span class=\"built_in\">console</span>.info($(<span class=\"keyword\">this</span>));</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">      &#125;);</span><br><span class=\"line\">      data[key] = $.html();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<p>由于<code>hexo-asset-image</code>插件存在bug，会导致博文中引用图片时无法生成正确的链接地址，进而导致图片无法访问的现象。<br>具体解决方案为将文件<code>node_modules/hexo-asset-image/index.js</code>替换为如下的内容：</p>","more":"<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">'use strict'</span>;</span><br><span class=\"line\"><span class=\"keyword\">var</span> cheerio = <span class=\"built_in\">require</span>(<span class=\"string\">'cheerio'</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-string</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">getPosition</span>(<span class=\"params\">str, m, i</span>) </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> str.split(m, i).join(m).length;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> version = <span class=\"built_in\">String</span>(hexo.version).split(<span class=\"string\">'.'</span>);</span><br><span class=\"line\">hexo.extend.filter.register(<span class=\"string\">'after_post_render'</span>, <span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">data</span>)</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> config = hexo.config;</span><br><span class=\"line\">  <span class=\"keyword\">if</span>(config.post_asset_folder)&#123;</span><br><span class=\"line\">    \t<span class=\"keyword\">var</span> link = data.permalink;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span>(version.length &gt; <span class=\"number\">0</span> &amp;&amp; <span class=\"built_in\">Number</span>(version[<span class=\"number\">0</span>]) == <span class=\"number\">3</span>)</span><br><span class=\"line\">\t   <span class=\"keyword\">var</span> beginPos = getPosition(link, <span class=\"string\">'/'</span>, <span class=\"number\">1</span>) + <span class=\"number\">1</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">else</span></span><br><span class=\"line\">\t   <span class=\"keyword\">var</span> beginPos = getPosition(link, <span class=\"string\">'/'</span>, <span class=\"number\">3</span>) + <span class=\"number\">1</span>;</span><br><span class=\"line\">\t<span class=\"comment\">// In hexo 3.1.1, the permalink of \"about\" page is like \".../about/index.html\".</span></span><br><span class=\"line\">\t<span class=\"keyword\">var</span> endPos = link.lastIndexOf(<span class=\"string\">'/'</span>) + <span class=\"number\">1</span>;</span><br><span class=\"line\">    link = link.substring(beginPos, endPos);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">var</span> toprocess = [<span class=\"string\">'excerpt'</span>, <span class=\"string\">'more'</span>, <span class=\"string\">'content'</span>];</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">var</span> i = <span class=\"number\">0</span>; i &lt; toprocess.length; i++)&#123;</span><br><span class=\"line\">      <span class=\"keyword\">var</span> key = toprocess[i];</span><br><span class=\"line\"> </span><br><span class=\"line\">      <span class=\"keyword\">var</span> $ = cheerio.load(data[key], &#123;</span><br><span class=\"line\">        ignoreWhitespace: <span class=\"literal\">false</span>,</span><br><span class=\"line\">        xmlMode: <span class=\"literal\">false</span>,</span><br><span class=\"line\">        lowerCaseTags: <span class=\"literal\">false</span>,</span><br><span class=\"line\">        decodeEntities: <span class=\"literal\">false</span></span><br><span class=\"line\">      &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">      $(<span class=\"string\">'img'</span>).each(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> ($(<span class=\"keyword\">this</span>).attr(<span class=\"string\">'src'</span>))&#123;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// For windows style path, we replace '\\' to '/'.</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">var</span> src = $(<span class=\"keyword\">this</span>).attr(<span class=\"string\">'src'</span>).replace(<span class=\"string\">'\\\\'</span>, <span class=\"string\">'/'</span>);</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span>(!<span class=\"regexp\">/http[s]*.*|\\/\\/.*/</span>.test(src) &amp;&amp;</span><br><span class=\"line\">\t\t\t   !<span class=\"regexp\">/^\\s*\\//</span>.test(src)) &#123;</span><br><span class=\"line\">\t\t\t  <span class=\"comment\">// For \"about\" page, the first part of \"src\" can't be removed.</span></span><br><span class=\"line\">\t\t\t  <span class=\"comment\">// In addition, to support multi-level local directory.</span></span><br><span class=\"line\">\t\t\t  <span class=\"keyword\">var</span> linkArray = link.split(<span class=\"string\">'/'</span>).filter(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">elem</span>)</span>&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> elem != <span class=\"string\">''</span>;</span><br><span class=\"line\">\t\t\t  &#125;);</span><br><span class=\"line\">\t\t\t  <span class=\"keyword\">var</span> srcArray = src.split(<span class=\"string\">'/'</span>).filter(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">elem</span>)</span>&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> elem != <span class=\"string\">''</span> &amp;&amp; elem != <span class=\"string\">'.'</span>;</span><br><span class=\"line\">\t\t\t  &#125;);</span><br><span class=\"line\">\t\t\t  <span class=\"keyword\">if</span>(srcArray.length &gt; <span class=\"number\">1</span>)</span><br><span class=\"line\">\t\t\t\tsrcArray.shift();</span><br><span class=\"line\">\t\t\t  src = srcArray.join(<span class=\"string\">'/'</span>);</span><br><span class=\"line\">\t\t\t  $(<span class=\"keyword\">this</span>).attr(<span class=\"string\">'src'</span>, config.root + link + src);</span><br><span class=\"line\">\t\t\t  <span class=\"built_in\">console</span>.info&amp;&amp;<span class=\"built_in\">console</span>.info(<span class=\"string\">\"update link as:--&gt;\"</span>+config.root + link + src);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">console</span>.info&amp;&amp;<span class=\"built_in\">console</span>.info(<span class=\"string\">\"no src attr, skipped...\"</span>);</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">console</span>.info&amp;&amp;<span class=\"built_in\">console</span>.info($(<span class=\"keyword\">this</span>));</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">      &#125;);</span><br><span class=\"line\">      data[key] = $.html();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>"},{"title":"如何计算MS-SSIM","reward":false,"date":"2020-02-25T07:08:40.000Z","_content":"\n## SSIM的本质及其缺点\n在[FFMpeg如何计算图像的SSIM](/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/)中，详细介绍了$SSIM$的相关概念，并对FFMpeg中的$SSIM$实现做了详细的分析。$SSIM$算法基于HVS更擅长从图像中提取结构信息的事实，并且利用结构相似度来计算图像的感知质量。在Z. Wang等人的论文**Multi-scale structural similarity for image quality assessment**中也提到，$SSIM$算法要好于当时的其它的感知图像质量指标。\n\n就其本质而言，$SSIM$是一种单尺度的算法，但是实际上正确的图像尺度取决于用户的观看条件，例如显示设备的分辨率，用户的观看距离等。因此，用单尺度的$SSIM$算法来评估图像的感知质量也存在其缺点。\n\n<!--more-->\n\n## MS-SSIM的基本概念\n图像细节的可感知性取决于：\n* 图像信号的采样密度\n* 用户的观看距离\n* HVS的感知能力\n\n当如上的因素发生变化时，则对给定图像的主观评估也会随之发生变化。单尺度的SSIM算法可能仅适用于某个特定的配置。为了解决该问题，论文**Multi-scale structural similarity for image quality assessment**在$SSIM$算法的基础上提出了如图1所示的多尺度的结构相似性评估算法，即$MS-SSIM$算法。\n\n![图1. MS-SSIM算法](1.jpg)\n\n图1. MS-SSIM算法，L 表示低通滤波器，2↓ 表示采样间隔为2的下采样\n\n因此，$MS-SSIM$实际上是一种以不同分辨率合并图像细节的图像质量评估方法。对于$MS-SSIM$，原始图像的$scale=1$，图像的最大$scale=M$。对$scale=j$的尺度而言，其亮度、对比度、结构的相似性分别表示为：\n* $l_j(X,Y)$\n* $c_j(X,Y)$\n* $s_j(X,Y)$\n\n因此，根据图1可以得到$MS-SSIM$的计算方式。\n\n$$\nMS-SSIM(X,Y)=\\big[l_M(X,Y)\\big]^{\\alpha_M} \\cdot \\prod _{j=1}^{M}  {\\big[c_j(X,Y)\\big]^{\\beta_j}\\big[s_j(X,Y)\\big]^{\\gamma_j}}\n$$\n\n一般，令$\\alpha_j=\\beta_j=\\gamma_j \\ \\ , \\ j \\in [1, M]$，我们得到：\n\n$$\nMS-SSIM(X,Y)=\\big[l_M(X,Y)\\big]^{\\alpha_M} \\cdot \\prod _{j=1}^{M}  {\\big[c_j(X,Y) \\cdot s_j(X,Y)\\big]^{\\alpha_j}}\n$$\n\n**Multi-scale structural similarity for image quality assessment**给出了一种计算各尺度参数的方法，并同时给出不同尺度的参数值：\n* $\\alpha_1=0.0448$\n* $\\alpha_2=0.2856$\n* $\\alpha_3=0.3001$\n* $\\alpha_4=0.2363$\n* $\\alpha_5=0.1333$\n\n## MS-SSIM算法的实现\n采用FFMpeg中的$SSIM$的实现方式来实现$MS-SSIM$。根据[FFMpeg如何计算图像的SSIM](/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/)的介绍：\n\n$$\nMS-SSIM(X,Y)=\\bigg(\\frac{2s1 \\cdot s2 + ssimC_{1}}{s1^2+s2^2+ssimC_{1}}\\bigg)^{\\alpha_{M}} \\cdot \\prod _{j=1}^{M}\\bigg(\\frac{2covar + ssimC_2}{vars + ssimC_2}\\bigg)^{\\alpha_j}\n$$\n\n对于图像采样而言，采用简单的$2 \\times 2$的卷积核执行图像的下采样，：\n\n$$\n\\begin{bmatrix}\n\\frac{1}{4} & \\frac{1}{4} \\\\\\\\\n\\frac{1}{4} & \\frac{1}{4} \\\\\\\\\n\\end{bmatrix}\n$$\n\n具体的采样代码如下所示：\n\n```\nvoid downsample_2x2_mean(pixel *input, int width, int height, pixel *output) {\n    int downsample_width =  width >> 1;\n    int downsample_height = height >> 1;\n\n    for (int y = 0; y < downsample_height; y++) {\n        for (int x =0; x < downsample_width; x++) {\n            output[y * downsample_width + x] = (input[2 * y * width + 2 * x] +\n                                                input[2 * y * width + 2 * x + 1] +\n                                                input[(2 * y + 1) * width + 2 * x] +\n                                                input[(2 * y + 1) * width + 2 * x + 1]) / 4;\n        }\n    }\n}\n```\n\n然后利用[tiny_ssim](https://github.com/FFmpeg/FFmpeg/blob/master/tests/tiny_ssim.c)中的`ssim_plane()`迭代计算$MS-SSIM$。\n\n```\nfloat ms_ssim_plane(pixel *pix1, pixel *pix2, int width, int height, int scale) {\n    for (int i = 0; i < w * h; i++) {\n        ori_img1[i] = pix1[i];\n        ori_img2[i] = pix2[i];\n    }\n\n    // 计算每个尺度的ssim值.\n    for (int i = 1; i <= scale; i++) {\n        if (i != 1) {\n            downsample_2x2_mean(ori_img1, w, h, sample_img1);\n            downsample_2x2_mean(ori_img2, w, h, sample_img2);\n            w = w >> 1;\n            h = h >> 1;\n            for (int j = 0; j < w * h; j++) {\n                ori_img1[j] = sample_img1[j];\n                ori_img2[j] = sample_img2[j];\n            }\n        }\n        value = ssim_plane(ori_img1, w, ori_img2, w, w, h, temp, NULL);\n        result *= pow(value.C_S, WEIGHT[i-1]);\n        luminance_value[i-1] = value.L;\n    }\n\n    result *= pow(luminance_value[scale-1], WEIGHT[scale-1]);\n    return result;\n}\n```\n\n完整的代码可以参考[test_msssim.cpp](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/25/how-to-calculate-the-MS-SSIM/test_msssim.cpp)。代码的很大部分是由我的同事*贤杰*(github: [@bodhisatan](https://github.com/bodhisatan))实现的，在此一并表示感谢。\n","source":"_posts/how-to-calculate-the-MS-SSIM.md","raw":"---\ntitle: 如何计算MS-SSIM\nreward: false\ndate: 2020-02-25 15:08:40\ncategories: 视频技术\ntags:\n  - SSIM\n  - MS-SSIM\n---\n\n## SSIM的本质及其缺点\n在[FFMpeg如何计算图像的SSIM](/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/)中，详细介绍了$SSIM$的相关概念，并对FFMpeg中的$SSIM$实现做了详细的分析。$SSIM$算法基于HVS更擅长从图像中提取结构信息的事实，并且利用结构相似度来计算图像的感知质量。在Z. Wang等人的论文**Multi-scale structural similarity for image quality assessment**中也提到，$SSIM$算法要好于当时的其它的感知图像质量指标。\n\n就其本质而言，$SSIM$是一种单尺度的算法，但是实际上正确的图像尺度取决于用户的观看条件，例如显示设备的分辨率，用户的观看距离等。因此，用单尺度的$SSIM$算法来评估图像的感知质量也存在其缺点。\n\n<!--more-->\n\n## MS-SSIM的基本概念\n图像细节的可感知性取决于：\n* 图像信号的采样密度\n* 用户的观看距离\n* HVS的感知能力\n\n当如上的因素发生变化时，则对给定图像的主观评估也会随之发生变化。单尺度的SSIM算法可能仅适用于某个特定的配置。为了解决该问题，论文**Multi-scale structural similarity for image quality assessment**在$SSIM$算法的基础上提出了如图1所示的多尺度的结构相似性评估算法，即$MS-SSIM$算法。\n\n![图1. MS-SSIM算法](1.jpg)\n\n图1. MS-SSIM算法，L 表示低通滤波器，2↓ 表示采样间隔为2的下采样\n\n因此，$MS-SSIM$实际上是一种以不同分辨率合并图像细节的图像质量评估方法。对于$MS-SSIM$，原始图像的$scale=1$，图像的最大$scale=M$。对$scale=j$的尺度而言，其亮度、对比度、结构的相似性分别表示为：\n* $l_j(X,Y)$\n* $c_j(X,Y)$\n* $s_j(X,Y)$\n\n因此，根据图1可以得到$MS-SSIM$的计算方式。\n\n$$\nMS-SSIM(X,Y)=\\big[l_M(X,Y)\\big]^{\\alpha_M} \\cdot \\prod _{j=1}^{M}  {\\big[c_j(X,Y)\\big]^{\\beta_j}\\big[s_j(X,Y)\\big]^{\\gamma_j}}\n$$\n\n一般，令$\\alpha_j=\\beta_j=\\gamma_j \\ \\ , \\ j \\in [1, M]$，我们得到：\n\n$$\nMS-SSIM(X,Y)=\\big[l_M(X,Y)\\big]^{\\alpha_M} \\cdot \\prod _{j=1}^{M}  {\\big[c_j(X,Y) \\cdot s_j(X,Y)\\big]^{\\alpha_j}}\n$$\n\n**Multi-scale structural similarity for image quality assessment**给出了一种计算各尺度参数的方法，并同时给出不同尺度的参数值：\n* $\\alpha_1=0.0448$\n* $\\alpha_2=0.2856$\n* $\\alpha_3=0.3001$\n* $\\alpha_4=0.2363$\n* $\\alpha_5=0.1333$\n\n## MS-SSIM算法的实现\n采用FFMpeg中的$SSIM$的实现方式来实现$MS-SSIM$。根据[FFMpeg如何计算图像的SSIM](/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/)的介绍：\n\n$$\nMS-SSIM(X,Y)=\\bigg(\\frac{2s1 \\cdot s2 + ssimC_{1}}{s1^2+s2^2+ssimC_{1}}\\bigg)^{\\alpha_{M}} \\cdot \\prod _{j=1}^{M}\\bigg(\\frac{2covar + ssimC_2}{vars + ssimC_2}\\bigg)^{\\alpha_j}\n$$\n\n对于图像采样而言，采用简单的$2 \\times 2$的卷积核执行图像的下采样，：\n\n$$\n\\begin{bmatrix}\n\\frac{1}{4} & \\frac{1}{4} \\\\\\\\\n\\frac{1}{4} & \\frac{1}{4} \\\\\\\\\n\\end{bmatrix}\n$$\n\n具体的采样代码如下所示：\n\n```\nvoid downsample_2x2_mean(pixel *input, int width, int height, pixel *output) {\n    int downsample_width =  width >> 1;\n    int downsample_height = height >> 1;\n\n    for (int y = 0; y < downsample_height; y++) {\n        for (int x =0; x < downsample_width; x++) {\n            output[y * downsample_width + x] = (input[2 * y * width + 2 * x] +\n                                                input[2 * y * width + 2 * x + 1] +\n                                                input[(2 * y + 1) * width + 2 * x] +\n                                                input[(2 * y + 1) * width + 2 * x + 1]) / 4;\n        }\n    }\n}\n```\n\n然后利用[tiny_ssim](https://github.com/FFmpeg/FFmpeg/blob/master/tests/tiny_ssim.c)中的`ssim_plane()`迭代计算$MS-SSIM$。\n\n```\nfloat ms_ssim_plane(pixel *pix1, pixel *pix2, int width, int height, int scale) {\n    for (int i = 0; i < w * h; i++) {\n        ori_img1[i] = pix1[i];\n        ori_img2[i] = pix2[i];\n    }\n\n    // 计算每个尺度的ssim值.\n    for (int i = 1; i <= scale; i++) {\n        if (i != 1) {\n            downsample_2x2_mean(ori_img1, w, h, sample_img1);\n            downsample_2x2_mean(ori_img2, w, h, sample_img2);\n            w = w >> 1;\n            h = h >> 1;\n            for (int j = 0; j < w * h; j++) {\n                ori_img1[j] = sample_img1[j];\n                ori_img2[j] = sample_img2[j];\n            }\n        }\n        value = ssim_plane(ori_img1, w, ori_img2, w, w, h, temp, NULL);\n        result *= pow(value.C_S, WEIGHT[i-1]);\n        luminance_value[i-1] = value.L;\n    }\n\n    result *= pow(luminance_value[scale-1], WEIGHT[scale-1]);\n    return result;\n}\n```\n\n完整的代码可以参考[test_msssim.cpp](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/25/how-to-calculate-the-MS-SSIM/test_msssim.cpp)。代码的很大部分是由我的同事*贤杰*(github: [@bodhisatan](https://github.com/bodhisatan))实现的，在此一并表示感谢。\n","slug":"how-to-calculate-the-MS-SSIM","published":1,"updated":"2020-03-30T09:24:58.431Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9kyh072000c1pdbhrwnfyje","content":"<h2 id=\"SSIM的本质及其缺点\"><a href=\"#SSIM的本质及其缺点\" class=\"headerlink\" title=\"SSIM的本质及其缺点\"></a>SSIM的本质及其缺点</h2><p>在<a href=\"/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/\">FFMpeg如何计算图像的SSIM</a>中，详细介绍了$SSIM$的相关概念，并对FFMpeg中的$SSIM$实现做了详细的分析。$SSIM$算法基于HVS更擅长从图像中提取结构信息的事实，并且利用结构相似度来计算图像的感知质量。在Z. Wang等人的论文<strong>Multi-scale structural similarity for image quality assessment</strong>中也提到，$SSIM$算法要好于当时的其它的感知图像质量指标。</p>\n<p>就其本质而言，$SSIM$是一种单尺度的算法，但是实际上正确的图像尺度取决于用户的观看条件，例如显示设备的分辨率，用户的观看距离等。因此，用单尺度的$SSIM$算法来评估图像的感知质量也存在其缺点。</p>\n<a id=\"more\"></a>\n\n<h2 id=\"MS-SSIM的基本概念\"><a href=\"#MS-SSIM的基本概念\" class=\"headerlink\" title=\"MS-SSIM的基本概念\"></a>MS-SSIM的基本概念</h2><p>图像细节的可感知性取决于：</p>\n<ul>\n<li>图像信号的采样密度</li>\n<li>用户的观看距离</li>\n<li>HVS的感知能力</li>\n</ul>\n<p>当如上的因素发生变化时，则对给定图像的主观评估也会随之发生变化。单尺度的SSIM算法可能仅适用于某个特定的配置。为了解决该问题，论文<strong>Multi-scale structural similarity for image quality assessment</strong>在$SSIM$算法的基础上提出了如图1所示的多尺度的结构相似性评估算法，即$MS-SSIM$算法。</p>\n<p><img src=\"/2020/02/25/how-to-calculate-the-MS-SSIM/1.jpg\" alt=\"图1. MS-SSIM算法\"></p>\n<p>图1. MS-SSIM算法，L 表示低通滤波器，2↓ 表示采样间隔为2的下采样</p>\n<p>因此，$MS-SSIM$实际上是一种以不同分辨率合并图像细节的图像质量评估方法。对于$MS-SSIM$，原始图像的$scale=1$，图像的最大$scale=M$。对$scale=j$的尺度而言，其亮度、对比度、结构的相似性分别表示为：</p>\n<ul>\n<li>$l_j(X,Y)$</li>\n<li>$c_j(X,Y)$</li>\n<li>$s_j(X,Y)$</li>\n</ul>\n<p>因此，根据图1可以得到$MS-SSIM$的计算方式。</p>\n<p>$$<br>MS-SSIM(X,Y)=\\big[l_M(X,Y)\\big]^{\\alpha_M} \\cdot \\prod _{j=1}^{M}  {\\big[c_j(X,Y)\\big]^{\\beta_j}\\big[s_j(X,Y)\\big]^{\\gamma_j}}<br>$$</p>\n<p>一般，令$\\alpha_j=\\beta_j=\\gamma_j \\ \\ , \\ j \\in [1, M]$，我们得到：</p>\n<p>$$<br>MS-SSIM(X,Y)=\\big[l_M(X,Y)\\big]^{\\alpha_M} \\cdot \\prod _{j=1}^{M}  {\\big[c_j(X,Y) \\cdot s_j(X,Y)\\big]^{\\alpha_j}}<br>$$</p>\n<p><strong>Multi-scale structural similarity for image quality assessment</strong>给出了一种计算各尺度参数的方法，并同时给出不同尺度的参数值：</p>\n<ul>\n<li>$\\alpha_1=0.0448$</li>\n<li>$\\alpha_2=0.2856$</li>\n<li>$\\alpha_3=0.3001$</li>\n<li>$\\alpha_4=0.2363$</li>\n<li>$\\alpha_5=0.1333$</li>\n</ul>\n<h2 id=\"MS-SSIM算法的实现\"><a href=\"#MS-SSIM算法的实现\" class=\"headerlink\" title=\"MS-SSIM算法的实现\"></a>MS-SSIM算法的实现</h2><p>采用FFMpeg中的$SSIM$的实现方式来实现$MS-SSIM$。根据<a href=\"/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/\">FFMpeg如何计算图像的SSIM</a>的介绍：</p>\n<p>$$<br>MS-SSIM(X,Y)=\\bigg(\\frac{2s1 \\cdot s2 + ssimC_{1}}{s1^2+s2^2+ssimC_{1}}\\bigg)^{\\alpha_{M}} \\cdot \\prod _{j=1}^{M}\\bigg(\\frac{2covar + ssimC_2}{vars + ssimC_2}\\bigg)^{\\alpha_j}<br>$$</p>\n<p>对于图像采样而言，采用简单的$2 \\times 2$的卷积核执行图像的下采样，：</p>\n<p>$$<br>\\begin{bmatrix}<br>\\frac{1}{4} &amp; \\frac{1}{4} \\\\<br>\\frac{1}{4} &amp; \\frac{1}{4} \\\\<br>\\end{bmatrix}<br>$$</p>\n<p>具体的采样代码如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void downsample_2x2_mean(pixel *input, int width, int height, pixel *output) &#123;</span><br><span class=\"line\">    int downsample_width &#x3D;  width &gt;&gt; 1;</span><br><span class=\"line\">    int downsample_height &#x3D; height &gt;&gt; 1;</span><br><span class=\"line\"></span><br><span class=\"line\">    for (int y &#x3D; 0; y &lt; downsample_height; y++) &#123;</span><br><span class=\"line\">        for (int x &#x3D;0; x &lt; downsample_width; x++) &#123;</span><br><span class=\"line\">            output[y * downsample_width + x] &#x3D; (input[2 * y * width + 2 * x] +</span><br><span class=\"line\">                                                input[2 * y * width + 2 * x + 1] +</span><br><span class=\"line\">                                                input[(2 * y + 1) * width + 2 * x] +</span><br><span class=\"line\">                                                input[(2 * y + 1) * width + 2 * x + 1]) &#x2F; 4;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>然后利用<a href=\"https://github.com/FFmpeg/FFmpeg/blob/master/tests/tiny_ssim.c\" target=\"_blank\" rel=\"noopener\">tiny_ssim</a>中的<code>ssim_plane()</code>迭代计算$MS-SSIM$。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">float ms_ssim_plane(pixel *pix1, pixel *pix2, int width, int height, int scale) &#123;</span><br><span class=\"line\">    for (int i &#x3D; 0; i &lt; w * h; i++) &#123;</span><br><span class=\"line\">        ori_img1[i] &#x3D; pix1[i];</span><br><span class=\"line\">        ori_img2[i] &#x3D; pix2[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; 计算每个尺度的ssim值.</span><br><span class=\"line\">    for (int i &#x3D; 1; i &lt;&#x3D; scale; i++) &#123;</span><br><span class=\"line\">        if (i !&#x3D; 1) &#123;</span><br><span class=\"line\">            downsample_2x2_mean(ori_img1, w, h, sample_img1);</span><br><span class=\"line\">            downsample_2x2_mean(ori_img2, w, h, sample_img2);</span><br><span class=\"line\">            w &#x3D; w &gt;&gt; 1;</span><br><span class=\"line\">            h &#x3D; h &gt;&gt; 1;</span><br><span class=\"line\">            for (int j &#x3D; 0; j &lt; w * h; j++) &#123;</span><br><span class=\"line\">                ori_img1[j] &#x3D; sample_img1[j];</span><br><span class=\"line\">                ori_img2[j] &#x3D; sample_img2[j];</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        value &#x3D; ssim_plane(ori_img1, w, ori_img2, w, w, h, temp, NULL);</span><br><span class=\"line\">        result *&#x3D; pow(value.C_S, WEIGHT[i-1]);</span><br><span class=\"line\">        luminance_value[i-1] &#x3D; value.L;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    result *&#x3D; pow(luminance_value[scale-1], WEIGHT[scale-1]);</span><br><span class=\"line\">    return result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>完整的代码可以参考<a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/25/how-to-calculate-the-MS-SSIM/test_msssim.cpp\" target=\"_blank\" rel=\"noopener\">test_msssim.cpp</a>。代码的很大部分是由我的同事<em>贤杰</em>(github: <a href=\"https://github.com/bodhisatan\" target=\"_blank\" rel=\"noopener\">@bodhisatan</a>)实现的，在此一并表示感谢。</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"SSIM的本质及其缺点\"><a href=\"#SSIM的本质及其缺点\" class=\"headerlink\" title=\"SSIM的本质及其缺点\"></a>SSIM的本质及其缺点</h2><p>在<a href=\"/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/\">FFMpeg如何计算图像的SSIM</a>中，详细介绍了$SSIM$的相关概念，并对FFMpeg中的$SSIM$实现做了详细的分析。$SSIM$算法基于HVS更擅长从图像中提取结构信息的事实，并且利用结构相似度来计算图像的感知质量。在Z. Wang等人的论文<strong>Multi-scale structural similarity for image quality assessment</strong>中也提到，$SSIM$算法要好于当时的其它的感知图像质量指标。</p>\n<p>就其本质而言，$SSIM$是一种单尺度的算法，但是实际上正确的图像尺度取决于用户的观看条件，例如显示设备的分辨率，用户的观看距离等。因此，用单尺度的$SSIM$算法来评估图像的感知质量也存在其缺点。</p>","more":"<h2 id=\"MS-SSIM的基本概念\"><a href=\"#MS-SSIM的基本概念\" class=\"headerlink\" title=\"MS-SSIM的基本概念\"></a>MS-SSIM的基本概念</h2><p>图像细节的可感知性取决于：</p>\n<ul>\n<li>图像信号的采样密度</li>\n<li>用户的观看距离</li>\n<li>HVS的感知能力</li>\n</ul>\n<p>当如上的因素发生变化时，则对给定图像的主观评估也会随之发生变化。单尺度的SSIM算法可能仅适用于某个特定的配置。为了解决该问题，论文<strong>Multi-scale structural similarity for image quality assessment</strong>在$SSIM$算法的基础上提出了如图1所示的多尺度的结构相似性评估算法，即$MS-SSIM$算法。</p>\n<p><img src=\"/2020/02/25/how-to-calculate-the-MS-SSIM/1.jpg\" alt=\"图1. MS-SSIM算法\"></p>\n<p>图1. MS-SSIM算法，L 表示低通滤波器，2↓ 表示采样间隔为2的下采样</p>\n<p>因此，$MS-SSIM$实际上是一种以不同分辨率合并图像细节的图像质量评估方法。对于$MS-SSIM$，原始图像的$scale=1$，图像的最大$scale=M$。对$scale=j$的尺度而言，其亮度、对比度、结构的相似性分别表示为：</p>\n<ul>\n<li>$l_j(X,Y)$</li>\n<li>$c_j(X,Y)$</li>\n<li>$s_j(X,Y)$</li>\n</ul>\n<p>因此，根据图1可以得到$MS-SSIM$的计算方式。</p>\n<p>$$<br>MS-SSIM(X,Y)=\\big[l_M(X,Y)\\big]^{\\alpha_M} \\cdot \\prod _{j=1}^{M}  {\\big[c_j(X,Y)\\big]^{\\beta_j}\\big[s_j(X,Y)\\big]^{\\gamma_j}}<br>$$</p>\n<p>一般，令$\\alpha_j=\\beta_j=\\gamma_j \\ \\ , \\ j \\in [1, M]$，我们得到：</p>\n<p>$$<br>MS-SSIM(X,Y)=\\big[l_M(X,Y)\\big]^{\\alpha_M} \\cdot \\prod _{j=1}^{M}  {\\big[c_j(X,Y) \\cdot s_j(X,Y)\\big]^{\\alpha_j}}<br>$$</p>\n<p><strong>Multi-scale structural similarity for image quality assessment</strong>给出了一种计算各尺度参数的方法，并同时给出不同尺度的参数值：</p>\n<ul>\n<li>$\\alpha_1=0.0448$</li>\n<li>$\\alpha_2=0.2856$</li>\n<li>$\\alpha_3=0.3001$</li>\n<li>$\\alpha_4=0.2363$</li>\n<li>$\\alpha_5=0.1333$</li>\n</ul>\n<h2 id=\"MS-SSIM算法的实现\"><a href=\"#MS-SSIM算法的实现\" class=\"headerlink\" title=\"MS-SSIM算法的实现\"></a>MS-SSIM算法的实现</h2><p>采用FFMpeg中的$SSIM$的实现方式来实现$MS-SSIM$。根据<a href=\"/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/\">FFMpeg如何计算图像的SSIM</a>的介绍：</p>\n<p>$$<br>MS-SSIM(X,Y)=\\bigg(\\frac{2s1 \\cdot s2 + ssimC_{1}}{s1^2+s2^2+ssimC_{1}}\\bigg)^{\\alpha_{M}} \\cdot \\prod _{j=1}^{M}\\bigg(\\frac{2covar + ssimC_2}{vars + ssimC_2}\\bigg)^{\\alpha_j}<br>$$</p>\n<p>对于图像采样而言，采用简单的$2 \\times 2$的卷积核执行图像的下采样，：</p>\n<p>$$<br>\\begin{bmatrix}<br>\\frac{1}{4} &amp; \\frac{1}{4} \\\\<br>\\frac{1}{4} &amp; \\frac{1}{4} \\\\<br>\\end{bmatrix}<br>$$</p>\n<p>具体的采样代码如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void downsample_2x2_mean(pixel *input, int width, int height, pixel *output) &#123;</span><br><span class=\"line\">    int downsample_width &#x3D;  width &gt;&gt; 1;</span><br><span class=\"line\">    int downsample_height &#x3D; height &gt;&gt; 1;</span><br><span class=\"line\"></span><br><span class=\"line\">    for (int y &#x3D; 0; y &lt; downsample_height; y++) &#123;</span><br><span class=\"line\">        for (int x &#x3D;0; x &lt; downsample_width; x++) &#123;</span><br><span class=\"line\">            output[y * downsample_width + x] &#x3D; (input[2 * y * width + 2 * x] +</span><br><span class=\"line\">                                                input[2 * y * width + 2 * x + 1] +</span><br><span class=\"line\">                                                input[(2 * y + 1) * width + 2 * x] +</span><br><span class=\"line\">                                                input[(2 * y + 1) * width + 2 * x + 1]) &#x2F; 4;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>然后利用<a href=\"https://github.com/FFmpeg/FFmpeg/blob/master/tests/tiny_ssim.c\" target=\"_blank\" rel=\"noopener\">tiny_ssim</a>中的<code>ssim_plane()</code>迭代计算$MS-SSIM$。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">float ms_ssim_plane(pixel *pix1, pixel *pix2, int width, int height, int scale) &#123;</span><br><span class=\"line\">    for (int i &#x3D; 0; i &lt; w * h; i++) &#123;</span><br><span class=\"line\">        ori_img1[i] &#x3D; pix1[i];</span><br><span class=\"line\">        ori_img2[i] &#x3D; pix2[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; 计算每个尺度的ssim值.</span><br><span class=\"line\">    for (int i &#x3D; 1; i &lt;&#x3D; scale; i++) &#123;</span><br><span class=\"line\">        if (i !&#x3D; 1) &#123;</span><br><span class=\"line\">            downsample_2x2_mean(ori_img1, w, h, sample_img1);</span><br><span class=\"line\">            downsample_2x2_mean(ori_img2, w, h, sample_img2);</span><br><span class=\"line\">            w &#x3D; w &gt;&gt; 1;</span><br><span class=\"line\">            h &#x3D; h &gt;&gt; 1;</span><br><span class=\"line\">            for (int j &#x3D; 0; j &lt; w * h; j++) &#123;</span><br><span class=\"line\">                ori_img1[j] &#x3D; sample_img1[j];</span><br><span class=\"line\">                ori_img2[j] &#x3D; sample_img2[j];</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        value &#x3D; ssim_plane(ori_img1, w, ori_img2, w, w, h, temp, NULL);</span><br><span class=\"line\">        result *&#x3D; pow(value.C_S, WEIGHT[i-1]);</span><br><span class=\"line\">        luminance_value[i-1] &#x3D; value.L;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    result *&#x3D; pow(luminance_value[scale-1], WEIGHT[scale-1]);</span><br><span class=\"line\">    return result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>完整的代码可以参考<a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/25/how-to-calculate-the-MS-SSIM/test_msssim.cpp\" target=\"_blank\" rel=\"noopener\">test_msssim.cpp</a>。代码的很大部分是由我的同事<em>贤杰</em>(github: <a href=\"https://github.com/bodhisatan\" target=\"_blank\" rel=\"noopener\">@bodhisatan</a>)实现的，在此一并表示感谢。</p>"},{"title":"FFMpeg如何计算图像的SSIM","reward":false,"date":"2020-02-15T07:08:41.000Z","_content":"\n## SSIM基本概念\n关于$SSIM$的具体解释，此处不再介绍，具体可以参见[数字视频相关概念](https://wangwei1237.github.io/digital-video-concept/)中的[SSIM算法](https://wangwei1237.github.io/digital-video-concept/docs/4_2_2_3_StructuralSimilarityBasedApproaches.html)一节的介绍。\n\n直接给出$SSIM$的计算方法：\n$$\nSSIM(x,y)=\\frac{(2\\mu_x\\mu_y+C_1)(2\\sigma_{xy}+C_2)}{(\\mu_x^2+\\mu_y^2+C_1)(\\sigma_x^2+\\sigma_y^2+C_2)}\n$$\n\n$C_1=(K_1L)^2, C_2=(K_2L)^2$。$K_1\\ll1$，$K_2\\ll1$均为常数，计算时，一般$K_1=0.01$，$K_2=0.03$。$L$是灰度的动态范围，由图像的数据类型决定，如果数据为*uint8*，则$L=255$。\n\n<!--more-->\n\n## SSIM计算中的图像分割\n在整幅图片的跨度上，图像亮度的均值和方差变化较为剧烈；并且图像上不同区块的失真程度也有可能不同；再者人眼睛每次只能聚焦于一处，更关注局部数据而非全局数据。因此如上的$SSIM$算法不能直接作用于一整副图像。\n\n在论文**Image quality assessment: From error visibility to structural similarity**中，作者采用$11 \\times 11$的滑动窗口将整副图像分割为$N$个patch，然后计算每一个patch的$SSIM$，最后计算所有patch的$SSIM$值的平均数（$Mean \\ \\ SSIM:MSSIM$）作为整副图像的$SSIM$。为了避免滑动窗口带来的块效应，在计算每个patch的均值$\\mu$和方差$\\sigma^2$时，作者采用了$\\sigma=1.5$的高斯卷积核作加权平均。\n\n如果整副图像有$N$个patch，则$MSSIM$的计算方式为：\n\n$$\nMSSIM(X,Y) = \\frac{1}{N}\\sum_{i=1}^{N}{SSIM(x_i, y_i)}\n$$\n\n其中，$SSIM(x_i, y_i)$为第$i$个patch的$SSIM$。\n\n## FFMpeg中计算SSIM的算法\n在FFMpeg中，也提供了计算$SSIM$的实现：[tiny_ssim](https://github.com/FFmpeg/FFmpeg/blob/master/tests/tiny_ssim.c)。从代码的注释中可以看到：为了提升算法的性能，没有采用论文中的高斯加权方式计算每个patch的$SSIM$，而是采用了一个$8 \\times 8$的块来计算每个patch的$SSIM$。\n\n```\n/*\n * tiny_ssim.c\n * Computes the Structural Similarity Metric between two rawYV12 video files.\n * original algorithm:\n * Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli,\n *   \"Image quality assessment: From error visibility to structural similarity,\"\n *   IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, Apr. 2004.\n *\n * To improve speed, this implementation uses the standard approximation of\n * overlapped 8x8 block sums, rather than the original gaussian weights.\n */\n```\n\n### standard approximation of overlapped 8x8 block sums\n接下来就解释一下注释中的*standard approximation of overlapped 8x8 block sums*究竟是什么含义。在解释的过程中会分解成两个部分来解释：*overlapped 8x8 block*和*sums*。\n\n### overlapped 8x8 block的含义\nFFMpeg在计算图像$SSIM$时，首先以$4 \\times 4$的块大小把**图1**所示的分辨率为$W \\times H$的图像：\n![图1](1.jpg)\n图1：原始图像\n\n分割为**图2**的样式。\n![图2](2.jpg)\n\n图2：分割后的图像\n\n对于**图2**中的每一块用$block(i,j)$来表示（*图2中的红色块*），FFMpeg使用$block(i,j)$及其**上、右、右上块**（*图2中的绿色块*）来计算其$SSIM:SSIM(x_{ij},y_{ij})$。\n\n$block(i,j)$及其**上、右、右上块**构成一个$8\\times8$的像素块，并且该$8\\times8$的块和计算$block(i,j+1)$的$SSIM$用到的$8\\times8$的块存在重合像素，这就是注释中的**overlapped 8x8 block**的真正含义。\n\n因此，根据如上规则：$i \\in [1,\\frac{H}{4}],j \\in [0,\\frac{W}{4}-1]$。也就是说：第0行和最后一列的块不会计算$SSIM$。\n\n最后得到FFMpeg中的$SSIM$计算方式为：\n\n$$\nSSIM = MSSIM(X,Y) = \\frac{1}{N}\\sum_{i=1}^{\\frac{H}{4}}\\sum_{j=0}^{\\frac{W}{4}-1}{SSIM(x_{ij}, y_{ij})}\n$$\n\n$$\nN=(\\frac{H}{4}-1)(\\frac{W}{4}-1)\n$$\n\n### sums的含义\n如前所述，我们分析了FFMpeg计算图像的$SSIM$的整体思路，接下来我们继续分析FFMpeg是如何计算$block(i,j)$的$SSIM(x_{ij},y_{ij})$的。\n\n首先利用函数`ssim_4x4x2_core()`来计算$block(i,j)$块的结构相似性指标，主要是如下的4个指标：\n* *s1*：参考图像在$block(i,j)$块的像素之和\n* *s2*：受损图像在$block(i,j)$块的像素之和\n* *ss*：参考图像和受损图像在$block(i,j)$块的像素平方之和\n* *s12*：参考图像和受损图像在$block(i,j)$块的对应像素乘积之和\n\n$s1=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{x(i,j)}$\n$s2=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{y(i,j)}$\n$ss=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{\\Big(\\big(x(i,j)\\big)^2+\\big(y(i,j)\\big)^2\\Big)}$\n$s12=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{\\big(x(i,j) \\cdot y(i,j)\\big)}$\n\n如上的4个指标就是我们后续会用到的sums，该sums也就是*overlapped 8x8 block sums*中的*sums*的概念。\n\n### 利用sums计算各块的SSIM\n接下来利用该sums值计算$SSIM$。\n\n为了提升效率，FFMpeg会按照行来计算每一行的各个块的sums数据，并将每个行块的sums数据存储在长度为$\\frac{W}{4}$的数组指针sum（`(int(*)[4])`）中。\n\n其中sum指针有两种：\n* sum0：存储当前行的各块的sums结果\n* sum1：存储当前行的上一行的sums结果\n\n先计算第$i-1$行块和第$i$行块的sums结果，并分别存入`sum1`和`sum0`中。然后遍历第$i$行块的每一个块，并利用`sum1`和`sum0`中计算的结果来计算每一块的$SSIM$。\n\n函数`ssim_end4()`展示了如何利用$block(i-1,j)$，$block(i-1,j+1)$，$block(i,j)$，$block(i,j+1)$的sums信息来计算$SSIM(x_{ij},y_{ij})$：\n* 先对4个块的sums结果进行加和处理，得到$8\\times8$块的sums结果\n* 然后利用该$8\\times8$块的sums来计算$block(i,j)$的$SSIM$\n\n函数`ssim_end1()`就展示了如何利用$8\\times8$块的sums信息来计算$SSIM$。具体的计算方法如下。\n\n将红色区块$block(i,j)$的图像放大一点，如图3所示。我们接下来计算其$SSIM$。\n![图3](3.jpg)\n图3：*block(i,j)*的示意图\n\n在计算时，首先将4个区块的sums值求和，得到$8\\times8$区块的sums值，分别为：\n* $s1=\\sum_{i=0}^{63}{x_i}$\n* $s2=\\sum_{i=0}^{63}{y_i}$\n* $ss=\\sum_{i=0}^{63}{(x_i^2+y_i^2)}$\n* $s12=\\sum_{i=0}^{63}{(x_i \\cdot y_i)}$\n\n根据如上的计算，可以得到$\\mu_x$，$\\mu_y$，$\\mu_x\\mu_y$，$\\mu_x^2+\\mu_y^2$，$\\sigma_x^2+\\sigma_y^2$，$\\sigma_{xy}$：\n* $\\mu_x=\\frac{1}{64} \\cdot s1$\n* $\\mu_y=\\frac{1}{64} \\cdot s2$\n* $\\mu_x\\mu_y=\\frac{1}{64 \\cdot 64}(s1 \\cdot s2)$\n* $\\mu_x^2+\\mu_y^2=\\frac{1}{64 \\cdot 64}\\big((s1)^2+(s2)^2\\big)$\n* $\\sigma_x^2+\\sigma_y^2=\\frac{1}{64 \\cdot 63}\\big(64 \\cdot ss-(s1)^2- (s2)^2\\big)$\n* $\\sigma_{xy}=\\frac{1}{64 \\cdot 63}(64 \\cdot s12 - s1 \\cdot s2)$\n\n利用如上的公式（具体推导可以参考[ssim_end1()的推导](/2020/02/18/the-proof-of-the-SSIM-in-FFMpeg/)）对$SSIM$的公式进行计算可以得到：\n$$\nSSIM(x,y)=\\frac{(2\\mu_x\\mu_y+C_1)(2\\sigma_{xy}+C_2)}{(\\mu_x^2+\\mu_y^2+C_1)(\\sigma_x^2+\\sigma_y^2+C_2)}\n$$\n\n$$\n=\\frac{(2s1s2+64^2C_1)(2\\cdot64s12-2s1s2+64\\cdot63C_2)}{(s1^2+s2^2+64^2C_1)(64ss-s1^2-s2^2+64\\cdot63C_2)}\n$$\n\nFFMpeg中，对$C_1$和$C_2$的定义中的因子**64**或**63**也是根据上面的公式，但是从公式看，[**FFMpeg对`ssim_c1`的计算少乘了64**](https://trac.ffmpeg.org/ticket/8529)：\n```\nssim_c1 = 0.01 * 0.01 * 255 * 255 * 64 + 0.5\nssim_c2 = 0.03 * 0.03 * 255 * 255 * 64 * 63 + 0.5\n```\n\n当然，为了简化处理，FFMpeg还做了如下的定义：\n```\nvars  = ss  * 64 - s1 * s1 - s2 * s2;\ncovar = s12 * 64 - s1 * s2;\n```\n\n因此，最终在FFMpeg中，计算$SSIM$的公式为：\n\n$$\nSSIM(x,y)=\\frac{(2s1s2+ssimC_1)(2covar+ssimC_2)}{(s1^2+s2^2+ssimC_1)(vars + ssimC_2)}\n$$\n\n如上的公式就是函数`ssim_end1()`中最终的计算方式。\n\n### 利用各块的SSIM计算图像的SSIM\n计算完所有块的$SSIM$之后，就可以计算所有块的平均$SSIM$并作为该图像的$SSIM$：\n\n$$\nSSIM(X,Y)= \\frac{1}{N}\\sum_{i=1}^{\\frac{H}{4}}\\sum_{j=0}^{\\frac{W}{4}-1}{SSIM(x_{ij}, y_{ij})}\n$$\n\n$$\nN=(\\frac{H}{4}-1)(\\frac{W}{4}-1)\n$$\n\n### 编码过程中的技巧\n在FFMpeg计算$SSIM$的算法实现中，为了提升效率和抽象代码逻辑，也利用了很多的编程技巧，例如：\n* 计算YUV各分量图像宽度时用`w >> !!i`\n* 为了避免对第0行的特殊处理，采用两层循环来处理\n* 计算每一行的各块的sums信息时，为了降低循环次数，每次循环计算2个块的sums结果，`ssim_4x4x2_core`的函数名可能就是这么来的。\n* 计算每一行的各块的$SSIM$时，为了降低循环次数，每次循环计算4个块的$SSIM$，`ssim_end4`的函数名可能就是这么来的。\n\n## 感谢\n分析过程离不开和贤杰(github: [@bodhisatan](https://github.com/bodhisatan))的不断讨论和交流，感谢@贤杰在繁忙的工作之余抽出时间来一起分析FFMpeg中SSIM算法的实现原理。","source":"_posts/how-to-calculate-the-SSIM-in-FFMpeg.md","raw":"---\ntitle: FFMpeg如何计算图像的SSIM\nreward: false\ndate: 2020-02-15 15:08:41\ncategories: 视频技术\ntags:\n  - SSIM\n  - FFMpeg\n---\n\n## SSIM基本概念\n关于$SSIM$的具体解释，此处不再介绍，具体可以参见[数字视频相关概念](https://wangwei1237.github.io/digital-video-concept/)中的[SSIM算法](https://wangwei1237.github.io/digital-video-concept/docs/4_2_2_3_StructuralSimilarityBasedApproaches.html)一节的介绍。\n\n直接给出$SSIM$的计算方法：\n$$\nSSIM(x,y)=\\frac{(2\\mu_x\\mu_y+C_1)(2\\sigma_{xy}+C_2)}{(\\mu_x^2+\\mu_y^2+C_1)(\\sigma_x^2+\\sigma_y^2+C_2)}\n$$\n\n$C_1=(K_1L)^2, C_2=(K_2L)^2$。$K_1\\ll1$，$K_2\\ll1$均为常数，计算时，一般$K_1=0.01$，$K_2=0.03$。$L$是灰度的动态范围，由图像的数据类型决定，如果数据为*uint8*，则$L=255$。\n\n<!--more-->\n\n## SSIM计算中的图像分割\n在整幅图片的跨度上，图像亮度的均值和方差变化较为剧烈；并且图像上不同区块的失真程度也有可能不同；再者人眼睛每次只能聚焦于一处，更关注局部数据而非全局数据。因此如上的$SSIM$算法不能直接作用于一整副图像。\n\n在论文**Image quality assessment: From error visibility to structural similarity**中，作者采用$11 \\times 11$的滑动窗口将整副图像分割为$N$个patch，然后计算每一个patch的$SSIM$，最后计算所有patch的$SSIM$值的平均数（$Mean \\ \\ SSIM:MSSIM$）作为整副图像的$SSIM$。为了避免滑动窗口带来的块效应，在计算每个patch的均值$\\mu$和方差$\\sigma^2$时，作者采用了$\\sigma=1.5$的高斯卷积核作加权平均。\n\n如果整副图像有$N$个patch，则$MSSIM$的计算方式为：\n\n$$\nMSSIM(X,Y) = \\frac{1}{N}\\sum_{i=1}^{N}{SSIM(x_i, y_i)}\n$$\n\n其中，$SSIM(x_i, y_i)$为第$i$个patch的$SSIM$。\n\n## FFMpeg中计算SSIM的算法\n在FFMpeg中，也提供了计算$SSIM$的实现：[tiny_ssim](https://github.com/FFmpeg/FFmpeg/blob/master/tests/tiny_ssim.c)。从代码的注释中可以看到：为了提升算法的性能，没有采用论文中的高斯加权方式计算每个patch的$SSIM$，而是采用了一个$8 \\times 8$的块来计算每个patch的$SSIM$。\n\n```\n/*\n * tiny_ssim.c\n * Computes the Structural Similarity Metric between two rawYV12 video files.\n * original algorithm:\n * Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli,\n *   \"Image quality assessment: From error visibility to structural similarity,\"\n *   IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, Apr. 2004.\n *\n * To improve speed, this implementation uses the standard approximation of\n * overlapped 8x8 block sums, rather than the original gaussian weights.\n */\n```\n\n### standard approximation of overlapped 8x8 block sums\n接下来就解释一下注释中的*standard approximation of overlapped 8x8 block sums*究竟是什么含义。在解释的过程中会分解成两个部分来解释：*overlapped 8x8 block*和*sums*。\n\n### overlapped 8x8 block的含义\nFFMpeg在计算图像$SSIM$时，首先以$4 \\times 4$的块大小把**图1**所示的分辨率为$W \\times H$的图像：\n![图1](1.jpg)\n图1：原始图像\n\n分割为**图2**的样式。\n![图2](2.jpg)\n\n图2：分割后的图像\n\n对于**图2**中的每一块用$block(i,j)$来表示（*图2中的红色块*），FFMpeg使用$block(i,j)$及其**上、右、右上块**（*图2中的绿色块*）来计算其$SSIM:SSIM(x_{ij},y_{ij})$。\n\n$block(i,j)$及其**上、右、右上块**构成一个$8\\times8$的像素块，并且该$8\\times8$的块和计算$block(i,j+1)$的$SSIM$用到的$8\\times8$的块存在重合像素，这就是注释中的**overlapped 8x8 block**的真正含义。\n\n因此，根据如上规则：$i \\in [1,\\frac{H}{4}],j \\in [0,\\frac{W}{4}-1]$。也就是说：第0行和最后一列的块不会计算$SSIM$。\n\n最后得到FFMpeg中的$SSIM$计算方式为：\n\n$$\nSSIM = MSSIM(X,Y) = \\frac{1}{N}\\sum_{i=1}^{\\frac{H}{4}}\\sum_{j=0}^{\\frac{W}{4}-1}{SSIM(x_{ij}, y_{ij})}\n$$\n\n$$\nN=(\\frac{H}{4}-1)(\\frac{W}{4}-1)\n$$\n\n### sums的含义\n如前所述，我们分析了FFMpeg计算图像的$SSIM$的整体思路，接下来我们继续分析FFMpeg是如何计算$block(i,j)$的$SSIM(x_{ij},y_{ij})$的。\n\n首先利用函数`ssim_4x4x2_core()`来计算$block(i,j)$块的结构相似性指标，主要是如下的4个指标：\n* *s1*：参考图像在$block(i,j)$块的像素之和\n* *s2*：受损图像在$block(i,j)$块的像素之和\n* *ss*：参考图像和受损图像在$block(i,j)$块的像素平方之和\n* *s12*：参考图像和受损图像在$block(i,j)$块的对应像素乘积之和\n\n$s1=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{x(i,j)}$\n$s2=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{y(i,j)}$\n$ss=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{\\Big(\\big(x(i,j)\\big)^2+\\big(y(i,j)\\big)^2\\Big)}$\n$s12=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{\\big(x(i,j) \\cdot y(i,j)\\big)}$\n\n如上的4个指标就是我们后续会用到的sums，该sums也就是*overlapped 8x8 block sums*中的*sums*的概念。\n\n### 利用sums计算各块的SSIM\n接下来利用该sums值计算$SSIM$。\n\n为了提升效率，FFMpeg会按照行来计算每一行的各个块的sums数据，并将每个行块的sums数据存储在长度为$\\frac{W}{4}$的数组指针sum（`(int(*)[4])`）中。\n\n其中sum指针有两种：\n* sum0：存储当前行的各块的sums结果\n* sum1：存储当前行的上一行的sums结果\n\n先计算第$i-1$行块和第$i$行块的sums结果，并分别存入`sum1`和`sum0`中。然后遍历第$i$行块的每一个块，并利用`sum1`和`sum0`中计算的结果来计算每一块的$SSIM$。\n\n函数`ssim_end4()`展示了如何利用$block(i-1,j)$，$block(i-1,j+1)$，$block(i,j)$，$block(i,j+1)$的sums信息来计算$SSIM(x_{ij},y_{ij})$：\n* 先对4个块的sums结果进行加和处理，得到$8\\times8$块的sums结果\n* 然后利用该$8\\times8$块的sums来计算$block(i,j)$的$SSIM$\n\n函数`ssim_end1()`就展示了如何利用$8\\times8$块的sums信息来计算$SSIM$。具体的计算方法如下。\n\n将红色区块$block(i,j)$的图像放大一点，如图3所示。我们接下来计算其$SSIM$。\n![图3](3.jpg)\n图3：*block(i,j)*的示意图\n\n在计算时，首先将4个区块的sums值求和，得到$8\\times8$区块的sums值，分别为：\n* $s1=\\sum_{i=0}^{63}{x_i}$\n* $s2=\\sum_{i=0}^{63}{y_i}$\n* $ss=\\sum_{i=0}^{63}{(x_i^2+y_i^2)}$\n* $s12=\\sum_{i=0}^{63}{(x_i \\cdot y_i)}$\n\n根据如上的计算，可以得到$\\mu_x$，$\\mu_y$，$\\mu_x\\mu_y$，$\\mu_x^2+\\mu_y^2$，$\\sigma_x^2+\\sigma_y^2$，$\\sigma_{xy}$：\n* $\\mu_x=\\frac{1}{64} \\cdot s1$\n* $\\mu_y=\\frac{1}{64} \\cdot s2$\n* $\\mu_x\\mu_y=\\frac{1}{64 \\cdot 64}(s1 \\cdot s2)$\n* $\\mu_x^2+\\mu_y^2=\\frac{1}{64 \\cdot 64}\\big((s1)^2+(s2)^2\\big)$\n* $\\sigma_x^2+\\sigma_y^2=\\frac{1}{64 \\cdot 63}\\big(64 \\cdot ss-(s1)^2- (s2)^2\\big)$\n* $\\sigma_{xy}=\\frac{1}{64 \\cdot 63}(64 \\cdot s12 - s1 \\cdot s2)$\n\n利用如上的公式（具体推导可以参考[ssim_end1()的推导](/2020/02/18/the-proof-of-the-SSIM-in-FFMpeg/)）对$SSIM$的公式进行计算可以得到：\n$$\nSSIM(x,y)=\\frac{(2\\mu_x\\mu_y+C_1)(2\\sigma_{xy}+C_2)}{(\\mu_x^2+\\mu_y^2+C_1)(\\sigma_x^2+\\sigma_y^2+C_2)}\n$$\n\n$$\n=\\frac{(2s1s2+64^2C_1)(2\\cdot64s12-2s1s2+64\\cdot63C_2)}{(s1^2+s2^2+64^2C_1)(64ss-s1^2-s2^2+64\\cdot63C_2)}\n$$\n\nFFMpeg中，对$C_1$和$C_2$的定义中的因子**64**或**63**也是根据上面的公式，但是从公式看，[**FFMpeg对`ssim_c1`的计算少乘了64**](https://trac.ffmpeg.org/ticket/8529)：\n```\nssim_c1 = 0.01 * 0.01 * 255 * 255 * 64 + 0.5\nssim_c2 = 0.03 * 0.03 * 255 * 255 * 64 * 63 + 0.5\n```\n\n当然，为了简化处理，FFMpeg还做了如下的定义：\n```\nvars  = ss  * 64 - s1 * s1 - s2 * s2;\ncovar = s12 * 64 - s1 * s2;\n```\n\n因此，最终在FFMpeg中，计算$SSIM$的公式为：\n\n$$\nSSIM(x,y)=\\frac{(2s1s2+ssimC_1)(2covar+ssimC_2)}{(s1^2+s2^2+ssimC_1)(vars + ssimC_2)}\n$$\n\n如上的公式就是函数`ssim_end1()`中最终的计算方式。\n\n### 利用各块的SSIM计算图像的SSIM\n计算完所有块的$SSIM$之后，就可以计算所有块的平均$SSIM$并作为该图像的$SSIM$：\n\n$$\nSSIM(X,Y)= \\frac{1}{N}\\sum_{i=1}^{\\frac{H}{4}}\\sum_{j=0}^{\\frac{W}{4}-1}{SSIM(x_{ij}, y_{ij})}\n$$\n\n$$\nN=(\\frac{H}{4}-1)(\\frac{W}{4}-1)\n$$\n\n### 编码过程中的技巧\n在FFMpeg计算$SSIM$的算法实现中，为了提升效率和抽象代码逻辑，也利用了很多的编程技巧，例如：\n* 计算YUV各分量图像宽度时用`w >> !!i`\n* 为了避免对第0行的特殊处理，采用两层循环来处理\n* 计算每一行的各块的sums信息时，为了降低循环次数，每次循环计算2个块的sums结果，`ssim_4x4x2_core`的函数名可能就是这么来的。\n* 计算每一行的各块的$SSIM$时，为了降低循环次数，每次循环计算4个块的$SSIM$，`ssim_end4`的函数名可能就是这么来的。\n\n## 感谢\n分析过程离不开和贤杰(github: [@bodhisatan](https://github.com/bodhisatan))的不断讨论和交流，感谢@贤杰在繁忙的工作之余抽出时间来一起分析FFMpeg中SSIM算法的实现原理。","slug":"how-to-calculate-the-SSIM-in-FFMpeg","published":1,"updated":"2020-03-30T09:30:27.786Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9kyh075000g1pdbdfwzgz73","content":"<h2 id=\"SSIM基本概念\"><a href=\"#SSIM基本概念\" class=\"headerlink\" title=\"SSIM基本概念\"></a>SSIM基本概念</h2><p>关于$SSIM$的具体解释，此处不再介绍，具体可以参见<a href=\"https://wangwei1237.github.io/digital-video-concept/\">数字视频相关概念</a>中的<a href=\"https://wangwei1237.github.io/digital-video-concept/docs/4_2_2_3_StructuralSimilarityBasedApproaches.html\">SSIM算法</a>一节的介绍。</p>\n<p>直接给出$SSIM$的计算方法：<br>$$<br>SSIM(x,y)=\\frac{(2\\mu_x\\mu_y+C_1)(2\\sigma_{xy}+C_2)}{(\\mu_x^2+\\mu_y^2+C_1)(\\sigma_x^2+\\sigma_y^2+C_2)}<br>$$</p>\n<p>$C_1=(K_1L)^2, C_2=(K_2L)^2$。$K_1\\ll1$，$K_2\\ll1$均为常数，计算时，一般$K_1=0.01$，$K_2=0.03$。$L$是灰度的动态范围，由图像的数据类型决定，如果数据为<em>uint8</em>，则$L=255$。</p>\n<a id=\"more\"></a>\n\n<h2 id=\"SSIM计算中的图像分割\"><a href=\"#SSIM计算中的图像分割\" class=\"headerlink\" title=\"SSIM计算中的图像分割\"></a>SSIM计算中的图像分割</h2><p>在整幅图片的跨度上，图像亮度的均值和方差变化较为剧烈；并且图像上不同区块的失真程度也有可能不同；再者人眼睛每次只能聚焦于一处，更关注局部数据而非全局数据。因此如上的$SSIM$算法不能直接作用于一整副图像。</p>\n<p>在论文<strong>Image quality assessment: From error visibility to structural similarity</strong>中，作者采用$11 \\times 11$的滑动窗口将整副图像分割为$N$个patch，然后计算每一个patch的$SSIM$，最后计算所有patch的$SSIM$值的平均数（$Mean \\ \\ SSIM:MSSIM$）作为整副图像的$SSIM$。为了避免滑动窗口带来的块效应，在计算每个patch的均值$\\mu$和方差$\\sigma^2$时，作者采用了$\\sigma=1.5$的高斯卷积核作加权平均。</p>\n<p>如果整副图像有$N$个patch，则$MSSIM$的计算方式为：</p>\n<p>$$<br>MSSIM(X,Y) = \\frac{1}{N}\\sum_{i=1}^{N}{SSIM(x_i, y_i)}<br>$$</p>\n<p>其中，$SSIM(x_i, y_i)$为第$i$个patch的$SSIM$。</p>\n<h2 id=\"FFMpeg中计算SSIM的算法\"><a href=\"#FFMpeg中计算SSIM的算法\" class=\"headerlink\" title=\"FFMpeg中计算SSIM的算法\"></a>FFMpeg中计算SSIM的算法</h2><p>在FFMpeg中，也提供了计算$SSIM$的实现：<a href=\"https://github.com/FFmpeg/FFmpeg/blob/master/tests/tiny_ssim.c\" target=\"_blank\" rel=\"noopener\">tiny_ssim</a>。从代码的注释中可以看到：为了提升算法的性能，没有采用论文中的高斯加权方式计算每个patch的$SSIM$，而是采用了一个$8 \\times 8$的块来计算每个patch的$SSIM$。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;*</span><br><span class=\"line\"> * tiny_ssim.c</span><br><span class=\"line\"> * Computes the Structural Similarity Metric between two rawYV12 video files.</span><br><span class=\"line\"> * original algorithm:</span><br><span class=\"line\"> * Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli,</span><br><span class=\"line\"> *   &quot;Image quality assessment: From error visibility to structural similarity,&quot;</span><br><span class=\"line\"> *   IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, Apr. 2004.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * To improve speed, this implementation uses the standard approximation of</span><br><span class=\"line\"> * overlapped 8x8 block sums, rather than the original gaussian weights.</span><br><span class=\"line\"> *&#x2F;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"standard-approximation-of-overlapped-8x8-block-sums\"><a href=\"#standard-approximation-of-overlapped-8x8-block-sums\" class=\"headerlink\" title=\"standard approximation of overlapped 8x8 block sums\"></a>standard approximation of overlapped 8x8 block sums</h3><p>接下来就解释一下注释中的<em>standard approximation of overlapped 8x8 block sums</em>究竟是什么含义。在解释的过程中会分解成两个部分来解释：<em>overlapped 8x8 block</em>和<em>sums</em>。</p>\n<h3 id=\"overlapped-8x8-block的含义\"><a href=\"#overlapped-8x8-block的含义\" class=\"headerlink\" title=\"overlapped 8x8 block的含义\"></a>overlapped 8x8 block的含义</h3><p>FFMpeg在计算图像$SSIM$时，首先以$4 \\times 4$的块大小把<strong>图1</strong>所示的分辨率为$W \\times H$的图像：<br><img src=\"/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/1.jpg\" alt=\"图1\"><br>图1：原始图像</p>\n<p>分割为<strong>图2</strong>的样式。<br><img src=\"/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/2.jpg\" alt=\"图2\"></p>\n<p>图2：分割后的图像</p>\n<p>对于<strong>图2</strong>中的每一块用$block(i,j)$来表示（<em>图2中的红色块</em>），FFMpeg使用$block(i,j)$及其<strong>上、右、右上块</strong>（<em>图2中的绿色块</em>）来计算其$SSIM:SSIM(x_{ij},y_{ij})$。</p>\n<p>$block(i,j)$及其<strong>上、右、右上块</strong>构成一个$8\\times8$的像素块，并且该$8\\times8$的块和计算$block(i,j+1)$的$SSIM$用到的$8\\times8$的块存在重合像素，这就是注释中的<strong>overlapped 8x8 block</strong>的真正含义。</p>\n<p>因此，根据如上规则：$i \\in [1,\\frac{H}{4}],j \\in [0,\\frac{W}{4}-1]$。也就是说：第0行和最后一列的块不会计算$SSIM$。</p>\n<p>最后得到FFMpeg中的$SSIM$计算方式为：</p>\n<p>$$<br>SSIM = MSSIM(X,Y) = \\frac{1}{N}\\sum_{i=1}^{\\frac{H}{4}}\\sum_{j=0}^{\\frac{W}{4}-1}{SSIM(x_{ij}, y_{ij})}<br>$$</p>\n<p>$$<br>N=(\\frac{H}{4}-1)(\\frac{W}{4}-1)<br>$$</p>\n<h3 id=\"sums的含义\"><a href=\"#sums的含义\" class=\"headerlink\" title=\"sums的含义\"></a>sums的含义</h3><p>如前所述，我们分析了FFMpeg计算图像的$SSIM$的整体思路，接下来我们继续分析FFMpeg是如何计算$block(i,j)$的$SSIM(x_{ij},y_{ij})$的。</p>\n<p>首先利用函数<code>ssim_4x4x2_core()</code>来计算$block(i,j)$块的结构相似性指标，主要是如下的4个指标：</p>\n<ul>\n<li><em>s1</em>：参考图像在$block(i,j)$块的像素之和</li>\n<li><em>s2</em>：受损图像在$block(i,j)$块的像素之和</li>\n<li><em>ss</em>：参考图像和受损图像在$block(i,j)$块的像素平方之和</li>\n<li><em>s12</em>：参考图像和受损图像在$block(i,j)$块的对应像素乘积之和</li>\n</ul>\n<p>$s1=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{x(i,j)}$<br>$s2=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{y(i,j)}$<br>$ss=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{\\Big(\\big(x(i,j)\\big)^2+\\big(y(i,j)\\big)^2\\Big)}$<br>$s12=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{\\big(x(i,j) \\cdot y(i,j)\\big)}$</p>\n<p>如上的4个指标就是我们后续会用到的sums，该sums也就是<em>overlapped 8x8 block sums</em>中的<em>sums</em>的概念。</p>\n<h3 id=\"利用sums计算各块的SSIM\"><a href=\"#利用sums计算各块的SSIM\" class=\"headerlink\" title=\"利用sums计算各块的SSIM\"></a>利用sums计算各块的SSIM</h3><p>接下来利用该sums值计算$SSIM$。</p>\n<p>为了提升效率，FFMpeg会按照行来计算每一行的各个块的sums数据，并将每个行块的sums数据存储在长度为$\\frac{W}{4}$的数组指针sum（<code>(int(*)[4])</code>）中。</p>\n<p>其中sum指针有两种：</p>\n<ul>\n<li>sum0：存储当前行的各块的sums结果</li>\n<li>sum1：存储当前行的上一行的sums结果</li>\n</ul>\n<p>先计算第$i-1$行块和第$i$行块的sums结果，并分别存入<code>sum1</code>和<code>sum0</code>中。然后遍历第$i$行块的每一个块，并利用<code>sum1</code>和<code>sum0</code>中计算的结果来计算每一块的$SSIM$。</p>\n<p>函数<code>ssim_end4()</code>展示了如何利用$block(i-1,j)$，$block(i-1,j+1)$，$block(i,j)$，$block(i,j+1)$的sums信息来计算$SSIM(x_{ij},y_{ij})$：</p>\n<ul>\n<li>先对4个块的sums结果进行加和处理，得到$8\\times8$块的sums结果</li>\n<li>然后利用该$8\\times8$块的sums来计算$block(i,j)$的$SSIM$</li>\n</ul>\n<p>函数<code>ssim_end1()</code>就展示了如何利用$8\\times8$块的sums信息来计算$SSIM$。具体的计算方法如下。</p>\n<p>将红色区块$block(i,j)$的图像放大一点，如图3所示。我们接下来计算其$SSIM$。<br><img src=\"/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/3.jpg\" alt=\"图3\"><br>图3：<em>block(i,j)</em>的示意图</p>\n<p>在计算时，首先将4个区块的sums值求和，得到$8\\times8$区块的sums值，分别为：</p>\n<ul>\n<li>$s1=\\sum_{i=0}^{63}{x_i}$</li>\n<li>$s2=\\sum_{i=0}^{63}{y_i}$</li>\n<li>$ss=\\sum_{i=0}^{63}{(x_i^2+y_i^2)}$</li>\n<li>$s12=\\sum_{i=0}^{63}{(x_i \\cdot y_i)}$</li>\n</ul>\n<p>根据如上的计算，可以得到$\\mu_x$，$\\mu_y$，$\\mu_x\\mu_y$，$\\mu_x^2+\\mu_y^2$，$\\sigma_x^2+\\sigma_y^2$，$\\sigma_{xy}$：</p>\n<ul>\n<li>$\\mu_x=\\frac{1}{64} \\cdot s1$</li>\n<li>$\\mu_y=\\frac{1}{64} \\cdot s2$</li>\n<li>$\\mu_x\\mu_y=\\frac{1}{64 \\cdot 64}(s1 \\cdot s2)$</li>\n<li>$\\mu_x^2+\\mu_y^2=\\frac{1}{64 \\cdot 64}\\big((s1)^2+(s2)^2\\big)$</li>\n<li>$\\sigma_x^2+\\sigma_y^2=\\frac{1}{64 \\cdot 63}\\big(64 \\cdot ss-(s1)^2- (s2)^2\\big)$</li>\n<li>$\\sigma_{xy}=\\frac{1}{64 \\cdot 63}(64 \\cdot s12 - s1 \\cdot s2)$</li>\n</ul>\n<p>利用如上的公式（具体推导可以参考<a href=\"/2020/02/18/the-proof-of-the-SSIM-in-FFMpeg/\">ssim_end1()的推导</a>）对$SSIM$的公式进行计算可以得到：<br>$$<br>SSIM(x,y)=\\frac{(2\\mu_x\\mu_y+C_1)(2\\sigma_{xy}+C_2)}{(\\mu_x^2+\\mu_y^2+C_1)(\\sigma_x^2+\\sigma_y^2+C_2)}<br>$$</p>\n<p>$$<br>=\\frac{(2s1s2+64^2C_1)(2\\cdot64s12-2s1s2+64\\cdot63C_2)}{(s1^2+s2^2+64^2C_1)(64ss-s1^2-s2^2+64\\cdot63C_2)}<br>$$</p>\n<p>FFMpeg中，对$C_1$和$C_2$的定义中的因子<strong>64</strong>或<strong>63</strong>也是根据上面的公式，但是从公式看，<a href=\"https://trac.ffmpeg.org/ticket/8529\" target=\"_blank\" rel=\"noopener\"><strong>FFMpeg对<code>ssim_c1</code>的计算少乘了64</strong></a>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssim_c1 &#x3D; 0.01 * 0.01 * 255 * 255 * 64 + 0.5</span><br><span class=\"line\">ssim_c2 &#x3D; 0.03 * 0.03 * 255 * 255 * 64 * 63 + 0.5</span><br></pre></td></tr></table></figure>\n\n<p>当然，为了简化处理，FFMpeg还做了如下的定义：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vars  &#x3D; ss  * 64 - s1 * s1 - s2 * s2;</span><br><span class=\"line\">covar &#x3D; s12 * 64 - s1 * s2;</span><br></pre></td></tr></table></figure>\n\n<p>因此，最终在FFMpeg中，计算$SSIM$的公式为：</p>\n<p>$$<br>SSIM(x,y)=\\frac{(2s1s2+ssimC_1)(2covar+ssimC_2)}{(s1^2+s2^2+ssimC_1)(vars + ssimC_2)}<br>$$</p>\n<p>如上的公式就是函数<code>ssim_end1()</code>中最终的计算方式。</p>\n<h3 id=\"利用各块的SSIM计算图像的SSIM\"><a href=\"#利用各块的SSIM计算图像的SSIM\" class=\"headerlink\" title=\"利用各块的SSIM计算图像的SSIM\"></a>利用各块的SSIM计算图像的SSIM</h3><p>计算完所有块的$SSIM$之后，就可以计算所有块的平均$SSIM$并作为该图像的$SSIM$：</p>\n<p>$$<br>SSIM(X,Y)= \\frac{1}{N}\\sum_{i=1}^{\\frac{H}{4}}\\sum_{j=0}^{\\frac{W}{4}-1}{SSIM(x_{ij}, y_{ij})}<br>$$</p>\n<p>$$<br>N=(\\frac{H}{4}-1)(\\frac{W}{4}-1)<br>$$</p>\n<h3 id=\"编码过程中的技巧\"><a href=\"#编码过程中的技巧\" class=\"headerlink\" title=\"编码过程中的技巧\"></a>编码过程中的技巧</h3><p>在FFMpeg计算$SSIM$的算法实现中，为了提升效率和抽象代码逻辑，也利用了很多的编程技巧，例如：</p>\n<ul>\n<li>计算YUV各分量图像宽度时用<code>w &gt;&gt; !!i</code></li>\n<li>为了避免对第0行的特殊处理，采用两层循环来处理</li>\n<li>计算每一行的各块的sums信息时，为了降低循环次数，每次循环计算2个块的sums结果，<code>ssim_4x4x2_core</code>的函数名可能就是这么来的。</li>\n<li>计算每一行的各块的$SSIM$时，为了降低循环次数，每次循环计算4个块的$SSIM$，<code>ssim_end4</code>的函数名可能就是这么来的。</li>\n</ul>\n<h2 id=\"感谢\"><a href=\"#感谢\" class=\"headerlink\" title=\"感谢\"></a>感谢</h2><p>分析过程离不开和贤杰(github: <a href=\"https://github.com/bodhisatan\" target=\"_blank\" rel=\"noopener\">@bodhisatan</a>)的不断讨论和交流，感谢@贤杰在繁忙的工作之余抽出时间来一起分析FFMpeg中SSIM算法的实现原理。</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"SSIM基本概念\"><a href=\"#SSIM基本概念\" class=\"headerlink\" title=\"SSIM基本概念\"></a>SSIM基本概念</h2><p>关于$SSIM$的具体解释，此处不再介绍，具体可以参见<a href=\"https://wangwei1237.github.io/digital-video-concept/\">数字视频相关概念</a>中的<a href=\"https://wangwei1237.github.io/digital-video-concept/docs/4_2_2_3_StructuralSimilarityBasedApproaches.html\">SSIM算法</a>一节的介绍。</p>\n<p>直接给出$SSIM$的计算方法：<br>$$<br>SSIM(x,y)=\\frac{(2\\mu_x\\mu_y+C_1)(2\\sigma_{xy}+C_2)}{(\\mu_x^2+\\mu_y^2+C_1)(\\sigma_x^2+\\sigma_y^2+C_2)}<br>$$</p>\n<p>$C_1=(K_1L)^2, C_2=(K_2L)^2$。$K_1\\ll1$，$K_2\\ll1$均为常数，计算时，一般$K_1=0.01$，$K_2=0.03$。$L$是灰度的动态范围，由图像的数据类型决定，如果数据为<em>uint8</em>，则$L=255$。</p>","more":"<h2 id=\"SSIM计算中的图像分割\"><a href=\"#SSIM计算中的图像分割\" class=\"headerlink\" title=\"SSIM计算中的图像分割\"></a>SSIM计算中的图像分割</h2><p>在整幅图片的跨度上，图像亮度的均值和方差变化较为剧烈；并且图像上不同区块的失真程度也有可能不同；再者人眼睛每次只能聚焦于一处，更关注局部数据而非全局数据。因此如上的$SSIM$算法不能直接作用于一整副图像。</p>\n<p>在论文<strong>Image quality assessment: From error visibility to structural similarity</strong>中，作者采用$11 \\times 11$的滑动窗口将整副图像分割为$N$个patch，然后计算每一个patch的$SSIM$，最后计算所有patch的$SSIM$值的平均数（$Mean \\ \\ SSIM:MSSIM$）作为整副图像的$SSIM$。为了避免滑动窗口带来的块效应，在计算每个patch的均值$\\mu$和方差$\\sigma^2$时，作者采用了$\\sigma=1.5$的高斯卷积核作加权平均。</p>\n<p>如果整副图像有$N$个patch，则$MSSIM$的计算方式为：</p>\n<p>$$<br>MSSIM(X,Y) = \\frac{1}{N}\\sum_{i=1}^{N}{SSIM(x_i, y_i)}<br>$$</p>\n<p>其中，$SSIM(x_i, y_i)$为第$i$个patch的$SSIM$。</p>\n<h2 id=\"FFMpeg中计算SSIM的算法\"><a href=\"#FFMpeg中计算SSIM的算法\" class=\"headerlink\" title=\"FFMpeg中计算SSIM的算法\"></a>FFMpeg中计算SSIM的算法</h2><p>在FFMpeg中，也提供了计算$SSIM$的实现：<a href=\"https://github.com/FFmpeg/FFmpeg/blob/master/tests/tiny_ssim.c\" target=\"_blank\" rel=\"noopener\">tiny_ssim</a>。从代码的注释中可以看到：为了提升算法的性能，没有采用论文中的高斯加权方式计算每个patch的$SSIM$，而是采用了一个$8 \\times 8$的块来计算每个patch的$SSIM$。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;*</span><br><span class=\"line\"> * tiny_ssim.c</span><br><span class=\"line\"> * Computes the Structural Similarity Metric between two rawYV12 video files.</span><br><span class=\"line\"> * original algorithm:</span><br><span class=\"line\"> * Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli,</span><br><span class=\"line\"> *   &quot;Image quality assessment: From error visibility to structural similarity,&quot;</span><br><span class=\"line\"> *   IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, Apr. 2004.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * To improve speed, this implementation uses the standard approximation of</span><br><span class=\"line\"> * overlapped 8x8 block sums, rather than the original gaussian weights.</span><br><span class=\"line\"> *&#x2F;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"standard-approximation-of-overlapped-8x8-block-sums\"><a href=\"#standard-approximation-of-overlapped-8x8-block-sums\" class=\"headerlink\" title=\"standard approximation of overlapped 8x8 block sums\"></a>standard approximation of overlapped 8x8 block sums</h3><p>接下来就解释一下注释中的<em>standard approximation of overlapped 8x8 block sums</em>究竟是什么含义。在解释的过程中会分解成两个部分来解释：<em>overlapped 8x8 block</em>和<em>sums</em>。</p>\n<h3 id=\"overlapped-8x8-block的含义\"><a href=\"#overlapped-8x8-block的含义\" class=\"headerlink\" title=\"overlapped 8x8 block的含义\"></a>overlapped 8x8 block的含义</h3><p>FFMpeg在计算图像$SSIM$时，首先以$4 \\times 4$的块大小把<strong>图1</strong>所示的分辨率为$W \\times H$的图像：<br><img src=\"/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/1.jpg\" alt=\"图1\"><br>图1：原始图像</p>\n<p>分割为<strong>图2</strong>的样式。<br><img src=\"/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/2.jpg\" alt=\"图2\"></p>\n<p>图2：分割后的图像</p>\n<p>对于<strong>图2</strong>中的每一块用$block(i,j)$来表示（<em>图2中的红色块</em>），FFMpeg使用$block(i,j)$及其<strong>上、右、右上块</strong>（<em>图2中的绿色块</em>）来计算其$SSIM:SSIM(x_{ij},y_{ij})$。</p>\n<p>$block(i,j)$及其<strong>上、右、右上块</strong>构成一个$8\\times8$的像素块，并且该$8\\times8$的块和计算$block(i,j+1)$的$SSIM$用到的$8\\times8$的块存在重合像素，这就是注释中的<strong>overlapped 8x8 block</strong>的真正含义。</p>\n<p>因此，根据如上规则：$i \\in [1,\\frac{H}{4}],j \\in [0,\\frac{W}{4}-1]$。也就是说：第0行和最后一列的块不会计算$SSIM$。</p>\n<p>最后得到FFMpeg中的$SSIM$计算方式为：</p>\n<p>$$<br>SSIM = MSSIM(X,Y) = \\frac{1}{N}\\sum_{i=1}^{\\frac{H}{4}}\\sum_{j=0}^{\\frac{W}{4}-1}{SSIM(x_{ij}, y_{ij})}<br>$$</p>\n<p>$$<br>N=(\\frac{H}{4}-1)(\\frac{W}{4}-1)<br>$$</p>\n<h3 id=\"sums的含义\"><a href=\"#sums的含义\" class=\"headerlink\" title=\"sums的含义\"></a>sums的含义</h3><p>如前所述，我们分析了FFMpeg计算图像的$SSIM$的整体思路，接下来我们继续分析FFMpeg是如何计算$block(i,j)$的$SSIM(x_{ij},y_{ij})$的。</p>\n<p>首先利用函数<code>ssim_4x4x2_core()</code>来计算$block(i,j)$块的结构相似性指标，主要是如下的4个指标：</p>\n<ul>\n<li><em>s1</em>：参考图像在$block(i,j)$块的像素之和</li>\n<li><em>s2</em>：受损图像在$block(i,j)$块的像素之和</li>\n<li><em>ss</em>：参考图像和受损图像在$block(i,j)$块的像素平方之和</li>\n<li><em>s12</em>：参考图像和受损图像在$block(i,j)$块的对应像素乘积之和</li>\n</ul>\n<p>$s1=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{x(i,j)}$<br>$s2=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{y(i,j)}$<br>$ss=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{\\Big(\\big(x(i,j)\\big)^2+\\big(y(i,j)\\big)^2\\Big)}$<br>$s12=\\sum_{i=0}^{3}\\sum_{j=0}^{3}{\\big(x(i,j) \\cdot y(i,j)\\big)}$</p>\n<p>如上的4个指标就是我们后续会用到的sums，该sums也就是<em>overlapped 8x8 block sums</em>中的<em>sums</em>的概念。</p>\n<h3 id=\"利用sums计算各块的SSIM\"><a href=\"#利用sums计算各块的SSIM\" class=\"headerlink\" title=\"利用sums计算各块的SSIM\"></a>利用sums计算各块的SSIM</h3><p>接下来利用该sums值计算$SSIM$。</p>\n<p>为了提升效率，FFMpeg会按照行来计算每一行的各个块的sums数据，并将每个行块的sums数据存储在长度为$\\frac{W}{4}$的数组指针sum（<code>(int(*)[4])</code>）中。</p>\n<p>其中sum指针有两种：</p>\n<ul>\n<li>sum0：存储当前行的各块的sums结果</li>\n<li>sum1：存储当前行的上一行的sums结果</li>\n</ul>\n<p>先计算第$i-1$行块和第$i$行块的sums结果，并分别存入<code>sum1</code>和<code>sum0</code>中。然后遍历第$i$行块的每一个块，并利用<code>sum1</code>和<code>sum0</code>中计算的结果来计算每一块的$SSIM$。</p>\n<p>函数<code>ssim_end4()</code>展示了如何利用$block(i-1,j)$，$block(i-1,j+1)$，$block(i,j)$，$block(i,j+1)$的sums信息来计算$SSIM(x_{ij},y_{ij})$：</p>\n<ul>\n<li>先对4个块的sums结果进行加和处理，得到$8\\times8$块的sums结果</li>\n<li>然后利用该$8\\times8$块的sums来计算$block(i,j)$的$SSIM$</li>\n</ul>\n<p>函数<code>ssim_end1()</code>就展示了如何利用$8\\times8$块的sums信息来计算$SSIM$。具体的计算方法如下。</p>\n<p>将红色区块$block(i,j)$的图像放大一点，如图3所示。我们接下来计算其$SSIM$。<br><img src=\"/2020/02/15/how-to-calculate-the-SSIM-in-FFMpeg/3.jpg\" alt=\"图3\"><br>图3：<em>block(i,j)</em>的示意图</p>\n<p>在计算时，首先将4个区块的sums值求和，得到$8\\times8$区块的sums值，分别为：</p>\n<ul>\n<li>$s1=\\sum_{i=0}^{63}{x_i}$</li>\n<li>$s2=\\sum_{i=0}^{63}{y_i}$</li>\n<li>$ss=\\sum_{i=0}^{63}{(x_i^2+y_i^2)}$</li>\n<li>$s12=\\sum_{i=0}^{63}{(x_i \\cdot y_i)}$</li>\n</ul>\n<p>根据如上的计算，可以得到$\\mu_x$，$\\mu_y$，$\\mu_x\\mu_y$，$\\mu_x^2+\\mu_y^2$，$\\sigma_x^2+\\sigma_y^2$，$\\sigma_{xy}$：</p>\n<ul>\n<li>$\\mu_x=\\frac{1}{64} \\cdot s1$</li>\n<li>$\\mu_y=\\frac{1}{64} \\cdot s2$</li>\n<li>$\\mu_x\\mu_y=\\frac{1}{64 \\cdot 64}(s1 \\cdot s2)$</li>\n<li>$\\mu_x^2+\\mu_y^2=\\frac{1}{64 \\cdot 64}\\big((s1)^2+(s2)^2\\big)$</li>\n<li>$\\sigma_x^2+\\sigma_y^2=\\frac{1}{64 \\cdot 63}\\big(64 \\cdot ss-(s1)^2- (s2)^2\\big)$</li>\n<li>$\\sigma_{xy}=\\frac{1}{64 \\cdot 63}(64 \\cdot s12 - s1 \\cdot s2)$</li>\n</ul>\n<p>利用如上的公式（具体推导可以参考<a href=\"/2020/02/18/the-proof-of-the-SSIM-in-FFMpeg/\">ssim_end1()的推导</a>）对$SSIM$的公式进行计算可以得到：<br>$$<br>SSIM(x,y)=\\frac{(2\\mu_x\\mu_y+C_1)(2\\sigma_{xy}+C_2)}{(\\mu_x^2+\\mu_y^2+C_1)(\\sigma_x^2+\\sigma_y^2+C_2)}<br>$$</p>\n<p>$$<br>=\\frac{(2s1s2+64^2C_1)(2\\cdot64s12-2s1s2+64\\cdot63C_2)}{(s1^2+s2^2+64^2C_1)(64ss-s1^2-s2^2+64\\cdot63C_2)}<br>$$</p>\n<p>FFMpeg中，对$C_1$和$C_2$的定义中的因子<strong>64</strong>或<strong>63</strong>也是根据上面的公式，但是从公式看，<a href=\"https://trac.ffmpeg.org/ticket/8529\" target=\"_blank\" rel=\"noopener\"><strong>FFMpeg对<code>ssim_c1</code>的计算少乘了64</strong></a>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssim_c1 &#x3D; 0.01 * 0.01 * 255 * 255 * 64 + 0.5</span><br><span class=\"line\">ssim_c2 &#x3D; 0.03 * 0.03 * 255 * 255 * 64 * 63 + 0.5</span><br></pre></td></tr></table></figure>\n\n<p>当然，为了简化处理，FFMpeg还做了如下的定义：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vars  &#x3D; ss  * 64 - s1 * s1 - s2 * s2;</span><br><span class=\"line\">covar &#x3D; s12 * 64 - s1 * s2;</span><br></pre></td></tr></table></figure>\n\n<p>因此，最终在FFMpeg中，计算$SSIM$的公式为：</p>\n<p>$$<br>SSIM(x,y)=\\frac{(2s1s2+ssimC_1)(2covar+ssimC_2)}{(s1^2+s2^2+ssimC_1)(vars + ssimC_2)}<br>$$</p>\n<p>如上的公式就是函数<code>ssim_end1()</code>中最终的计算方式。</p>\n<h3 id=\"利用各块的SSIM计算图像的SSIM\"><a href=\"#利用各块的SSIM计算图像的SSIM\" class=\"headerlink\" title=\"利用各块的SSIM计算图像的SSIM\"></a>利用各块的SSIM计算图像的SSIM</h3><p>计算完所有块的$SSIM$之后，就可以计算所有块的平均$SSIM$并作为该图像的$SSIM$：</p>\n<p>$$<br>SSIM(X,Y)= \\frac{1}{N}\\sum_{i=1}^{\\frac{H}{4}}\\sum_{j=0}^{\\frac{W}{4}-1}{SSIM(x_{ij}, y_{ij})}<br>$$</p>\n<p>$$<br>N=(\\frac{H}{4}-1)(\\frac{W}{4}-1)<br>$$</p>\n<h3 id=\"编码过程中的技巧\"><a href=\"#编码过程中的技巧\" class=\"headerlink\" title=\"编码过程中的技巧\"></a>编码过程中的技巧</h3><p>在FFMpeg计算$SSIM$的算法实现中，为了提升效率和抽象代码逻辑，也利用了很多的编程技巧，例如：</p>\n<ul>\n<li>计算YUV各分量图像宽度时用<code>w &gt;&gt; !!i</code></li>\n<li>为了避免对第0行的特殊处理，采用两层循环来处理</li>\n<li>计算每一行的各块的sums信息时，为了降低循环次数，每次循环计算2个块的sums结果，<code>ssim_4x4x2_core</code>的函数名可能就是这么来的。</li>\n<li>计算每一行的各块的$SSIM$时，为了降低循环次数，每次循环计算4个块的$SSIM$，<code>ssim_end4</code>的函数名可能就是这么来的。</li>\n</ul>\n<h2 id=\"感谢\"><a href=\"#感谢\" class=\"headerlink\" title=\"感谢\"></a>感谢</h2><p>分析过程离不开和贤杰(github: <a href=\"https://github.com/bodhisatan\" target=\"_blank\" rel=\"noopener\">@bodhisatan</a>)的不断讨论和交流，感谢@贤杰在繁忙的工作之余抽出时间来一起分析FFMpeg中SSIM算法的实现原理。</p>"},{"title":"图像金字塔简介","reward":false,"date":"2020-03-18T13:38:18.000Z","_content":"\n# 图像金字塔\n## 图像金字塔的概念\n一般而言，我们处理的图像通常是恒定大小（分辨率）的图像。但是在某些特殊的场景下，需要处理同一图像的不同分辨率的图像。例如，在图像中搜索某些内容时（例如面部），由于不确定待搜索的内容在图像中的区域大小，因此需要创建一个具有不同分辨率的图像集，并在该图像集的所有图像中执行搜索操作。\n\n这些具有不同分辨率的图像集称为“图像金字塔”。之所以称其为“图像金字塔”，是因为当这些图像按照分辨率由低到高堆叠在一起时，底部的最大图像和顶部的最小图像看起来就像是一座金字塔。\n\n<!--more-->\n\n因此，图像金字塔是图像的集合，这些图像集合由单个原始图像通过连续的降采样——直到达到某个期望的暂停点为止——产生。\n\n\n\n![](1.jpg)\n\n## 图像金字塔的分类\n应用中经常用到的图像金字塔主要有两种：\n* 高斯金字塔\n* 拉普拉斯金字塔\n\n高斯金字塔一般用于图像的降采样，而拉普拉斯金字塔则用于通过图像金字塔中的低分辨率图像重构高分率图像的图像上采样。\n\n### 高斯金字塔\n高斯金字塔中的低分辨率图像是通过删除高分辨率图像中的行和列而产生，具体如下所示：\n* 令高斯金字塔中第$i$层的图像为$G_i$\n* 使用一个高斯核对$G_i$执行卷积操作得到$G_i^{\\prime}$\n* 然后删除$G_i^{\\prime}$中的偶数行和偶数列，得到$G_{i+1}$\n\n根据如上的步骤，可以知道，第$i+1$级图像的分辨率是第$i$级图像的分辨率的$\\frac{1}{4}$。\n\n#### pyrDown\n在OpenCV中，可以使用`cv2.pyrDown()`生成图像的上一层级图像。\n\n```python\nhigher_resolution = cv2.imread('img.jpg')\nlower_resolution  = cv2.pyrDown(higher_resolution)\n```\n\n下图是一个4级的高斯金字塔的例子。\n![](2.jpg)\n\n#### pyrUp\n我们还可以使用`cv2.pyrUp()`将图像转换为每个方向两倍大小的图像，该函数\n* 首先将图像的大小在各个维度上扩展2倍\n* 然后对新增的行用0进行填充\n* 最后再使用高斯滤波器执行卷积计算来获取“丢失”的像素的值\n\n```python\nhigher_resolution_2 = cv2.pyrUp(lower_resolution)\n```\n\n从如上的步骤不难发现，`cv2.pyrUp()`并不是`cv2.pyrDown()`的逆运算，因为`cv2.pyrDown()`是一个丢失信息的操作，一旦分辨率降低，就会存在信息丢失。\n\n因此，在示例代码中，`higher_resolution_2`和`higher_resolution`是不同的。\n\n对[pyrDown](#pyrDown)中的图像执行`pyrUp()`效果如下所示：\n![](3.jpg)\n\n### 拉普拉斯金字塔\n为了从图像金字塔中的低分辨率图像恢复高分辨率的图像，需要使用下采样过程中丢弃的信息。而这些数据则构成了拉普拉斯金字塔。\n\n令$L_i$为高斯金字塔的第$i$级图像$G_i$下采样得到第$i+1$级图像$G_{i+1}$时丢失的信息，则：\n\n$$L_i=G_i - cv2.pyrUp(G_{i+1})$$\n\n$\\\\{L_i | i \\in 1...n\\\\}$则构成拉普拉斯金字塔。\n\n拉普拉斯金字塔由高斯金字塔而形成，并没有其它的额外功能，并且拉普拉斯金字塔图像和图像的边缘图像很像。在拉普拉斯金字塔中的图像的大多数元素为零。一个4级拉普拉斯金字塔的图像如下所示（已调整图像的曝光度以增强图像内容）：\n![](4.jpg)\n\n## 尺度空间和图像金字塔卷积核的选择\n在现实世界中，客观物体在不同尺度时有着不同的结构。这意味着，如果从不同的尺度去观察同一个物体，会得出不一样的结果。例如，从不同的距离观察同一个图像时，随着距离的不断缩小，能够观察到的图像的结构也有所不同。\n\n观察一棵树的尺度和观察一片树叶的尺度也是不同的：观察一棵树的适当尺度应该是**米**，而观察一片叶子可能需要更细粒度的尺度才能得出较好的结果。\n\n当计算机系统要对一个未知的场景进行分析时，并不能提前预知用什么样的尺度来描述图像信息中的**interesting structures**是最合适的。因此，唯一可行的方案就是将多个不同尺度的描述都考虑进来，以便捕获未知的尺度变化。\n\n尺度空间理论和生物视觉之间也有着十分密切的联系。哺乳动物的视网膜以及视觉皮层第一阶段所记录的接受场的分布，与许多尺度空间操作都高度近似。\n\n如[高斯金字塔](#高斯金字塔)的描述，在生成高斯金字塔时，会采用高斯核(*filter*)对图像进行处理，那么为什么非要采用高斯核呢？\n\n实际上，并非任何低通滤波器（*low-pass filter*）都可用于生成尺度空间。可用于生成尺度空间的filter必须满足如下的条件：\n\n> 由该平滑filter生成的粗尺度图像（高层图像）不会引入不存在于细尺度图像（低层图像）中的杂散结构。\n\n如上的条件的言外之意就是：给定粗尺度图像中的任何一个区域，细尺度图像上总能找到相应的区域。对这两个区域而言，粗尺度图像区域不能有新的结构。\n\n受制于[尺度空间公理](https://en.wikipedia.org/wiki/Scale-space_axioms)，高斯卷积核是实现尺度变换的唯一线性核。因此，在图像金字塔中，需要采用高斯卷积核对图像进行处理。\n\n[尺度空间公理](https://en.wikipedia.org/wiki/Scale-space_axioms)需要满足如下的条件：\n* 线性\n* 平移不变性\n* 半群特性\n* 旋转不变性\n* 尺度不变性\n* 正定性\n* 正规性(积分为1)\n* 不会引入新的极点\n* 不会增强极点\n* 存在无穷小的算子（可微性）\n\n## 图像金字塔的应用\n图像金字塔的一种应用是图像融合。例如，在图像拼接中会将两个图像堆叠，但是由于图像之间的不连续性，这种对原始图像的直接拼接的效果并不好。例如，我们对如下图所示的两幅图像：\n\n![](5.jpg)\n\n的直接拼接效果和采用图像金字塔拼接效果分别为（左图为直接拼接）：\n\n![](8.jpg)\n\n由此可以看出，使用“图像金字塔”融合图像则可以让拼接之后的图像看起来天衣无缝。\n\n可以按照如下的步骤使用“图像金字塔”来拼接如上的图像（[玩转多尺度图像融合](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/03/18/introduction-to-image-pyramid/image_pyramid_blend.ipynb))：\n\n1. 加载图像\n2. 计算图像的高斯金字塔（在此示例中，级数为6）\n3. 从高斯金字塔中计算拉普拉斯金字塔\n4. 在每个拉普拉斯金字塔中加入苹果的左半部分和橙子的右半部分\n5. 最后，从联合图像金字塔中重建原始图像\n\n除了如上的**多分辨率图像融合算法会用到图像金字塔**之外，图像金字塔还可用于如下的场景：\n* sift算法\n* 在from coarse to fine由粗到精的搜索策略中都可以用金字塔\n* optical flow光流法\n* slam当中的姿态估计 \n* ……\n","source":"_posts/introduction-to-image-pyramid.md","raw":"---\ntitle: 图像金字塔简介\nreward: false\ndate: 2020-03-18 21:38:18\ncategories: 视觉技术\ntags: \n  - 图像金字塔\n  - 高斯金字塔\n  - 拉普拉斯金字塔\n  - image pyramid\n---\n\n# 图像金字塔\n## 图像金字塔的概念\n一般而言，我们处理的图像通常是恒定大小（分辨率）的图像。但是在某些特殊的场景下，需要处理同一图像的不同分辨率的图像。例如，在图像中搜索某些内容时（例如面部），由于不确定待搜索的内容在图像中的区域大小，因此需要创建一个具有不同分辨率的图像集，并在该图像集的所有图像中执行搜索操作。\n\n这些具有不同分辨率的图像集称为“图像金字塔”。之所以称其为“图像金字塔”，是因为当这些图像按照分辨率由低到高堆叠在一起时，底部的最大图像和顶部的最小图像看起来就像是一座金字塔。\n\n<!--more-->\n\n因此，图像金字塔是图像的集合，这些图像集合由单个原始图像通过连续的降采样——直到达到某个期望的暂停点为止——产生。\n\n\n\n![](1.jpg)\n\n## 图像金字塔的分类\n应用中经常用到的图像金字塔主要有两种：\n* 高斯金字塔\n* 拉普拉斯金字塔\n\n高斯金字塔一般用于图像的降采样，而拉普拉斯金字塔则用于通过图像金字塔中的低分辨率图像重构高分率图像的图像上采样。\n\n### 高斯金字塔\n高斯金字塔中的低分辨率图像是通过删除高分辨率图像中的行和列而产生，具体如下所示：\n* 令高斯金字塔中第$i$层的图像为$G_i$\n* 使用一个高斯核对$G_i$执行卷积操作得到$G_i^{\\prime}$\n* 然后删除$G_i^{\\prime}$中的偶数行和偶数列，得到$G_{i+1}$\n\n根据如上的步骤，可以知道，第$i+1$级图像的分辨率是第$i$级图像的分辨率的$\\frac{1}{4}$。\n\n#### pyrDown\n在OpenCV中，可以使用`cv2.pyrDown()`生成图像的上一层级图像。\n\n```python\nhigher_resolution = cv2.imread('img.jpg')\nlower_resolution  = cv2.pyrDown(higher_resolution)\n```\n\n下图是一个4级的高斯金字塔的例子。\n![](2.jpg)\n\n#### pyrUp\n我们还可以使用`cv2.pyrUp()`将图像转换为每个方向两倍大小的图像，该函数\n* 首先将图像的大小在各个维度上扩展2倍\n* 然后对新增的行用0进行填充\n* 最后再使用高斯滤波器执行卷积计算来获取“丢失”的像素的值\n\n```python\nhigher_resolution_2 = cv2.pyrUp(lower_resolution)\n```\n\n从如上的步骤不难发现，`cv2.pyrUp()`并不是`cv2.pyrDown()`的逆运算，因为`cv2.pyrDown()`是一个丢失信息的操作，一旦分辨率降低，就会存在信息丢失。\n\n因此，在示例代码中，`higher_resolution_2`和`higher_resolution`是不同的。\n\n对[pyrDown](#pyrDown)中的图像执行`pyrUp()`效果如下所示：\n![](3.jpg)\n\n### 拉普拉斯金字塔\n为了从图像金字塔中的低分辨率图像恢复高分辨率的图像，需要使用下采样过程中丢弃的信息。而这些数据则构成了拉普拉斯金字塔。\n\n令$L_i$为高斯金字塔的第$i$级图像$G_i$下采样得到第$i+1$级图像$G_{i+1}$时丢失的信息，则：\n\n$$L_i=G_i - cv2.pyrUp(G_{i+1})$$\n\n$\\\\{L_i | i \\in 1...n\\\\}$则构成拉普拉斯金字塔。\n\n拉普拉斯金字塔由高斯金字塔而形成，并没有其它的额外功能，并且拉普拉斯金字塔图像和图像的边缘图像很像。在拉普拉斯金字塔中的图像的大多数元素为零。一个4级拉普拉斯金字塔的图像如下所示（已调整图像的曝光度以增强图像内容）：\n![](4.jpg)\n\n## 尺度空间和图像金字塔卷积核的选择\n在现实世界中，客观物体在不同尺度时有着不同的结构。这意味着，如果从不同的尺度去观察同一个物体，会得出不一样的结果。例如，从不同的距离观察同一个图像时，随着距离的不断缩小，能够观察到的图像的结构也有所不同。\n\n观察一棵树的尺度和观察一片树叶的尺度也是不同的：观察一棵树的适当尺度应该是**米**，而观察一片叶子可能需要更细粒度的尺度才能得出较好的结果。\n\n当计算机系统要对一个未知的场景进行分析时，并不能提前预知用什么样的尺度来描述图像信息中的**interesting structures**是最合适的。因此，唯一可行的方案就是将多个不同尺度的描述都考虑进来，以便捕获未知的尺度变化。\n\n尺度空间理论和生物视觉之间也有着十分密切的联系。哺乳动物的视网膜以及视觉皮层第一阶段所记录的接受场的分布，与许多尺度空间操作都高度近似。\n\n如[高斯金字塔](#高斯金字塔)的描述，在生成高斯金字塔时，会采用高斯核(*filter*)对图像进行处理，那么为什么非要采用高斯核呢？\n\n实际上，并非任何低通滤波器（*low-pass filter*）都可用于生成尺度空间。可用于生成尺度空间的filter必须满足如下的条件：\n\n> 由该平滑filter生成的粗尺度图像（高层图像）不会引入不存在于细尺度图像（低层图像）中的杂散结构。\n\n如上的条件的言外之意就是：给定粗尺度图像中的任何一个区域，细尺度图像上总能找到相应的区域。对这两个区域而言，粗尺度图像区域不能有新的结构。\n\n受制于[尺度空间公理](https://en.wikipedia.org/wiki/Scale-space_axioms)，高斯卷积核是实现尺度变换的唯一线性核。因此，在图像金字塔中，需要采用高斯卷积核对图像进行处理。\n\n[尺度空间公理](https://en.wikipedia.org/wiki/Scale-space_axioms)需要满足如下的条件：\n* 线性\n* 平移不变性\n* 半群特性\n* 旋转不变性\n* 尺度不变性\n* 正定性\n* 正规性(积分为1)\n* 不会引入新的极点\n* 不会增强极点\n* 存在无穷小的算子（可微性）\n\n## 图像金字塔的应用\n图像金字塔的一种应用是图像融合。例如，在图像拼接中会将两个图像堆叠，但是由于图像之间的不连续性，这种对原始图像的直接拼接的效果并不好。例如，我们对如下图所示的两幅图像：\n\n![](5.jpg)\n\n的直接拼接效果和采用图像金字塔拼接效果分别为（左图为直接拼接）：\n\n![](8.jpg)\n\n由此可以看出，使用“图像金字塔”融合图像则可以让拼接之后的图像看起来天衣无缝。\n\n可以按照如下的步骤使用“图像金字塔”来拼接如上的图像（[玩转多尺度图像融合](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/03/18/introduction-to-image-pyramid/image_pyramid_blend.ipynb))：\n\n1. 加载图像\n2. 计算图像的高斯金字塔（在此示例中，级数为6）\n3. 从高斯金字塔中计算拉普拉斯金字塔\n4. 在每个拉普拉斯金字塔中加入苹果的左半部分和橙子的右半部分\n5. 最后，从联合图像金字塔中重建原始图像\n\n除了如上的**多分辨率图像融合算法会用到图像金字塔**之外，图像金字塔还可用于如下的场景：\n* sift算法\n* 在from coarse to fine由粗到精的搜索策略中都可以用金字塔\n* optical flow光流法\n* slam当中的姿态估计 \n* ……\n","slug":"introduction-to-image-pyramid","published":1,"updated":"2020-03-22T12:32:13.199Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9kyh076000i1pdbc2lrc8ap","content":"<h1 id=\"图像金字塔\"><a href=\"#图像金字塔\" class=\"headerlink\" title=\"图像金字塔\"></a>图像金字塔</h1><h2 id=\"图像金字塔的概念\"><a href=\"#图像金字塔的概念\" class=\"headerlink\" title=\"图像金字塔的概念\"></a>图像金字塔的概念</h2><p>一般而言，我们处理的图像通常是恒定大小（分辨率）的图像。但是在某些特殊的场景下，需要处理同一图像的不同分辨率的图像。例如，在图像中搜索某些内容时（例如面部），由于不确定待搜索的内容在图像中的区域大小，因此需要创建一个具有不同分辨率的图像集，并在该图像集的所有图像中执行搜索操作。</p>\n<p>这些具有不同分辨率的图像集称为“图像金字塔”。之所以称其为“图像金字塔”，是因为当这些图像按照分辨率由低到高堆叠在一起时，底部的最大图像和顶部的最小图像看起来就像是一座金字塔。</p>\n<a id=\"more\"></a>\n\n<p>因此，图像金字塔是图像的集合，这些图像集合由单个原始图像通过连续的降采样——直到达到某个期望的暂停点为止——产生。</p>\n<p><img src=\"/2020/03/18/introduction-to-image-pyramid/1.jpg\" alt></p>\n<h2 id=\"图像金字塔的分类\"><a href=\"#图像金字塔的分类\" class=\"headerlink\" title=\"图像金字塔的分类\"></a>图像金字塔的分类</h2><p>应用中经常用到的图像金字塔主要有两种：</p>\n<ul>\n<li>高斯金字塔</li>\n<li>拉普拉斯金字塔</li>\n</ul>\n<p>高斯金字塔一般用于图像的降采样，而拉普拉斯金字塔则用于通过图像金字塔中的低分辨率图像重构高分率图像的图像上采样。</p>\n<h3 id=\"高斯金字塔\"><a href=\"#高斯金字塔\" class=\"headerlink\" title=\"高斯金字塔\"></a>高斯金字塔</h3><p>高斯金字塔中的低分辨率图像是通过删除高分辨率图像中的行和列而产生，具体如下所示：</p>\n<ul>\n<li>令高斯金字塔中第$i$层的图像为$G_i$</li>\n<li>使用一个高斯核对$G_i$执行卷积操作得到$G_i^{\\prime}$</li>\n<li>然后删除$G_i^{\\prime}$中的偶数行和偶数列，得到$G_{i+1}$</li>\n</ul>\n<p>根据如上的步骤，可以知道，第$i+1$级图像的分辨率是第$i$级图像的分辨率的$\\frac{1}{4}$。</p>\n<h4 id=\"pyrDown\"><a href=\"#pyrDown\" class=\"headerlink\" title=\"pyrDown\"></a>pyrDown</h4><p>在OpenCV中，可以使用<code>cv2.pyrDown()</code>生成图像的上一层级图像。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">higher_resolution = cv2.imread(<span class=\"string\">'img.jpg'</span>)</span><br><span class=\"line\">lower_resolution  = cv2.pyrDown(higher_resolution)</span><br></pre></td></tr></table></figure>\n\n<p>下图是一个4级的高斯金字塔的例子。<br><img src=\"/2020/03/18/introduction-to-image-pyramid/2.jpg\" alt></p>\n<h4 id=\"pyrUp\"><a href=\"#pyrUp\" class=\"headerlink\" title=\"pyrUp\"></a>pyrUp</h4><p>我们还可以使用<code>cv2.pyrUp()</code>将图像转换为每个方向两倍大小的图像，该函数</p>\n<ul>\n<li>首先将图像的大小在各个维度上扩展2倍</li>\n<li>然后对新增的行用0进行填充</li>\n<li>最后再使用高斯滤波器执行卷积计算来获取“丢失”的像素的值</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">higher_resolution_2 = cv2.pyrUp(lower_resolution)</span><br></pre></td></tr></table></figure>\n\n<p>从如上的步骤不难发现，<code>cv2.pyrUp()</code>并不是<code>cv2.pyrDown()</code>的逆运算，因为<code>cv2.pyrDown()</code>是一个丢失信息的操作，一旦分辨率降低，就会存在信息丢失。</p>\n<p>因此，在示例代码中，<code>higher_resolution_2</code>和<code>higher_resolution</code>是不同的。</p>\n<p>对<a href=\"#pyrDown\">pyrDown</a>中的图像执行<code>pyrUp()</code>效果如下所示：<br><img src=\"/2020/03/18/introduction-to-image-pyramid/3.jpg\" alt></p>\n<h3 id=\"拉普拉斯金字塔\"><a href=\"#拉普拉斯金字塔\" class=\"headerlink\" title=\"拉普拉斯金字塔\"></a>拉普拉斯金字塔</h3><p>为了从图像金字塔中的低分辨率图像恢复高分辨率的图像，需要使用下采样过程中丢弃的信息。而这些数据则构成了拉普拉斯金字塔。</p>\n<p>令$L_i$为高斯金字塔的第$i$级图像$G_i$下采样得到第$i+1$级图像$G_{i+1}$时丢失的信息，则：</p>\n<p>$$L_i=G_i - cv2.pyrUp(G_{i+1})$$</p>\n<p>$\\{L_i | i \\in 1…n\\}$则构成拉普拉斯金字塔。</p>\n<p>拉普拉斯金字塔由高斯金字塔而形成，并没有其它的额外功能，并且拉普拉斯金字塔图像和图像的边缘图像很像。在拉普拉斯金字塔中的图像的大多数元素为零。一个4级拉普拉斯金字塔的图像如下所示（已调整图像的曝光度以增强图像内容）：<br><img src=\"/2020/03/18/introduction-to-image-pyramid/4.jpg\" alt></p>\n<h2 id=\"尺度空间和图像金字塔卷积核的选择\"><a href=\"#尺度空间和图像金字塔卷积核的选择\" class=\"headerlink\" title=\"尺度空间和图像金字塔卷积核的选择\"></a>尺度空间和图像金字塔卷积核的选择</h2><p>在现实世界中，客观物体在不同尺度时有着不同的结构。这意味着，如果从不同的尺度去观察同一个物体，会得出不一样的结果。例如，从不同的距离观察同一个图像时，随着距离的不断缩小，能够观察到的图像的结构也有所不同。</p>\n<p>观察一棵树的尺度和观察一片树叶的尺度也是不同的：观察一棵树的适当尺度应该是<strong>米</strong>，而观察一片叶子可能需要更细粒度的尺度才能得出较好的结果。</p>\n<p>当计算机系统要对一个未知的场景进行分析时，并不能提前预知用什么样的尺度来描述图像信息中的<strong>interesting structures</strong>是最合适的。因此，唯一可行的方案就是将多个不同尺度的描述都考虑进来，以便捕获未知的尺度变化。</p>\n<p>尺度空间理论和生物视觉之间也有着十分密切的联系。哺乳动物的视网膜以及视觉皮层第一阶段所记录的接受场的分布，与许多尺度空间操作都高度近似。</p>\n<p>如<a href=\"#高斯金字塔\">高斯金字塔</a>的描述，在生成高斯金字塔时，会采用高斯核(<em>filter</em>)对图像进行处理，那么为什么非要采用高斯核呢？</p>\n<p>实际上，并非任何低通滤波器（<em>low-pass filter</em>）都可用于生成尺度空间。可用于生成尺度空间的filter必须满足如下的条件：</p>\n<blockquote>\n<p>由该平滑filter生成的粗尺度图像（高层图像）不会引入不存在于细尺度图像（低层图像）中的杂散结构。</p>\n</blockquote>\n<p>如上的条件的言外之意就是：给定粗尺度图像中的任何一个区域，细尺度图像上总能找到相应的区域。对这两个区域而言，粗尺度图像区域不能有新的结构。</p>\n<p>受制于<a href=\"https://en.wikipedia.org/wiki/Scale-space_axioms\" target=\"_blank\" rel=\"noopener\">尺度空间公理</a>，高斯卷积核是实现尺度变换的唯一线性核。因此，在图像金字塔中，需要采用高斯卷积核对图像进行处理。</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Scale-space_axioms\" target=\"_blank\" rel=\"noopener\">尺度空间公理</a>需要满足如下的条件：</p>\n<ul>\n<li>线性</li>\n<li>平移不变性</li>\n<li>半群特性</li>\n<li>旋转不变性</li>\n<li>尺度不变性</li>\n<li>正定性</li>\n<li>正规性(积分为1)</li>\n<li>不会引入新的极点</li>\n<li>不会增强极点</li>\n<li>存在无穷小的算子（可微性）</li>\n</ul>\n<h2 id=\"图像金字塔的应用\"><a href=\"#图像金字塔的应用\" class=\"headerlink\" title=\"图像金字塔的应用\"></a>图像金字塔的应用</h2><p>图像金字塔的一种应用是图像融合。例如，在图像拼接中会将两个图像堆叠，但是由于图像之间的不连续性，这种对原始图像的直接拼接的效果并不好。例如，我们对如下图所示的两幅图像：</p>\n<p><img src=\"/2020/03/18/introduction-to-image-pyramid/5.jpg\" alt></p>\n<p>的直接拼接效果和采用图像金字塔拼接效果分别为（左图为直接拼接）：</p>\n<p><img src=\"/2020/03/18/introduction-to-image-pyramid/8.jpg\" alt></p>\n<p>由此可以看出，使用“图像金字塔”融合图像则可以让拼接之后的图像看起来天衣无缝。</p>\n<p>可以按照如下的步骤使用“图像金字塔”来拼接如上的图像（<a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/03/18/introduction-to-image-pyramid/image_pyramid_blend.ipynb\" target=\"_blank\" rel=\"noopener\">玩转多尺度图像融合</a>)：</p>\n<ol>\n<li>加载图像</li>\n<li>计算图像的高斯金字塔（在此示例中，级数为6）</li>\n<li>从高斯金字塔中计算拉普拉斯金字塔</li>\n<li>在每个拉普拉斯金字塔中加入苹果的左半部分和橙子的右半部分</li>\n<li>最后，从联合图像金字塔中重建原始图像</li>\n</ol>\n<p>除了如上的<strong>多分辨率图像融合算法会用到图像金字塔</strong>之外，图像金字塔还可用于如下的场景：</p>\n<ul>\n<li>sift算法</li>\n<li>在from coarse to fine由粗到精的搜索策略中都可以用金字塔</li>\n<li>optical flow光流法</li>\n<li>slam当中的姿态估计 </li>\n<li>……</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"图像金字塔\"><a href=\"#图像金字塔\" class=\"headerlink\" title=\"图像金字塔\"></a>图像金字塔</h1><h2 id=\"图像金字塔的概念\"><a href=\"#图像金字塔的概念\" class=\"headerlink\" title=\"图像金字塔的概念\"></a>图像金字塔的概念</h2><p>一般而言，我们处理的图像通常是恒定大小（分辨率）的图像。但是在某些特殊的场景下，需要处理同一图像的不同分辨率的图像。例如，在图像中搜索某些内容时（例如面部），由于不确定待搜索的内容在图像中的区域大小，因此需要创建一个具有不同分辨率的图像集，并在该图像集的所有图像中执行搜索操作。</p>\n<p>这些具有不同分辨率的图像集称为“图像金字塔”。之所以称其为“图像金字塔”，是因为当这些图像按照分辨率由低到高堆叠在一起时，底部的最大图像和顶部的最小图像看起来就像是一座金字塔。</p>","more":"<p>因此，图像金字塔是图像的集合，这些图像集合由单个原始图像通过连续的降采样——直到达到某个期望的暂停点为止——产生。</p>\n<p><img src=\"/2020/03/18/introduction-to-image-pyramid/1.jpg\" alt></p>\n<h2 id=\"图像金字塔的分类\"><a href=\"#图像金字塔的分类\" class=\"headerlink\" title=\"图像金字塔的分类\"></a>图像金字塔的分类</h2><p>应用中经常用到的图像金字塔主要有两种：</p>\n<ul>\n<li>高斯金字塔</li>\n<li>拉普拉斯金字塔</li>\n</ul>\n<p>高斯金字塔一般用于图像的降采样，而拉普拉斯金字塔则用于通过图像金字塔中的低分辨率图像重构高分率图像的图像上采样。</p>\n<h3 id=\"高斯金字塔\"><a href=\"#高斯金字塔\" class=\"headerlink\" title=\"高斯金字塔\"></a>高斯金字塔</h3><p>高斯金字塔中的低分辨率图像是通过删除高分辨率图像中的行和列而产生，具体如下所示：</p>\n<ul>\n<li>令高斯金字塔中第$i$层的图像为$G_i$</li>\n<li>使用一个高斯核对$G_i$执行卷积操作得到$G_i^{\\prime}$</li>\n<li>然后删除$G_i^{\\prime}$中的偶数行和偶数列，得到$G_{i+1}$</li>\n</ul>\n<p>根据如上的步骤，可以知道，第$i+1$级图像的分辨率是第$i$级图像的分辨率的$\\frac{1}{4}$。</p>\n<h4 id=\"pyrDown\"><a href=\"#pyrDown\" class=\"headerlink\" title=\"pyrDown\"></a>pyrDown</h4><p>在OpenCV中，可以使用<code>cv2.pyrDown()</code>生成图像的上一层级图像。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">higher_resolution = cv2.imread(<span class=\"string\">'img.jpg'</span>)</span><br><span class=\"line\">lower_resolution  = cv2.pyrDown(higher_resolution)</span><br></pre></td></tr></table></figure>\n\n<p>下图是一个4级的高斯金字塔的例子。<br><img src=\"/2020/03/18/introduction-to-image-pyramid/2.jpg\" alt></p>\n<h4 id=\"pyrUp\"><a href=\"#pyrUp\" class=\"headerlink\" title=\"pyrUp\"></a>pyrUp</h4><p>我们还可以使用<code>cv2.pyrUp()</code>将图像转换为每个方向两倍大小的图像，该函数</p>\n<ul>\n<li>首先将图像的大小在各个维度上扩展2倍</li>\n<li>然后对新增的行用0进行填充</li>\n<li>最后再使用高斯滤波器执行卷积计算来获取“丢失”的像素的值</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">higher_resolution_2 = cv2.pyrUp(lower_resolution)</span><br></pre></td></tr></table></figure>\n\n<p>从如上的步骤不难发现，<code>cv2.pyrUp()</code>并不是<code>cv2.pyrDown()</code>的逆运算，因为<code>cv2.pyrDown()</code>是一个丢失信息的操作，一旦分辨率降低，就会存在信息丢失。</p>\n<p>因此，在示例代码中，<code>higher_resolution_2</code>和<code>higher_resolution</code>是不同的。</p>\n<p>对<a href=\"#pyrDown\">pyrDown</a>中的图像执行<code>pyrUp()</code>效果如下所示：<br><img src=\"/2020/03/18/introduction-to-image-pyramid/3.jpg\" alt></p>\n<h3 id=\"拉普拉斯金字塔\"><a href=\"#拉普拉斯金字塔\" class=\"headerlink\" title=\"拉普拉斯金字塔\"></a>拉普拉斯金字塔</h3><p>为了从图像金字塔中的低分辨率图像恢复高分辨率的图像，需要使用下采样过程中丢弃的信息。而这些数据则构成了拉普拉斯金字塔。</p>\n<p>令$L_i$为高斯金字塔的第$i$级图像$G_i$下采样得到第$i+1$级图像$G_{i+1}$时丢失的信息，则：</p>\n<p>$$L_i=G_i - cv2.pyrUp(G_{i+1})$$</p>\n<p>$\\{L_i | i \\in 1…n\\}$则构成拉普拉斯金字塔。</p>\n<p>拉普拉斯金字塔由高斯金字塔而形成，并没有其它的额外功能，并且拉普拉斯金字塔图像和图像的边缘图像很像。在拉普拉斯金字塔中的图像的大多数元素为零。一个4级拉普拉斯金字塔的图像如下所示（已调整图像的曝光度以增强图像内容）：<br><img src=\"/2020/03/18/introduction-to-image-pyramid/4.jpg\" alt></p>\n<h2 id=\"尺度空间和图像金字塔卷积核的选择\"><a href=\"#尺度空间和图像金字塔卷积核的选择\" class=\"headerlink\" title=\"尺度空间和图像金字塔卷积核的选择\"></a>尺度空间和图像金字塔卷积核的选择</h2><p>在现实世界中，客观物体在不同尺度时有着不同的结构。这意味着，如果从不同的尺度去观察同一个物体，会得出不一样的结果。例如，从不同的距离观察同一个图像时，随着距离的不断缩小，能够观察到的图像的结构也有所不同。</p>\n<p>观察一棵树的尺度和观察一片树叶的尺度也是不同的：观察一棵树的适当尺度应该是<strong>米</strong>，而观察一片叶子可能需要更细粒度的尺度才能得出较好的结果。</p>\n<p>当计算机系统要对一个未知的场景进行分析时，并不能提前预知用什么样的尺度来描述图像信息中的<strong>interesting structures</strong>是最合适的。因此，唯一可行的方案就是将多个不同尺度的描述都考虑进来，以便捕获未知的尺度变化。</p>\n<p>尺度空间理论和生物视觉之间也有着十分密切的联系。哺乳动物的视网膜以及视觉皮层第一阶段所记录的接受场的分布，与许多尺度空间操作都高度近似。</p>\n<p>如<a href=\"#高斯金字塔\">高斯金字塔</a>的描述，在生成高斯金字塔时，会采用高斯核(<em>filter</em>)对图像进行处理，那么为什么非要采用高斯核呢？</p>\n<p>实际上，并非任何低通滤波器（<em>low-pass filter</em>）都可用于生成尺度空间。可用于生成尺度空间的filter必须满足如下的条件：</p>\n<blockquote>\n<p>由该平滑filter生成的粗尺度图像（高层图像）不会引入不存在于细尺度图像（低层图像）中的杂散结构。</p>\n</blockquote>\n<p>如上的条件的言外之意就是：给定粗尺度图像中的任何一个区域，细尺度图像上总能找到相应的区域。对这两个区域而言，粗尺度图像区域不能有新的结构。</p>\n<p>受制于<a href=\"https://en.wikipedia.org/wiki/Scale-space_axioms\" target=\"_blank\" rel=\"noopener\">尺度空间公理</a>，高斯卷积核是实现尺度变换的唯一线性核。因此，在图像金字塔中，需要采用高斯卷积核对图像进行处理。</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Scale-space_axioms\" target=\"_blank\" rel=\"noopener\">尺度空间公理</a>需要满足如下的条件：</p>\n<ul>\n<li>线性</li>\n<li>平移不变性</li>\n<li>半群特性</li>\n<li>旋转不变性</li>\n<li>尺度不变性</li>\n<li>正定性</li>\n<li>正规性(积分为1)</li>\n<li>不会引入新的极点</li>\n<li>不会增强极点</li>\n<li>存在无穷小的算子（可微性）</li>\n</ul>\n<h2 id=\"图像金字塔的应用\"><a href=\"#图像金字塔的应用\" class=\"headerlink\" title=\"图像金字塔的应用\"></a>图像金字塔的应用</h2><p>图像金字塔的一种应用是图像融合。例如，在图像拼接中会将两个图像堆叠，但是由于图像之间的不连续性，这种对原始图像的直接拼接的效果并不好。例如，我们对如下图所示的两幅图像：</p>\n<p><img src=\"/2020/03/18/introduction-to-image-pyramid/5.jpg\" alt></p>\n<p>的直接拼接效果和采用图像金字塔拼接效果分别为（左图为直接拼接）：</p>\n<p><img src=\"/2020/03/18/introduction-to-image-pyramid/8.jpg\" alt></p>\n<p>由此可以看出，使用“图像金字塔”融合图像则可以让拼接之后的图像看起来天衣无缝。</p>\n<p>可以按照如下的步骤使用“图像金字塔”来拼接如上的图像（<a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/03/18/introduction-to-image-pyramid/image_pyramid_blend.ipynb\" target=\"_blank\" rel=\"noopener\">玩转多尺度图像融合</a>)：</p>\n<ol>\n<li>加载图像</li>\n<li>计算图像的高斯金字塔（在此示例中，级数为6）</li>\n<li>从高斯金字塔中计算拉普拉斯金字塔</li>\n<li>在每个拉普拉斯金字塔中加入苹果的左半部分和橙子的右半部分</li>\n<li>最后，从联合图像金字塔中重建原始图像</li>\n</ol>\n<p>除了如上的<strong>多分辨率图像融合算法会用到图像金字塔</strong>之外，图像金字塔还可用于如下的场景：</p>\n<ul>\n<li>sift算法</li>\n<li>在from coarse to fine由粗到精的搜索策略中都可以用金字塔</li>\n<li>optical flow光流法</li>\n<li>slam当中的姿态估计 </li>\n<li>……</li>\n</ul>"},{"title":"The Proof of the ssim_end1() in FFMpeg","reward":false,"date":"2020-02-18T01:41:36.000Z","_content":"\n$$\nSSIM(a,b)=\\frac{(2\\mu_a\\mu_b+C_1)(2\\sigma_{ab}+C_2)}{(\\mu_a^2+\\mu_b^2+C_1)(\\sigma_a^2+\\sigma_b^2+C_2)}\n$$\n\n\n$$\n\\mu_a=\\frac{1}{64}fs1\n$$\n\n<!--more-->\n\n$$\n\\mu_b=\\frac{1}{64}fs2\n$$\n\n$$\n\\sigma_a^2+\\sigma_b^2=\\frac{1}{63}(\\sum_{i,j}(a(i,j)-\\mu_a)^2 + \\sum_{i,j}(b(i,j)-\\mu_b)^2)\n$$\n\n$$\n=\\frac{1}{63}\\sum_{i,j}(a(i,j)^2+\\mu_a^2-2 \\cdot a(i,j) \\cdot \\mu_a+b(i,j)^2+\\mu_b^2-2 \\cdot b(i,j) \\cdot \\mu_b)\n$$\n\n$$\n=\\frac{1}{63}(\\sum_{i,j}(a(i,j)^2+b(i,j)^2)-2(\\mu_a\\sum_{i,j}a(i,j)+\\mu_b\\sum_{i,j}b(i,j))+\\sum_{i,j}\n(\\mu_a^2+\\mu_b^2))\n$$\n$$\n=\\frac{1}{63}(fss-2 \\cdot \\frac{1}{64}(fs1^2+fs2^2)+\\frac{1}{64}(fs1^2+fs2^2)\n$$\n\n$$\n=\\frac{1}{63}(fss-\\frac{1}{64}(fs1^2+fs2^2))\n$$\n\n$$\n=\\frac{1}{63}\\frac{1}{64}(64fss-fs1^2-fs2^2)\n$$\n\n$$\n\\sigma_{ab}=\\frac{1}{63}\\sum_{i,j}((a(i,j)-\\mu_a)(b(i,j)-\\mu_b))\n$$\n\n$$\n=\\frac{1}{63}\\sum_{i,j}(a(i,j) \\cdot b(i,j)-a(i,j)\\mu_b-b(i,j)\\mu_a+\\mu_a \\cdot \\mu_b)\n$$\n\n$$\n=\\frac{1}{63}(fs12-fs1 \\cdot \\mu_b-fs2 \\cdot \\mu_a+fs1 \\cdot \\mu_b)\n$$\n\n$$\n=\\frac{1}{63}(fs12-\\frac{1}{64}fs1 \\cdot fs2)\n$$\n\n$$\n=\\frac{1}{63}\\frac{1}{64}(64fs12-fs1 \\cdot fs2)\n$$\n\n$$\n\\therefore SSIM(a,b)=\\frac{(2fs1 \\cdot fs2+64^2C_1)(2 \\cdot 64fs12-2fs1fs2+63 \\cdot 64C_2)}{(fs1^2+fs2^2+64^2C_1)(64fss-fs1^2-fs2^2+63 \\cdot 64C_2)}\n$$","source":"_posts/the-proof-of-the-SSIM-in-FFMpeg.md","raw":"---\ntitle: The Proof of the ssim_end1() in FFMpeg\nreward: false\ndate: 2020-02-18 09:41:36\ncategories: \ntags: \n  - FFMpeg\n  - SSIM\n---\n\n$$\nSSIM(a,b)=\\frac{(2\\mu_a\\mu_b+C_1)(2\\sigma_{ab}+C_2)}{(\\mu_a^2+\\mu_b^2+C_1)(\\sigma_a^2+\\sigma_b^2+C_2)}\n$$\n\n\n$$\n\\mu_a=\\frac{1}{64}fs1\n$$\n\n<!--more-->\n\n$$\n\\mu_b=\\frac{1}{64}fs2\n$$\n\n$$\n\\sigma_a^2+\\sigma_b^2=\\frac{1}{63}(\\sum_{i,j}(a(i,j)-\\mu_a)^2 + \\sum_{i,j}(b(i,j)-\\mu_b)^2)\n$$\n\n$$\n=\\frac{1}{63}\\sum_{i,j}(a(i,j)^2+\\mu_a^2-2 \\cdot a(i,j) \\cdot \\mu_a+b(i,j)^2+\\mu_b^2-2 \\cdot b(i,j) \\cdot \\mu_b)\n$$\n\n$$\n=\\frac{1}{63}(\\sum_{i,j}(a(i,j)^2+b(i,j)^2)-2(\\mu_a\\sum_{i,j}a(i,j)+\\mu_b\\sum_{i,j}b(i,j))+\\sum_{i,j}\n(\\mu_a^2+\\mu_b^2))\n$$\n$$\n=\\frac{1}{63}(fss-2 \\cdot \\frac{1}{64}(fs1^2+fs2^2)+\\frac{1}{64}(fs1^2+fs2^2)\n$$\n\n$$\n=\\frac{1}{63}(fss-\\frac{1}{64}(fs1^2+fs2^2))\n$$\n\n$$\n=\\frac{1}{63}\\frac{1}{64}(64fss-fs1^2-fs2^2)\n$$\n\n$$\n\\sigma_{ab}=\\frac{1}{63}\\sum_{i,j}((a(i,j)-\\mu_a)(b(i,j)-\\mu_b))\n$$\n\n$$\n=\\frac{1}{63}\\sum_{i,j}(a(i,j) \\cdot b(i,j)-a(i,j)\\mu_b-b(i,j)\\mu_a+\\mu_a \\cdot \\mu_b)\n$$\n\n$$\n=\\frac{1}{63}(fs12-fs1 \\cdot \\mu_b-fs2 \\cdot \\mu_a+fs1 \\cdot \\mu_b)\n$$\n\n$$\n=\\frac{1}{63}(fs12-\\frac{1}{64}fs1 \\cdot fs2)\n$$\n\n$$\n=\\frac{1}{63}\\frac{1}{64}(64fs12-fs1 \\cdot fs2)\n$$\n\n$$\n\\therefore SSIM(a,b)=\\frac{(2fs1 \\cdot fs2+64^2C_1)(2 \\cdot 64fs12-2fs1fs2+63 \\cdot 64C_2)}{(fs1^2+fs2^2+64^2C_1)(64fss-fs1^2-fs2^2+63 \\cdot 64C_2)}\n$$","slug":"the-proof-of-the-SSIM-in-FFMpeg","published":1,"updated":"2020-02-18T02:32:41.736Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9kyh079000m1pdbhnef6ikm","content":"<p>$$<br>SSIM(a,b)=\\frac{(2\\mu_a\\mu_b+C_1)(2\\sigma_{ab}+C_2)}{(\\mu_a^2+\\mu_b^2+C_1)(\\sigma_a^2+\\sigma_b^2+C_2)}<br>$$</p>\n<p>$$<br>\\mu_a=\\frac{1}{64}fs1<br>$$</p>\n<a id=\"more\"></a>\n\n<p>$$<br>\\mu_b=\\frac{1}{64}fs2<br>$$</p>\n<p>$$<br>\\sigma_a^2+\\sigma_b^2=\\frac{1}{63}(\\sum_{i,j}(a(i,j)-\\mu_a)^2 + \\sum_{i,j}(b(i,j)-\\mu_b)^2)<br>$$</p>\n<p>$$<br>=\\frac{1}{63}\\sum_{i,j}(a(i,j)^2+\\mu_a^2-2 \\cdot a(i,j) \\cdot \\mu_a+b(i,j)^2+\\mu_b^2-2 \\cdot b(i,j) \\cdot \\mu_b)<br>$$</p>\n<p>$$<br>=\\frac{1}{63}(\\sum_{i,j}(a(i,j)^2+b(i,j)^2)-2(\\mu_a\\sum_{i,j}a(i,j)+\\mu_b\\sum_{i,j}b(i,j))+\\sum_{i,j}<br>(\\mu_a^2+\\mu_b^2))<br>$$<br>$$<br>=\\frac{1}{63}(fss-2 \\cdot \\frac{1}{64}(fs1^2+fs2^2)+\\frac{1}{64}(fs1^2+fs2^2)<br>$$</p>\n<p>$$<br>=\\frac{1}{63}(fss-\\frac{1}{64}(fs1^2+fs2^2))<br>$$</p>\n<p>$$<br>=\\frac{1}{63}\\frac{1}{64}(64fss-fs1^2-fs2^2)<br>$$</p>\n<p>$$<br>\\sigma_{ab}=\\frac{1}{63}\\sum_{i,j}((a(i,j)-\\mu_a)(b(i,j)-\\mu_b))<br>$$</p>\n<p>$$<br>=\\frac{1}{63}\\sum_{i,j}(a(i,j) \\cdot b(i,j)-a(i,j)\\mu_b-b(i,j)\\mu_a+\\mu_a \\cdot \\mu_b)<br>$$</p>\n<p>$$<br>=\\frac{1}{63}(fs12-fs1 \\cdot \\mu_b-fs2 \\cdot \\mu_a+fs1 \\cdot \\mu_b)<br>$$</p>\n<p>$$<br>=\\frac{1}{63}(fs12-\\frac{1}{64}fs1 \\cdot fs2)<br>$$</p>\n<p>$$<br>=\\frac{1}{63}\\frac{1}{64}(64fs12-fs1 \\cdot fs2)<br>$$</p>\n<p>$$<br>\\therefore SSIM(a,b)=\\frac{(2fs1 \\cdot fs2+64^2C_1)(2 \\cdot 64fs12-2fs1fs2+63 \\cdot 64C_2)}{(fs1^2+fs2^2+64^2C_1)(64fss-fs1^2-fs2^2+63 \\cdot 64C_2)}<br>$$</p>\n","site":{"data":{}},"excerpt":"<p>$$<br>SSIM(a,b)=\\frac{(2\\mu_a\\mu_b+C_1)(2\\sigma_{ab}+C_2)}{(\\mu_a^2+\\mu_b^2+C_1)(\\sigma_a^2+\\sigma_b^2+C_2)}<br>$$</p>\n<p>$$<br>\\mu_a=\\frac{1}{64}fs1<br>$$</p>","more":"<p>$$<br>\\mu_b=\\frac{1}{64}fs2<br>$$</p>\n<p>$$<br>\\sigma_a^2+\\sigma_b^2=\\frac{1}{63}(\\sum_{i,j}(a(i,j)-\\mu_a)^2 + \\sum_{i,j}(b(i,j)-\\mu_b)^2)<br>$$</p>\n<p>$$<br>=\\frac{1}{63}\\sum_{i,j}(a(i,j)^2+\\mu_a^2-2 \\cdot a(i,j) \\cdot \\mu_a+b(i,j)^2+\\mu_b^2-2 \\cdot b(i,j) \\cdot \\mu_b)<br>$$</p>\n<p>$$<br>=\\frac{1}{63}(\\sum_{i,j}(a(i,j)^2+b(i,j)^2)-2(\\mu_a\\sum_{i,j}a(i,j)+\\mu_b\\sum_{i,j}b(i,j))+\\sum_{i,j}<br>(\\mu_a^2+\\mu_b^2))<br>$$<br>$$<br>=\\frac{1}{63}(fss-2 \\cdot \\frac{1}{64}(fs1^2+fs2^2)+\\frac{1}{64}(fs1^2+fs2^2)<br>$$</p>\n<p>$$<br>=\\frac{1}{63}(fss-\\frac{1}{64}(fs1^2+fs2^2))<br>$$</p>\n<p>$$<br>=\\frac{1}{63}\\frac{1}{64}(64fss-fs1^2-fs2^2)<br>$$</p>\n<p>$$<br>\\sigma_{ab}=\\frac{1}{63}\\sum_{i,j}((a(i,j)-\\mu_a)(b(i,j)-\\mu_b))<br>$$</p>\n<p>$$<br>=\\frac{1}{63}\\sum_{i,j}(a(i,j) \\cdot b(i,j)-a(i,j)\\mu_b-b(i,j)\\mu_a+\\mu_a \\cdot \\mu_b)<br>$$</p>\n<p>$$<br>=\\frac{1}{63}(fs12-fs1 \\cdot \\mu_b-fs2 \\cdot \\mu_a+fs1 \\cdot \\mu_b)<br>$$</p>\n<p>$$<br>=\\frac{1}{63}(fs12-\\frac{1}{64}fs1 \\cdot fs2)<br>$$</p>\n<p>$$<br>=\\frac{1}{63}\\frac{1}{64}(64fs12-fs1 \\cdot fs2)<br>$$</p>\n<p>$$<br>\\therefore SSIM(a,b)=\\frac{(2fs1 \\cdot fs2+64^2C_1)(2 \\cdot 64fs12-2fs1fs2+63 \\cdot 64C_2)}{(fs1^2+fs2^2+64^2C_1)(64fss-fs1^2-fs2^2+63 \\cdot 64C_2)}<br>$$</p>"},{"title":"对QA的思考——我的这些年","date":"2020-02-05T04:12:37.000Z","reward":false,"_content":"\n2012年毕业后怀着一颗忐忑的心开始了测试工作的职业生涯，到现在也有7年啦。经历过奋斗和激情，也经历过徘徊和迷茫，勤恳却不庸碌。虽然对测试也有担忧，但是对这个行业始终保持着一种激情。\n\n接下来结合自己的工作谈一下对测试的一些思考，也回顾下自己过去的测试工作中遵循的一些工作原则。\n\n![](1.jpg)\n\n<!--more-->\n\n## 原则1：正确的认识自己\n好多人，自视甚高，傲视一切。但是，实际上，牛人很多。我们能做成一件事情，能力是一个方面，但能力绝对不是全部因素。没有上级的支持，没有同事的协作和配合，没有公司提供的资源，我们是什么事情也做不成的。不能因为自己的工作时间长，对业务了解的深，就对别人横加指责，也不能因为自己职称高就给别人以压力。\n\n![](2.png)\n\n## 原则2：先成就别人，再成就自己\n我们经常问QA的价值是什么？\n\n实际上有些东西是很难讲清楚的。好多QA为了证明自己的技术能力、技术价值，做了很多看似高超，但是实际上对业务快速迭代并没有用的事情。\n\n我总觉得，自己的价值不是由自己体现的，而是由别人来体现的。只有先成就别人，才能最终成就自己。这就是我工作以来一直遵循的一个工作原则。不争，不抢，认真做事，认真帮别人做事，认真的帮业务做事情，帮助RD分析线上问题，解决BUG。到了后来，也颇有点：桃李不言，下自成蹊的味道。\n\n好多人也会说，在这个时代，酒香也怕巷子深。如果不主动宣传自己的价值，宣传自己的影响力，别人是不会知道的。但是，实际上，影响力和价值是由别人来评估的，历史交由后人评就是这个道理。并且，影响力是处于某个圈层的，并通过圈层而扩散。我们首先在团队的圈层内做到影响力，才有可能通过圈层的扩散而扩散自己的价值。\n\n![](3.jpg)\n\n## 原则3：方便别人，方便自己\n做测试的同学，每天会写很多测试case，每天也会发很多测试报告，但是如果打开这些邮件，经常发现这些case很难让人理解，这些测试报告虽然篇幅很大，但是如果要从中找到接下来RD要做什么，还是要花费一点时间的。\n\n在测试工作过程中，无论是测试报告，还是风险通报，首先要了解到这些内容的接收者到底是谁，然后需要思考，要以什么样的描述和组织才能方便对方快速了解到内容的含义，也就是要做到：到什么山头，唱什么歌。\n\n我刚入职的时候，因为之前没有做过测试，也不知道测试报告要以什么样的格式来发，所以很害怕发测试报告。虽然学习了其他前辈的测试报告和前辈们的测试报告范式，但是总感觉还是害怕。因为，从RD的角度来看，阅读那些测试报告的成本还是很大的。\n\n然后，我站在RD的角度上想，怎么组织测试报告，才能让RD一眼就知道自己接下来要做什么呢？然后我重新组织了之前测测试报告，在报告的最开始用1句话概括项目的测试结论：该项目测试是否通过，原因是什么？具体的BUG量是多少？然后接下来是详细的测试清单，清单中明确表明每一个case的测试点，前置，预期结果，实际测试结果，是否通过等信息。对于测试通过的case，用绿色来标注；对于测试失败的case，用红色标注；对于有疑问的case，比如建议等，用黄色标注。RD收到测试报告之后，先看结论，然后看非绿色的测试case修复BUG。\n\n## 原则4：消除门槛，而不是提升门槛\n大禹治水之所以成功，最重要的思想就是：改“堵”为“疏”。\n\n但是真正的测试过程中，经常会发现如下的现象：领导质疑QA的测试时间太长，QA质疑RD的提测质量太差，然后QA增加了准入打回的流程，对于RD提测的、没有通过准入的项目不予测试。\n\n实际上，这种现象就是QA在项目的测试环节设置了一个门槛，而如果这种门槛设置的越来越多，那么对于业务而言也会发生“洪涝灾害”。单从测试团队的角度看，确实增加这个门槛，测试效率提升了，测试时间降低了，但是如果从整个业务的迭代来看，效率并没有提升。\n\n我们提了门槛，发了准入case，增加了流程，这一点也不能说不好。但是我们有没有想过：这些准入case别人看的懂吗？我们怎么设计case能够使得RD自测的时候更加方便呢？即便看懂了，我们怎么保证RD确实执行了呢？我们怎么保证增加的这个流程每次都贯彻落实了呢？\n\nQA不应该是一个门槛设置者，而应该是一种有效的武器。不能用看似合理的各种标准和要求卡住事情，而应该成为帮助团队解决问题的秘密武器。\n\nQA不能只报问题，只说现象，只说RD不靠谱，上线不检查。QA需要思考：\n* 我们能为快速迭代做些什么？\n* 我们能做些什么可以帮助RD快速发现自己的代码问题？\n* 我们能做什么可以帮助RD快速发现上线过程异常了？\n\n## 原则5：不断学习，不做伪学习者\n现在，技术更新换代的频率很快，所以一定要不断的学习，持续的学习。还记得刚入职的时候，那时候就会c/c++，java。现在看看自己的github和内网的提交记录，发现语言层面基本上涵盖了：c/c++，java，php，python，go，javascript，shell，object-c，swift，lua。项目类型覆盖了服务端，策略端，iOS，Android，自动化等不同的方向。从原来只会用IIS的小白，到今天了解Apache，Lighttpd，Nginx等各种web服务器。从原来的服务端，到慢慢了解前端，了解策略端，了解客户端。\n\n参加过好多会，和好多人沟通过，大家都希望组织提供学习的机会，提供分享的机会，希望组织能提供自己技术成长的机会。但是，实际上，成长是自己的事，学习是个人的事，为什么要求组织给提供这些机会呢？我组织过很长时间的分享，我也最反对组织大规模的技术学习和分享，因为让太多的伪学习者进入之后，效果并不理想。相反，规模较小的沙龙和讨论对个人的技术成长效果更为明显。伪学习者只是想学习而已，只是想让组织提供学习机会而已，仅此而已。\n\n学习是自己的事情，白天求生存，晚上求发展。凌晨2点的时候，也曾在灯下啃过Nginx的源码，分析一个个的线上问题，没有这些个凌晨2点就不会有《Nginx沉思录》，《Nginx洗冤录》等一系列的总结。\n\n![](4.jpg)\n\n## 原则6：做积极的抱怨者，不做消极的抱怨者\n没有“抱怨”就不会有技术的进步，但是反过来说没有“抱怨”就不会存在那么多的失败，一无是处，庸庸碌碌。关键是要看待我们对待“抱怨”的态度。当“抱怨”还仅仅是停留在嘴上，停留在思想上，被“抱怨”所控制时，基本上也就离碌碌无为不远了。当我们开始和“抱怨”对抗时，利用一切技术手段和努力消灭“抱怨”时，技术就开始进步了。所以，我一般不反对抱怨，并一直鼓励大家抱怨，但是接下来我会仔细分析抱怨的原因是什么？技术上需要什么探索才能消灭这些抱怨？\n\n每一个“抱怨”都是一次机会，都是一个机遇，都是一次成长。逢山开路，遇水搭桥，勇往直前。\n\n![](5.jpg)\n\n## 原则7：正确认识QA和RD的异同\n天分日夜黑白，季节有春夏秋冬，虽然全栈模式有他的优点，但是四季也有四季的美。文能提笔安天下，武能上马定乾坤。关键是，我们需要对QA和RD有正确的认识。\n\n我们不能按照一个原则来评定事情。比如：老驴拉磨，始终走不出那个圈圈。和日行千里的良马相比，当然老驴要羞愧了。但是如果换个角度，拉磨一天，产面千斤。良马再能跑，也跑不出千斤面。\n\nQA和RD虽然都是研发岗，都是技术体系，那他们的差异在哪呢？\n\n我觉得有以下几点吧：\n* 从工作关系上讲，QA是RD的下游，RD是QA的客户。\n* 从工作重点上讲，QA注重分析，而RD注重实践。RD要做的是解决问题，采用任何可能的方法，快速实现需求，快速上线。而QA没有太大的业务压力，可以有更多的时间来思考每一种技术的优势和劣势，有更多的时间来仔细分析和追查每一个bug的深层原因，帮助RD在快速解决问题后可以彻底解决问题，锦上添花。\n* 从涵盖业务上讲，QA关注的业务广度较高，而RD关注业务的深度更高。\n\n实际上，技术是一个很广泛的概念，不能认为只有写代码，设计架构才是技术，我们的思维本身就是一种技术。关键是，我们的技术要为业务的发展而存在，而不是孤立的存在。\n\n![](6.jpg)\n\n","source":"_posts/thinking-about-QA-my-years-for-work-as-a-QA.md","raw":"---\ntitle: '对QA的思考——我的这些年'\ndate: 2020-02-05 12:12:37\nreward: false\ncategories: 总结\ntags: 测试工作\n---\n\n2012年毕业后怀着一颗忐忑的心开始了测试工作的职业生涯，到现在也有7年啦。经历过奋斗和激情，也经历过徘徊和迷茫，勤恳却不庸碌。虽然对测试也有担忧，但是对这个行业始终保持着一种激情。\n\n接下来结合自己的工作谈一下对测试的一些思考，也回顾下自己过去的测试工作中遵循的一些工作原则。\n\n![](1.jpg)\n\n<!--more-->\n\n## 原则1：正确的认识自己\n好多人，自视甚高，傲视一切。但是，实际上，牛人很多。我们能做成一件事情，能力是一个方面，但能力绝对不是全部因素。没有上级的支持，没有同事的协作和配合，没有公司提供的资源，我们是什么事情也做不成的。不能因为自己的工作时间长，对业务了解的深，就对别人横加指责，也不能因为自己职称高就给别人以压力。\n\n![](2.png)\n\n## 原则2：先成就别人，再成就自己\n我们经常问QA的价值是什么？\n\n实际上有些东西是很难讲清楚的。好多QA为了证明自己的技术能力、技术价值，做了很多看似高超，但是实际上对业务快速迭代并没有用的事情。\n\n我总觉得，自己的价值不是由自己体现的，而是由别人来体现的。只有先成就别人，才能最终成就自己。这就是我工作以来一直遵循的一个工作原则。不争，不抢，认真做事，认真帮别人做事，认真的帮业务做事情，帮助RD分析线上问题，解决BUG。到了后来，也颇有点：桃李不言，下自成蹊的味道。\n\n好多人也会说，在这个时代，酒香也怕巷子深。如果不主动宣传自己的价值，宣传自己的影响力，别人是不会知道的。但是，实际上，影响力和价值是由别人来评估的，历史交由后人评就是这个道理。并且，影响力是处于某个圈层的，并通过圈层而扩散。我们首先在团队的圈层内做到影响力，才有可能通过圈层的扩散而扩散自己的价值。\n\n![](3.jpg)\n\n## 原则3：方便别人，方便自己\n做测试的同学，每天会写很多测试case，每天也会发很多测试报告，但是如果打开这些邮件，经常发现这些case很难让人理解，这些测试报告虽然篇幅很大，但是如果要从中找到接下来RD要做什么，还是要花费一点时间的。\n\n在测试工作过程中，无论是测试报告，还是风险通报，首先要了解到这些内容的接收者到底是谁，然后需要思考，要以什么样的描述和组织才能方便对方快速了解到内容的含义，也就是要做到：到什么山头，唱什么歌。\n\n我刚入职的时候，因为之前没有做过测试，也不知道测试报告要以什么样的格式来发，所以很害怕发测试报告。虽然学习了其他前辈的测试报告和前辈们的测试报告范式，但是总感觉还是害怕。因为，从RD的角度来看，阅读那些测试报告的成本还是很大的。\n\n然后，我站在RD的角度上想，怎么组织测试报告，才能让RD一眼就知道自己接下来要做什么呢？然后我重新组织了之前测测试报告，在报告的最开始用1句话概括项目的测试结论：该项目测试是否通过，原因是什么？具体的BUG量是多少？然后接下来是详细的测试清单，清单中明确表明每一个case的测试点，前置，预期结果，实际测试结果，是否通过等信息。对于测试通过的case，用绿色来标注；对于测试失败的case，用红色标注；对于有疑问的case，比如建议等，用黄色标注。RD收到测试报告之后，先看结论，然后看非绿色的测试case修复BUG。\n\n## 原则4：消除门槛，而不是提升门槛\n大禹治水之所以成功，最重要的思想就是：改“堵”为“疏”。\n\n但是真正的测试过程中，经常会发现如下的现象：领导质疑QA的测试时间太长，QA质疑RD的提测质量太差，然后QA增加了准入打回的流程，对于RD提测的、没有通过准入的项目不予测试。\n\n实际上，这种现象就是QA在项目的测试环节设置了一个门槛，而如果这种门槛设置的越来越多，那么对于业务而言也会发生“洪涝灾害”。单从测试团队的角度看，确实增加这个门槛，测试效率提升了，测试时间降低了，但是如果从整个业务的迭代来看，效率并没有提升。\n\n我们提了门槛，发了准入case，增加了流程，这一点也不能说不好。但是我们有没有想过：这些准入case别人看的懂吗？我们怎么设计case能够使得RD自测的时候更加方便呢？即便看懂了，我们怎么保证RD确实执行了呢？我们怎么保证增加的这个流程每次都贯彻落实了呢？\n\nQA不应该是一个门槛设置者，而应该是一种有效的武器。不能用看似合理的各种标准和要求卡住事情，而应该成为帮助团队解决问题的秘密武器。\n\nQA不能只报问题，只说现象，只说RD不靠谱，上线不检查。QA需要思考：\n* 我们能为快速迭代做些什么？\n* 我们能做些什么可以帮助RD快速发现自己的代码问题？\n* 我们能做什么可以帮助RD快速发现上线过程异常了？\n\n## 原则5：不断学习，不做伪学习者\n现在，技术更新换代的频率很快，所以一定要不断的学习，持续的学习。还记得刚入职的时候，那时候就会c/c++，java。现在看看自己的github和内网的提交记录，发现语言层面基本上涵盖了：c/c++，java，php，python，go，javascript，shell，object-c，swift，lua。项目类型覆盖了服务端，策略端，iOS，Android，自动化等不同的方向。从原来只会用IIS的小白，到今天了解Apache，Lighttpd，Nginx等各种web服务器。从原来的服务端，到慢慢了解前端，了解策略端，了解客户端。\n\n参加过好多会，和好多人沟通过，大家都希望组织提供学习的机会，提供分享的机会，希望组织能提供自己技术成长的机会。但是，实际上，成长是自己的事，学习是个人的事，为什么要求组织给提供这些机会呢？我组织过很长时间的分享，我也最反对组织大规模的技术学习和分享，因为让太多的伪学习者进入之后，效果并不理想。相反，规模较小的沙龙和讨论对个人的技术成长效果更为明显。伪学习者只是想学习而已，只是想让组织提供学习机会而已，仅此而已。\n\n学习是自己的事情，白天求生存，晚上求发展。凌晨2点的时候，也曾在灯下啃过Nginx的源码，分析一个个的线上问题，没有这些个凌晨2点就不会有《Nginx沉思录》，《Nginx洗冤录》等一系列的总结。\n\n![](4.jpg)\n\n## 原则6：做积极的抱怨者，不做消极的抱怨者\n没有“抱怨”就不会有技术的进步，但是反过来说没有“抱怨”就不会存在那么多的失败，一无是处，庸庸碌碌。关键是要看待我们对待“抱怨”的态度。当“抱怨”还仅仅是停留在嘴上，停留在思想上，被“抱怨”所控制时，基本上也就离碌碌无为不远了。当我们开始和“抱怨”对抗时，利用一切技术手段和努力消灭“抱怨”时，技术就开始进步了。所以，我一般不反对抱怨，并一直鼓励大家抱怨，但是接下来我会仔细分析抱怨的原因是什么？技术上需要什么探索才能消灭这些抱怨？\n\n每一个“抱怨”都是一次机会，都是一个机遇，都是一次成长。逢山开路，遇水搭桥，勇往直前。\n\n![](5.jpg)\n\n## 原则7：正确认识QA和RD的异同\n天分日夜黑白，季节有春夏秋冬，虽然全栈模式有他的优点，但是四季也有四季的美。文能提笔安天下，武能上马定乾坤。关键是，我们需要对QA和RD有正确的认识。\n\n我们不能按照一个原则来评定事情。比如：老驴拉磨，始终走不出那个圈圈。和日行千里的良马相比，当然老驴要羞愧了。但是如果换个角度，拉磨一天，产面千斤。良马再能跑，也跑不出千斤面。\n\nQA和RD虽然都是研发岗，都是技术体系，那他们的差异在哪呢？\n\n我觉得有以下几点吧：\n* 从工作关系上讲，QA是RD的下游，RD是QA的客户。\n* 从工作重点上讲，QA注重分析，而RD注重实践。RD要做的是解决问题，采用任何可能的方法，快速实现需求，快速上线。而QA没有太大的业务压力，可以有更多的时间来思考每一种技术的优势和劣势，有更多的时间来仔细分析和追查每一个bug的深层原因，帮助RD在快速解决问题后可以彻底解决问题，锦上添花。\n* 从涵盖业务上讲，QA关注的业务广度较高，而RD关注业务的深度更高。\n\n实际上，技术是一个很广泛的概念，不能认为只有写代码，设计架构才是技术，我们的思维本身就是一种技术。关键是，我们的技术要为业务的发展而存在，而不是孤立的存在。\n\n![](6.jpg)\n\n","slug":"thinking-about-QA-my-years-for-work-as-a-QA","published":1,"updated":"2020-02-05T07:00:58.278Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9kyh07c000p1pdbbk56eemv","content":"<p>2012年毕业后怀着一颗忐忑的心开始了测试工作的职业生涯，到现在也有7年啦。经历过奋斗和激情，也经历过徘徊和迷茫，勤恳却不庸碌。虽然对测试也有担忧，但是对这个行业始终保持着一种激情。</p>\n<p>接下来结合自己的工作谈一下对测试的一些思考，也回顾下自己过去的测试工作中遵循的一些工作原则。</p>\n<p><img src=\"/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/1.jpg\" alt></p>\n<a id=\"more\"></a>\n\n<h2 id=\"原则1：正确的认识自己\"><a href=\"#原则1：正确的认识自己\" class=\"headerlink\" title=\"原则1：正确的认识自己\"></a>原则1：正确的认识自己</h2><p>好多人，自视甚高，傲视一切。但是，实际上，牛人很多。我们能做成一件事情，能力是一个方面，但能力绝对不是全部因素。没有上级的支持，没有同事的协作和配合，没有公司提供的资源，我们是什么事情也做不成的。不能因为自己的工作时间长，对业务了解的深，就对别人横加指责，也不能因为自己职称高就给别人以压力。</p>\n<p><img src=\"/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/2.png\" alt></p>\n<h2 id=\"原则2：先成就别人，再成就自己\"><a href=\"#原则2：先成就别人，再成就自己\" class=\"headerlink\" title=\"原则2：先成就别人，再成就自己\"></a>原则2：先成就别人，再成就自己</h2><p>我们经常问QA的价值是什么？</p>\n<p>实际上有些东西是很难讲清楚的。好多QA为了证明自己的技术能力、技术价值，做了很多看似高超，但是实际上对业务快速迭代并没有用的事情。</p>\n<p>我总觉得，自己的价值不是由自己体现的，而是由别人来体现的。只有先成就别人，才能最终成就自己。这就是我工作以来一直遵循的一个工作原则。不争，不抢，认真做事，认真帮别人做事，认真的帮业务做事情，帮助RD分析线上问题，解决BUG。到了后来，也颇有点：桃李不言，下自成蹊的味道。</p>\n<p>好多人也会说，在这个时代，酒香也怕巷子深。如果不主动宣传自己的价值，宣传自己的影响力，别人是不会知道的。但是，实际上，影响力和价值是由别人来评估的，历史交由后人评就是这个道理。并且，影响力是处于某个圈层的，并通过圈层而扩散。我们首先在团队的圈层内做到影响力，才有可能通过圈层的扩散而扩散自己的价值。</p>\n<p><img src=\"/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/3.jpg\" alt></p>\n<h2 id=\"原则3：方便别人，方便自己\"><a href=\"#原则3：方便别人，方便自己\" class=\"headerlink\" title=\"原则3：方便别人，方便自己\"></a>原则3：方便别人，方便自己</h2><p>做测试的同学，每天会写很多测试case，每天也会发很多测试报告，但是如果打开这些邮件，经常发现这些case很难让人理解，这些测试报告虽然篇幅很大，但是如果要从中找到接下来RD要做什么，还是要花费一点时间的。</p>\n<p>在测试工作过程中，无论是测试报告，还是风险通报，首先要了解到这些内容的接收者到底是谁，然后需要思考，要以什么样的描述和组织才能方便对方快速了解到内容的含义，也就是要做到：到什么山头，唱什么歌。</p>\n<p>我刚入职的时候，因为之前没有做过测试，也不知道测试报告要以什么样的格式来发，所以很害怕发测试报告。虽然学习了其他前辈的测试报告和前辈们的测试报告范式，但是总感觉还是害怕。因为，从RD的角度来看，阅读那些测试报告的成本还是很大的。</p>\n<p>然后，我站在RD的角度上想，怎么组织测试报告，才能让RD一眼就知道自己接下来要做什么呢？然后我重新组织了之前测测试报告，在报告的最开始用1句话概括项目的测试结论：该项目测试是否通过，原因是什么？具体的BUG量是多少？然后接下来是详细的测试清单，清单中明确表明每一个case的测试点，前置，预期结果，实际测试结果，是否通过等信息。对于测试通过的case，用绿色来标注；对于测试失败的case，用红色标注；对于有疑问的case，比如建议等，用黄色标注。RD收到测试报告之后，先看结论，然后看非绿色的测试case修复BUG。</p>\n<h2 id=\"原则4：消除门槛，而不是提升门槛\"><a href=\"#原则4：消除门槛，而不是提升门槛\" class=\"headerlink\" title=\"原则4：消除门槛，而不是提升门槛\"></a>原则4：消除门槛，而不是提升门槛</h2><p>大禹治水之所以成功，最重要的思想就是：改“堵”为“疏”。</p>\n<p>但是真正的测试过程中，经常会发现如下的现象：领导质疑QA的测试时间太长，QA质疑RD的提测质量太差，然后QA增加了准入打回的流程，对于RD提测的、没有通过准入的项目不予测试。</p>\n<p>实际上，这种现象就是QA在项目的测试环节设置了一个门槛，而如果这种门槛设置的越来越多，那么对于业务而言也会发生“洪涝灾害”。单从测试团队的角度看，确实增加这个门槛，测试效率提升了，测试时间降低了，但是如果从整个业务的迭代来看，效率并没有提升。</p>\n<p>我们提了门槛，发了准入case，增加了流程，这一点也不能说不好。但是我们有没有想过：这些准入case别人看的懂吗？我们怎么设计case能够使得RD自测的时候更加方便呢？即便看懂了，我们怎么保证RD确实执行了呢？我们怎么保证增加的这个流程每次都贯彻落实了呢？</p>\n<p>QA不应该是一个门槛设置者，而应该是一种有效的武器。不能用看似合理的各种标准和要求卡住事情，而应该成为帮助团队解决问题的秘密武器。</p>\n<p>QA不能只报问题，只说现象，只说RD不靠谱，上线不检查。QA需要思考：</p>\n<ul>\n<li>我们能为快速迭代做些什么？</li>\n<li>我们能做些什么可以帮助RD快速发现自己的代码问题？</li>\n<li>我们能做什么可以帮助RD快速发现上线过程异常了？</li>\n</ul>\n<h2 id=\"原则5：不断学习，不做伪学习者\"><a href=\"#原则5：不断学习，不做伪学习者\" class=\"headerlink\" title=\"原则5：不断学习，不做伪学习者\"></a>原则5：不断学习，不做伪学习者</h2><p>现在，技术更新换代的频率很快，所以一定要不断的学习，持续的学习。还记得刚入职的时候，那时候就会c/c++，java。现在看看自己的github和内网的提交记录，发现语言层面基本上涵盖了：c/c++，java，php，python，go，javascript，shell，object-c，swift，lua。项目类型覆盖了服务端，策略端，iOS，Android，自动化等不同的方向。从原来只会用IIS的小白，到今天了解Apache，Lighttpd，Nginx等各种web服务器。从原来的服务端，到慢慢了解前端，了解策略端，了解客户端。</p>\n<p>参加过好多会，和好多人沟通过，大家都希望组织提供学习的机会，提供分享的机会，希望组织能提供自己技术成长的机会。但是，实际上，成长是自己的事，学习是个人的事，为什么要求组织给提供这些机会呢？我组织过很长时间的分享，我也最反对组织大规模的技术学习和分享，因为让太多的伪学习者进入之后，效果并不理想。相反，规模较小的沙龙和讨论对个人的技术成长效果更为明显。伪学习者只是想学习而已，只是想让组织提供学习机会而已，仅此而已。</p>\n<p>学习是自己的事情，白天求生存，晚上求发展。凌晨2点的时候，也曾在灯下啃过Nginx的源码，分析一个个的线上问题，没有这些个凌晨2点就不会有《Nginx沉思录》，《Nginx洗冤录》等一系列的总结。</p>\n<p><img src=\"/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/4.jpg\" alt></p>\n<h2 id=\"原则6：做积极的抱怨者，不做消极的抱怨者\"><a href=\"#原则6：做积极的抱怨者，不做消极的抱怨者\" class=\"headerlink\" title=\"原则6：做积极的抱怨者，不做消极的抱怨者\"></a>原则6：做积极的抱怨者，不做消极的抱怨者</h2><p>没有“抱怨”就不会有技术的进步，但是反过来说没有“抱怨”就不会存在那么多的失败，一无是处，庸庸碌碌。关键是要看待我们对待“抱怨”的态度。当“抱怨”还仅仅是停留在嘴上，停留在思想上，被“抱怨”所控制时，基本上也就离碌碌无为不远了。当我们开始和“抱怨”对抗时，利用一切技术手段和努力消灭“抱怨”时，技术就开始进步了。所以，我一般不反对抱怨，并一直鼓励大家抱怨，但是接下来我会仔细分析抱怨的原因是什么？技术上需要什么探索才能消灭这些抱怨？</p>\n<p>每一个“抱怨”都是一次机会，都是一个机遇，都是一次成长。逢山开路，遇水搭桥，勇往直前。</p>\n<p><img src=\"/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/5.jpg\" alt></p>\n<h2 id=\"原则7：正确认识QA和RD的异同\"><a href=\"#原则7：正确认识QA和RD的异同\" class=\"headerlink\" title=\"原则7：正确认识QA和RD的异同\"></a>原则7：正确认识QA和RD的异同</h2><p>天分日夜黑白，季节有春夏秋冬，虽然全栈模式有他的优点，但是四季也有四季的美。文能提笔安天下，武能上马定乾坤。关键是，我们需要对QA和RD有正确的认识。</p>\n<p>我们不能按照一个原则来评定事情。比如：老驴拉磨，始终走不出那个圈圈。和日行千里的良马相比，当然老驴要羞愧了。但是如果换个角度，拉磨一天，产面千斤。良马再能跑，也跑不出千斤面。</p>\n<p>QA和RD虽然都是研发岗，都是技术体系，那他们的差异在哪呢？</p>\n<p>我觉得有以下几点吧：</p>\n<ul>\n<li>从工作关系上讲，QA是RD的下游，RD是QA的客户。</li>\n<li>从工作重点上讲，QA注重分析，而RD注重实践。RD要做的是解决问题，采用任何可能的方法，快速实现需求，快速上线。而QA没有太大的业务压力，可以有更多的时间来思考每一种技术的优势和劣势，有更多的时间来仔细分析和追查每一个bug的深层原因，帮助RD在快速解决问题后可以彻底解决问题，锦上添花。</li>\n<li>从涵盖业务上讲，QA关注的业务广度较高，而RD关注业务的深度更高。</li>\n</ul>\n<p>实际上，技术是一个很广泛的概念，不能认为只有写代码，设计架构才是技术，我们的思维本身就是一种技术。关键是，我们的技术要为业务的发展而存在，而不是孤立的存在。</p>\n<p><img src=\"/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/6.jpg\" alt></p>\n","site":{"data":{}},"excerpt":"<p>2012年毕业后怀着一颗忐忑的心开始了测试工作的职业生涯，到现在也有7年啦。经历过奋斗和激情，也经历过徘徊和迷茫，勤恳却不庸碌。虽然对测试也有担忧，但是对这个行业始终保持着一种激情。</p>\n<p>接下来结合自己的工作谈一下对测试的一些思考，也回顾下自己过去的测试工作中遵循的一些工作原则。</p>\n<p><img src=\"/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/1.jpg\" alt></p>","more":"<h2 id=\"原则1：正确的认识自己\"><a href=\"#原则1：正确的认识自己\" class=\"headerlink\" title=\"原则1：正确的认识自己\"></a>原则1：正确的认识自己</h2><p>好多人，自视甚高，傲视一切。但是，实际上，牛人很多。我们能做成一件事情，能力是一个方面，但能力绝对不是全部因素。没有上级的支持，没有同事的协作和配合，没有公司提供的资源，我们是什么事情也做不成的。不能因为自己的工作时间长，对业务了解的深，就对别人横加指责，也不能因为自己职称高就给别人以压力。</p>\n<p><img src=\"/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/2.png\" alt></p>\n<h2 id=\"原则2：先成就别人，再成就自己\"><a href=\"#原则2：先成就别人，再成就自己\" class=\"headerlink\" title=\"原则2：先成就别人，再成就自己\"></a>原则2：先成就别人，再成就自己</h2><p>我们经常问QA的价值是什么？</p>\n<p>实际上有些东西是很难讲清楚的。好多QA为了证明自己的技术能力、技术价值，做了很多看似高超，但是实际上对业务快速迭代并没有用的事情。</p>\n<p>我总觉得，自己的价值不是由自己体现的，而是由别人来体现的。只有先成就别人，才能最终成就自己。这就是我工作以来一直遵循的一个工作原则。不争，不抢，认真做事，认真帮别人做事，认真的帮业务做事情，帮助RD分析线上问题，解决BUG。到了后来，也颇有点：桃李不言，下自成蹊的味道。</p>\n<p>好多人也会说，在这个时代，酒香也怕巷子深。如果不主动宣传自己的价值，宣传自己的影响力，别人是不会知道的。但是，实际上，影响力和价值是由别人来评估的，历史交由后人评就是这个道理。并且，影响力是处于某个圈层的，并通过圈层而扩散。我们首先在团队的圈层内做到影响力，才有可能通过圈层的扩散而扩散自己的价值。</p>\n<p><img src=\"/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/3.jpg\" alt></p>\n<h2 id=\"原则3：方便别人，方便自己\"><a href=\"#原则3：方便别人，方便自己\" class=\"headerlink\" title=\"原则3：方便别人，方便自己\"></a>原则3：方便别人，方便自己</h2><p>做测试的同学，每天会写很多测试case，每天也会发很多测试报告，但是如果打开这些邮件，经常发现这些case很难让人理解，这些测试报告虽然篇幅很大，但是如果要从中找到接下来RD要做什么，还是要花费一点时间的。</p>\n<p>在测试工作过程中，无论是测试报告，还是风险通报，首先要了解到这些内容的接收者到底是谁，然后需要思考，要以什么样的描述和组织才能方便对方快速了解到内容的含义，也就是要做到：到什么山头，唱什么歌。</p>\n<p>我刚入职的时候，因为之前没有做过测试，也不知道测试报告要以什么样的格式来发，所以很害怕发测试报告。虽然学习了其他前辈的测试报告和前辈们的测试报告范式，但是总感觉还是害怕。因为，从RD的角度来看，阅读那些测试报告的成本还是很大的。</p>\n<p>然后，我站在RD的角度上想，怎么组织测试报告，才能让RD一眼就知道自己接下来要做什么呢？然后我重新组织了之前测测试报告，在报告的最开始用1句话概括项目的测试结论：该项目测试是否通过，原因是什么？具体的BUG量是多少？然后接下来是详细的测试清单，清单中明确表明每一个case的测试点，前置，预期结果，实际测试结果，是否通过等信息。对于测试通过的case，用绿色来标注；对于测试失败的case，用红色标注；对于有疑问的case，比如建议等，用黄色标注。RD收到测试报告之后，先看结论，然后看非绿色的测试case修复BUG。</p>\n<h2 id=\"原则4：消除门槛，而不是提升门槛\"><a href=\"#原则4：消除门槛，而不是提升门槛\" class=\"headerlink\" title=\"原则4：消除门槛，而不是提升门槛\"></a>原则4：消除门槛，而不是提升门槛</h2><p>大禹治水之所以成功，最重要的思想就是：改“堵”为“疏”。</p>\n<p>但是真正的测试过程中，经常会发现如下的现象：领导质疑QA的测试时间太长，QA质疑RD的提测质量太差，然后QA增加了准入打回的流程，对于RD提测的、没有通过准入的项目不予测试。</p>\n<p>实际上，这种现象就是QA在项目的测试环节设置了一个门槛，而如果这种门槛设置的越来越多，那么对于业务而言也会发生“洪涝灾害”。单从测试团队的角度看，确实增加这个门槛，测试效率提升了，测试时间降低了，但是如果从整个业务的迭代来看，效率并没有提升。</p>\n<p>我们提了门槛，发了准入case，增加了流程，这一点也不能说不好。但是我们有没有想过：这些准入case别人看的懂吗？我们怎么设计case能够使得RD自测的时候更加方便呢？即便看懂了，我们怎么保证RD确实执行了呢？我们怎么保证增加的这个流程每次都贯彻落实了呢？</p>\n<p>QA不应该是一个门槛设置者，而应该是一种有效的武器。不能用看似合理的各种标准和要求卡住事情，而应该成为帮助团队解决问题的秘密武器。</p>\n<p>QA不能只报问题，只说现象，只说RD不靠谱，上线不检查。QA需要思考：</p>\n<ul>\n<li>我们能为快速迭代做些什么？</li>\n<li>我们能做些什么可以帮助RD快速发现自己的代码问题？</li>\n<li>我们能做什么可以帮助RD快速发现上线过程异常了？</li>\n</ul>\n<h2 id=\"原则5：不断学习，不做伪学习者\"><a href=\"#原则5：不断学习，不做伪学习者\" class=\"headerlink\" title=\"原则5：不断学习，不做伪学习者\"></a>原则5：不断学习，不做伪学习者</h2><p>现在，技术更新换代的频率很快，所以一定要不断的学习，持续的学习。还记得刚入职的时候，那时候就会c/c++，java。现在看看自己的github和内网的提交记录，发现语言层面基本上涵盖了：c/c++，java，php，python，go，javascript，shell，object-c，swift，lua。项目类型覆盖了服务端，策略端，iOS，Android，自动化等不同的方向。从原来只会用IIS的小白，到今天了解Apache，Lighttpd，Nginx等各种web服务器。从原来的服务端，到慢慢了解前端，了解策略端，了解客户端。</p>\n<p>参加过好多会，和好多人沟通过，大家都希望组织提供学习的机会，提供分享的机会，希望组织能提供自己技术成长的机会。但是，实际上，成长是自己的事，学习是个人的事，为什么要求组织给提供这些机会呢？我组织过很长时间的分享，我也最反对组织大规模的技术学习和分享，因为让太多的伪学习者进入之后，效果并不理想。相反，规模较小的沙龙和讨论对个人的技术成长效果更为明显。伪学习者只是想学习而已，只是想让组织提供学习机会而已，仅此而已。</p>\n<p>学习是自己的事情，白天求生存，晚上求发展。凌晨2点的时候，也曾在灯下啃过Nginx的源码，分析一个个的线上问题，没有这些个凌晨2点就不会有《Nginx沉思录》，《Nginx洗冤录》等一系列的总结。</p>\n<p><img src=\"/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/4.jpg\" alt></p>\n<h2 id=\"原则6：做积极的抱怨者，不做消极的抱怨者\"><a href=\"#原则6：做积极的抱怨者，不做消极的抱怨者\" class=\"headerlink\" title=\"原则6：做积极的抱怨者，不做消极的抱怨者\"></a>原则6：做积极的抱怨者，不做消极的抱怨者</h2><p>没有“抱怨”就不会有技术的进步，但是反过来说没有“抱怨”就不会存在那么多的失败，一无是处，庸庸碌碌。关键是要看待我们对待“抱怨”的态度。当“抱怨”还仅仅是停留在嘴上，停留在思想上，被“抱怨”所控制时，基本上也就离碌碌无为不远了。当我们开始和“抱怨”对抗时，利用一切技术手段和努力消灭“抱怨”时，技术就开始进步了。所以，我一般不反对抱怨，并一直鼓励大家抱怨，但是接下来我会仔细分析抱怨的原因是什么？技术上需要什么探索才能消灭这些抱怨？</p>\n<p>每一个“抱怨”都是一次机会，都是一个机遇，都是一次成长。逢山开路，遇水搭桥，勇往直前。</p>\n<p><img src=\"/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/5.jpg\" alt></p>\n<h2 id=\"原则7：正确认识QA和RD的异同\"><a href=\"#原则7：正确认识QA和RD的异同\" class=\"headerlink\" title=\"原则7：正确认识QA和RD的异同\"></a>原则7：正确认识QA和RD的异同</h2><p>天分日夜黑白，季节有春夏秋冬，虽然全栈模式有他的优点，但是四季也有四季的美。文能提笔安天下，武能上马定乾坤。关键是，我们需要对QA和RD有正确的认识。</p>\n<p>我们不能按照一个原则来评定事情。比如：老驴拉磨，始终走不出那个圈圈。和日行千里的良马相比，当然老驴要羞愧了。但是如果换个角度，拉磨一天，产面千斤。良马再能跑，也跑不出千斤面。</p>\n<p>QA和RD虽然都是研发岗，都是技术体系，那他们的差异在哪呢？</p>\n<p>我觉得有以下几点吧：</p>\n<ul>\n<li>从工作关系上讲，QA是RD的下游，RD是QA的客户。</li>\n<li>从工作重点上讲，QA注重分析，而RD注重实践。RD要做的是解决问题，采用任何可能的方法，快速实现需求，快速上线。而QA没有太大的业务压力，可以有更多的时间来思考每一种技术的优势和劣势，有更多的时间来仔细分析和追查每一个bug的深层原因，帮助RD在快速解决问题后可以彻底解决问题，锦上添花。</li>\n<li>从涵盖业务上讲，QA关注的业务广度较高，而RD关注业务的深度更高。</li>\n</ul>\n<p>实际上，技术是一个很广泛的概念，不能认为只有写代码，设计架构才是技术，我们的思维本身就是一种技术。关键是，我们的技术要为业务的发展而存在，而不是孤立的存在。</p>\n<p><img src=\"/2020/02/05/thinking-about-QA-my-years-for-work-as-a-QA/6.jpg\" alt></p>"},{"title":"使用hexo和github搭建个人博客","reward":false,"date":"2020-02-05T12:01:29.000Z","_content":"\n## 创建github仓库\n首先打开[github](https://github.com/)，点击`New repository`，创建一个新仓库用于存储博客的所有内容。\n\n仓库名必须为：`账户名.github.io`，并且需要勾选**Initialize this repository with a README**。\n\n在建好的仓库右侧有个**settings**按钮，点击**settings**，向下滑动到**GitHub Pages**，会发现有个网址，github会把该仓库中的项目部署到该网址下，该网址也是博客的默认地址。当然也可以购买域名，将其换成喜欢的地址。\n\n<!--more-->\n\n![](1.jpg)\n\n![](2.jpg)\n\n\n\n## 准备node.js环境\n由于hexo是基于[node.js](https://nodejs.org/en/)开发的，因此在安装hexo之前，需要先安装node.js并配置响应的node.js环境。具体如下所示：\n\n![](3.jpg)\n\n## 准备hexo环境\n根据[hexo官网](https://hexo.io/)的提示，安装hexo。具体如下所示：\n\n```\n$ npm install hexo-cli -g\n$ hexo init blog\n$ cd blog\n$ npm install\n$ hexo server\n```\n此时hexo会在4000端口启动一个webserver，用浏览器访问`localhost:4000`则会看到初始化之后默认的站点。\n\n## 安装hexo插件\n### hexo-generator-category\n该插件用于自动生成category，具体安装方式如下：\n\n```\n$ npm install hexo-generator-category --save\n$ hexo new page categories\n```\n然后修改`source/categories/index.md`的内容为：\n\n```\n---\ntitle: categories\ndate: 2020-02-05 11:53:35\ntype: \"categories\"\nlayout: \"categories\"\ncomments: false\n---\n```\n\n### hexo-generator-tag\n该插件用于自动生成tags，具体安装方式如下：\n\n```\n$ npm install hexo-generator-tag --save\n$ hexo new page tags\n```\n然后修改`source/tags/index.md`的内容为：\n\n```\n---\ntitle: tags\ndate: 2020-02-05 11:53:58\ntype: \"tags\"\nlayout: \"tags\"\ncomments: false\n---\n```\n\n### hexo-asset-image\n该插件主要用于解决在文章中引用图片的场景，具体安装方式如下：\n\n```\n$ npm install hexo-asset-image --save\n```\n\n安装该插件后，利用`hexo new post 'test'`生成新文章内容时，会在`source/_posts`目录下同时生成`test.md`和`test目录`，`test.md`中需要的图片可以存储在`test目录`下。在文档中，采用如下的方式来引用图片：\n\n```\n![图片的描述信息](1.jpg)\n```\n\n关于hexo-asset-image生成图片的bug，可以参考文章：[解决hexo-asset-image的图片地址错误问题](/2020/02/05/handle-the-bug-of-hexo-asset-image-plugin/)\n\n### hexo-deployer-git\n该插件用于将编译后生成的站点内容发布到第一步创建的github仓库中，从而实现内容更新。具体安装方式如下：\n\n```\n$ npm add hexo-deployer-git\n```\n\n然后配置_config.yml中的`url`和`deploy`配置，以便可以通过该插件自动部署项目。具体配置如下所示：\n\n```yaml\nurl: https://wangwei1237.github.io/\ndeploy:\n  type: git\n  repo: https://github.com/wangwei1237/wangwei1237.github.io\n```\n\n配置完毕之后，如果项目内容发生修改，则利用如下的命令就可以完成自动发布：\n\n```\n$ hexo clean\n$ hexo generate\n$ hexo deploy\n```","source":"_posts/use-hexo-and-github-for-blog.md","raw":"---\ntitle: 使用hexo和github搭建个人博客\nreward: false\ndate: 2020-02-05 20:01:29\ncategories: 技术\ntags: \n  - hexo\n  - github\n  - 建站\n---\n\n## 创建github仓库\n首先打开[github](https://github.com/)，点击`New repository`，创建一个新仓库用于存储博客的所有内容。\n\n仓库名必须为：`账户名.github.io`，并且需要勾选**Initialize this repository with a README**。\n\n在建好的仓库右侧有个**settings**按钮，点击**settings**，向下滑动到**GitHub Pages**，会发现有个网址，github会把该仓库中的项目部署到该网址下，该网址也是博客的默认地址。当然也可以购买域名，将其换成喜欢的地址。\n\n<!--more-->\n\n![](1.jpg)\n\n![](2.jpg)\n\n\n\n## 准备node.js环境\n由于hexo是基于[node.js](https://nodejs.org/en/)开发的，因此在安装hexo之前，需要先安装node.js并配置响应的node.js环境。具体如下所示：\n\n![](3.jpg)\n\n## 准备hexo环境\n根据[hexo官网](https://hexo.io/)的提示，安装hexo。具体如下所示：\n\n```\n$ npm install hexo-cli -g\n$ hexo init blog\n$ cd blog\n$ npm install\n$ hexo server\n```\n此时hexo会在4000端口启动一个webserver，用浏览器访问`localhost:4000`则会看到初始化之后默认的站点。\n\n## 安装hexo插件\n### hexo-generator-category\n该插件用于自动生成category，具体安装方式如下：\n\n```\n$ npm install hexo-generator-category --save\n$ hexo new page categories\n```\n然后修改`source/categories/index.md`的内容为：\n\n```\n---\ntitle: categories\ndate: 2020-02-05 11:53:35\ntype: \"categories\"\nlayout: \"categories\"\ncomments: false\n---\n```\n\n### hexo-generator-tag\n该插件用于自动生成tags，具体安装方式如下：\n\n```\n$ npm install hexo-generator-tag --save\n$ hexo new page tags\n```\n然后修改`source/tags/index.md`的内容为：\n\n```\n---\ntitle: tags\ndate: 2020-02-05 11:53:58\ntype: \"tags\"\nlayout: \"tags\"\ncomments: false\n---\n```\n\n### hexo-asset-image\n该插件主要用于解决在文章中引用图片的场景，具体安装方式如下：\n\n```\n$ npm install hexo-asset-image --save\n```\n\n安装该插件后，利用`hexo new post 'test'`生成新文章内容时，会在`source/_posts`目录下同时生成`test.md`和`test目录`，`test.md`中需要的图片可以存储在`test目录`下。在文档中，采用如下的方式来引用图片：\n\n```\n![图片的描述信息](1.jpg)\n```\n\n关于hexo-asset-image生成图片的bug，可以参考文章：[解决hexo-asset-image的图片地址错误问题](/2020/02/05/handle-the-bug-of-hexo-asset-image-plugin/)\n\n### hexo-deployer-git\n该插件用于将编译后生成的站点内容发布到第一步创建的github仓库中，从而实现内容更新。具体安装方式如下：\n\n```\n$ npm add hexo-deployer-git\n```\n\n然后配置_config.yml中的`url`和`deploy`配置，以便可以通过该插件自动部署项目。具体配置如下所示：\n\n```yaml\nurl: https://wangwei1237.github.io/\ndeploy:\n  type: git\n  repo: https://github.com/wangwei1237/wangwei1237.github.io\n```\n\n配置完毕之后，如果项目内容发生修改，则利用如下的命令就可以完成自动发布：\n\n```\n$ hexo clean\n$ hexo generate\n$ hexo deploy\n```","slug":"use-hexo-and-github-for-blog","published":1,"updated":"2020-02-29T07:19:54.069Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9kyh07f000u1pdb33981rry","content":"<h2 id=\"创建github仓库\"><a href=\"#创建github仓库\" class=\"headerlink\" title=\"创建github仓库\"></a>创建github仓库</h2><p>首先打开<a href=\"https://github.com/\" target=\"_blank\" rel=\"noopener\">github</a>，点击<code>New repository</code>，创建一个新仓库用于存储博客的所有内容。</p>\n<p>仓库名必须为：<code>账户名.github.io</code>，并且需要勾选<strong>Initialize this repository with a README</strong>。</p>\n<p>在建好的仓库右侧有个<strong>settings</strong>按钮，点击<strong>settings</strong>，向下滑动到<strong>GitHub Pages</strong>，会发现有个网址，github会把该仓库中的项目部署到该网址下，该网址也是博客的默认地址。当然也可以购买域名，将其换成喜欢的地址。</p>\n<a id=\"more\"></a>\n\n<p><img src=\"/2020/02/05/use-hexo-and-github-for-blog/1.jpg\" alt></p>\n<p><img src=\"/2020/02/05/use-hexo-and-github-for-blog/2.jpg\" alt></p>\n<h2 id=\"准备node-js环境\"><a href=\"#准备node-js环境\" class=\"headerlink\" title=\"准备node.js环境\"></a>准备node.js环境</h2><p>由于hexo是基于<a href=\"https://nodejs.org/en/\" target=\"_blank\" rel=\"noopener\">node.js</a>开发的，因此在安装hexo之前，需要先安装node.js并配置响应的node.js环境。具体如下所示：</p>\n<p><img src=\"/2020/02/05/use-hexo-and-github-for-blog/3.jpg\" alt></p>\n<h2 id=\"准备hexo环境\"><a href=\"#准备hexo环境\" class=\"headerlink\" title=\"准备hexo环境\"></a>准备hexo环境</h2><p>根据<a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">hexo官网</a>的提示，安装hexo。具体如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install hexo-cli -g</span><br><span class=\"line\">$ hexo init blog</span><br><span class=\"line\">$ cd blog</span><br><span class=\"line\">$ npm install</span><br><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>此时hexo会在4000端口启动一个webserver，用浏览器访问<code>localhost:4000</code>则会看到初始化之后默认的站点。</p>\n<h2 id=\"安装hexo插件\"><a href=\"#安装hexo插件\" class=\"headerlink\" title=\"安装hexo插件\"></a>安装hexo插件</h2><h3 id=\"hexo-generator-category\"><a href=\"#hexo-generator-category\" class=\"headerlink\" title=\"hexo-generator-category\"></a>hexo-generator-category</h3><p>该插件用于自动生成category，具体安装方式如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install hexo-generator-category --save</span><br><span class=\"line\">$ hexo new page categories</span><br></pre></td></tr></table></figure>\n<p>然后修改<code>source/categories/index.md</code>的内容为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: categories</span><br><span class=\"line\">date: 2020-02-05 11:53:35</span><br><span class=\"line\">type: &quot;categories&quot;</span><br><span class=\"line\">layout: &quot;categories&quot;</span><br><span class=\"line\">comments: false</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"hexo-generator-tag\"><a href=\"#hexo-generator-tag\" class=\"headerlink\" title=\"hexo-generator-tag\"></a>hexo-generator-tag</h3><p>该插件用于自动生成tags，具体安装方式如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install hexo-generator-tag --save</span><br><span class=\"line\">$ hexo new page tags</span><br></pre></td></tr></table></figure>\n<p>然后修改<code>source/tags/index.md</code>的内容为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: tags</span><br><span class=\"line\">date: 2020-02-05 11:53:58</span><br><span class=\"line\">type: &quot;tags&quot;</span><br><span class=\"line\">layout: &quot;tags&quot;</span><br><span class=\"line\">comments: false</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"hexo-asset-image\"><a href=\"#hexo-asset-image\" class=\"headerlink\" title=\"hexo-asset-image\"></a>hexo-asset-image</h3><p>该插件主要用于解决在文章中引用图片的场景，具体安装方式如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure>\n\n<p>安装该插件后，利用<code>hexo new post &#39;test&#39;</code>生成新文章内容时，会在<code>source/_posts</code>目录下同时生成<code>test.md</code>和<code>test目录</code>，<code>test.md</code>中需要的图片可以存储在<code>test目录</code>下。在文档中，采用如下的方式来引用图片：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">![图片的描述信息](1.jpg)</span><br></pre></td></tr></table></figure>\n\n<p>关于hexo-asset-image生成图片的bug，可以参考文章：<a href=\"/2020/02/05/handle-the-bug-of-hexo-asset-image-plugin/\">解决hexo-asset-image的图片地址错误问题</a></p>\n<h3 id=\"hexo-deployer-git\"><a href=\"#hexo-deployer-git\" class=\"headerlink\" title=\"hexo-deployer-git\"></a>hexo-deployer-git</h3><p>该插件用于将编译后生成的站点内容发布到第一步创建的github仓库中，从而实现内容更新。具体安装方式如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm add hexo-deployer-git</span><br></pre></td></tr></table></figure>\n\n<p>然后配置_config.yml中的<code>url</code>和<code>deploy</code>配置，以便可以通过该插件自动部署项目。具体配置如下所示：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">url:</span> <span class=\"string\">https://wangwei1237.github.io/</span></span><br><span class=\"line\"><span class=\"attr\">deploy:</span></span><br><span class=\"line\">  <span class=\"attr\">type:</span> <span class=\"string\">git</span></span><br><span class=\"line\">  <span class=\"attr\">repo:</span> <span class=\"string\">https://github.com/wangwei1237/wangwei1237.github.io</span></span><br></pre></td></tr></table></figure>\n\n<p>配置完毕之后，如果项目内容发生修改，则利用如下的命令就可以完成自动发布：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo clean</span><br><span class=\"line\">$ hexo generate</span><br><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<h2 id=\"创建github仓库\"><a href=\"#创建github仓库\" class=\"headerlink\" title=\"创建github仓库\"></a>创建github仓库</h2><p>首先打开<a href=\"https://github.com/\" target=\"_blank\" rel=\"noopener\">github</a>，点击<code>New repository</code>，创建一个新仓库用于存储博客的所有内容。</p>\n<p>仓库名必须为：<code>账户名.github.io</code>，并且需要勾选<strong>Initialize this repository with a README</strong>。</p>\n<p>在建好的仓库右侧有个<strong>settings</strong>按钮，点击<strong>settings</strong>，向下滑动到<strong>GitHub Pages</strong>，会发现有个网址，github会把该仓库中的项目部署到该网址下，该网址也是博客的默认地址。当然也可以购买域名，将其换成喜欢的地址。</p>","more":"<p><img src=\"/2020/02/05/use-hexo-and-github-for-blog/1.jpg\" alt></p>\n<p><img src=\"/2020/02/05/use-hexo-and-github-for-blog/2.jpg\" alt></p>\n<h2 id=\"准备node-js环境\"><a href=\"#准备node-js环境\" class=\"headerlink\" title=\"准备node.js环境\"></a>准备node.js环境</h2><p>由于hexo是基于<a href=\"https://nodejs.org/en/\" target=\"_blank\" rel=\"noopener\">node.js</a>开发的，因此在安装hexo之前，需要先安装node.js并配置响应的node.js环境。具体如下所示：</p>\n<p><img src=\"/2020/02/05/use-hexo-and-github-for-blog/3.jpg\" alt></p>\n<h2 id=\"准备hexo环境\"><a href=\"#准备hexo环境\" class=\"headerlink\" title=\"准备hexo环境\"></a>准备hexo环境</h2><p>根据<a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">hexo官网</a>的提示，安装hexo。具体如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install hexo-cli -g</span><br><span class=\"line\">$ hexo init blog</span><br><span class=\"line\">$ cd blog</span><br><span class=\"line\">$ npm install</span><br><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>此时hexo会在4000端口启动一个webserver，用浏览器访问<code>localhost:4000</code>则会看到初始化之后默认的站点。</p>\n<h2 id=\"安装hexo插件\"><a href=\"#安装hexo插件\" class=\"headerlink\" title=\"安装hexo插件\"></a>安装hexo插件</h2><h3 id=\"hexo-generator-category\"><a href=\"#hexo-generator-category\" class=\"headerlink\" title=\"hexo-generator-category\"></a>hexo-generator-category</h3><p>该插件用于自动生成category，具体安装方式如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install hexo-generator-category --save</span><br><span class=\"line\">$ hexo new page categories</span><br></pre></td></tr></table></figure>\n<p>然后修改<code>source/categories/index.md</code>的内容为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: categories</span><br><span class=\"line\">date: 2020-02-05 11:53:35</span><br><span class=\"line\">type: &quot;categories&quot;</span><br><span class=\"line\">layout: &quot;categories&quot;</span><br><span class=\"line\">comments: false</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"hexo-generator-tag\"><a href=\"#hexo-generator-tag\" class=\"headerlink\" title=\"hexo-generator-tag\"></a>hexo-generator-tag</h3><p>该插件用于自动生成tags，具体安装方式如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install hexo-generator-tag --save</span><br><span class=\"line\">$ hexo new page tags</span><br></pre></td></tr></table></figure>\n<p>然后修改<code>source/tags/index.md</code>的内容为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: tags</span><br><span class=\"line\">date: 2020-02-05 11:53:58</span><br><span class=\"line\">type: &quot;tags&quot;</span><br><span class=\"line\">layout: &quot;tags&quot;</span><br><span class=\"line\">comments: false</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"hexo-asset-image\"><a href=\"#hexo-asset-image\" class=\"headerlink\" title=\"hexo-asset-image\"></a>hexo-asset-image</h3><p>该插件主要用于解决在文章中引用图片的场景，具体安装方式如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure>\n\n<p>安装该插件后，利用<code>hexo new post &#39;test&#39;</code>生成新文章内容时，会在<code>source/_posts</code>目录下同时生成<code>test.md</code>和<code>test目录</code>，<code>test.md</code>中需要的图片可以存储在<code>test目录</code>下。在文档中，采用如下的方式来引用图片：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">![图片的描述信息](1.jpg)</span><br></pre></td></tr></table></figure>\n\n<p>关于hexo-asset-image生成图片的bug，可以参考文章：<a href=\"/2020/02/05/handle-the-bug-of-hexo-asset-image-plugin/\">解决hexo-asset-image的图片地址错误问题</a></p>\n<h3 id=\"hexo-deployer-git\"><a href=\"#hexo-deployer-git\" class=\"headerlink\" title=\"hexo-deployer-git\"></a>hexo-deployer-git</h3><p>该插件用于将编译后生成的站点内容发布到第一步创建的github仓库中，从而实现内容更新。具体安装方式如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm add hexo-deployer-git</span><br></pre></td></tr></table></figure>\n\n<p>然后配置_config.yml中的<code>url</code>和<code>deploy</code>配置，以便可以通过该插件自动部署项目。具体配置如下所示：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">url:</span> <span class=\"string\">https://wangwei1237.github.io/</span></span><br><span class=\"line\"><span class=\"attr\">deploy:</span></span><br><span class=\"line\">  <span class=\"attr\">type:</span> <span class=\"string\">git</span></span><br><span class=\"line\">  <span class=\"attr\">repo:</span> <span class=\"string\">https://github.com/wangwei1237/wangwei1237.github.io</span></span><br></pre></td></tr></table></figure>\n\n<p>配置完毕之后，如果项目内容发生修改，则利用如下的命令就可以完成自动发布：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo clean</span><br><span class=\"line\">$ hexo generate</span><br><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>"},{"title":"凡所过往，皆是序章","reward":false,"date":"2020-02-05T07:00:23.000Z","_content":"\n凡所过往，皆是序章，新的一年，用4个关键词来讲一讲2019年自己心中的过往和序章。\n\n![](1.jpg)\n\n<!--more-->\n\n## 变化\n如果要用1个词来形容2019年的话，我觉得就是 变化了。无论内部，还是外部，都在发生着翻天覆地的变化。变化这种滋味起初很痛苦，但是却蕴含着无限的机会。\n\n未来，变化会是常态。工作方式要变，工具平台要变，开发语言要变……所有的事情都在向着好的方向变化、发展。\n\n在这个变化的过程中，不能随波逐流，也不能固守己见。需要顺应变化，找到其中的机会，顺势而为，乘风而起。\n\n## 学习\n在快速的变化中，怎么才能乘风而起？秘诀也就是 学习了。\n\n2019年，我学习了opengl，kotlin；\n\n2019年，学习了数字视频，ffmpeg，opencv；\n\n2019年，学习了Android的相机系统，学习了逆向破解；\n\n2019年，学习了如何写作，学习了如何演讲，学习了如何系统化思考……\n\n在学校的时候，为了工作而学习。现在或者未来，要为了学习而工作，不断学习新的技能。\n\n学习是应对各种变化的不二法门。学习，在路上……\n\n## 创新\n创新这个词呢，总会和 造轮子这个词关联起来。\n\n2019年， 造轮子这个词很频繁，频繁的有点令我不知所措。实际上，我个人不反对造轮子，但是反对 为了造轮子而造轮子。每种技术都有他的能力边界，在 造轮子的时候，要首先识别到能力的边界，然后找到一种快速有效的方式来解决我们的问题，这才是创新。\n\n技术的进步就是一个造轮子的过程。为什么有了c++还会有java，有了java还会有kotlin？为什么有了苹果还会有安卓？为什么有了快手还会有抖音？……\n\n未来，在创新的路上，需要更多的践行work smarter not work harder。\n\n![](2.jpg)\n\n## 突破\n毫无疑问，未来肯定是新人的天下。\n\n未来需要更多的想，怎么能够让新人可以站在巨人的肩膀上成长；也要更多的想，怎们样才能够让自己成为人老心不老的老新人。\n\n工作中，更无须给自己设限，这个我可能不行呢，那个我可能也搞不定呀，这个我没接触过呀，那个我没学过呀。怕什么呢，不试试怎们知道不行，不搞一搞怎么知道搞不定，没接触过就接触嘛，没学过就学起来嘛，没有什么大不了的。\n\n工程师不就是要逢山开路遇水搭桥吗？要坚信，在我们可以想象的范围内，没有技术解决不了的问题。\n\n冲破牢笼，方得始终。\n\n加勒比海盗中有个桥段：\n> 船长：扬帆！起航！\n> 水手：什么方向？\n> 船长：海盗需要什么方向？起航！\n\n2020，扬帆，起航！乘着梦想之风，向着星光的方向，找到最好的自己。","source":"_posts/where-of-what-is-past-is-prologue.md","raw":"---\ntitle: 凡所过往，皆是序章\nreward: false\ndate: 2020-02-05 15:00:23\ncategories: 总结\ntags: 测试工作\n---\n\n凡所过往，皆是序章，新的一年，用4个关键词来讲一讲2019年自己心中的过往和序章。\n\n![](1.jpg)\n\n<!--more-->\n\n## 变化\n如果要用1个词来形容2019年的话，我觉得就是 变化了。无论内部，还是外部，都在发生着翻天覆地的变化。变化这种滋味起初很痛苦，但是却蕴含着无限的机会。\n\n未来，变化会是常态。工作方式要变，工具平台要变，开发语言要变……所有的事情都在向着好的方向变化、发展。\n\n在这个变化的过程中，不能随波逐流，也不能固守己见。需要顺应变化，找到其中的机会，顺势而为，乘风而起。\n\n## 学习\n在快速的变化中，怎么才能乘风而起？秘诀也就是 学习了。\n\n2019年，我学习了opengl，kotlin；\n\n2019年，学习了数字视频，ffmpeg，opencv；\n\n2019年，学习了Android的相机系统，学习了逆向破解；\n\n2019年，学习了如何写作，学习了如何演讲，学习了如何系统化思考……\n\n在学校的时候，为了工作而学习。现在或者未来，要为了学习而工作，不断学习新的技能。\n\n学习是应对各种变化的不二法门。学习，在路上……\n\n## 创新\n创新这个词呢，总会和 造轮子这个词关联起来。\n\n2019年， 造轮子这个词很频繁，频繁的有点令我不知所措。实际上，我个人不反对造轮子，但是反对 为了造轮子而造轮子。每种技术都有他的能力边界，在 造轮子的时候，要首先识别到能力的边界，然后找到一种快速有效的方式来解决我们的问题，这才是创新。\n\n技术的进步就是一个造轮子的过程。为什么有了c++还会有java，有了java还会有kotlin？为什么有了苹果还会有安卓？为什么有了快手还会有抖音？……\n\n未来，在创新的路上，需要更多的践行work smarter not work harder。\n\n![](2.jpg)\n\n## 突破\n毫无疑问，未来肯定是新人的天下。\n\n未来需要更多的想，怎么能够让新人可以站在巨人的肩膀上成长；也要更多的想，怎们样才能够让自己成为人老心不老的老新人。\n\n工作中，更无须给自己设限，这个我可能不行呢，那个我可能也搞不定呀，这个我没接触过呀，那个我没学过呀。怕什么呢，不试试怎们知道不行，不搞一搞怎么知道搞不定，没接触过就接触嘛，没学过就学起来嘛，没有什么大不了的。\n\n工程师不就是要逢山开路遇水搭桥吗？要坚信，在我们可以想象的范围内，没有技术解决不了的问题。\n\n冲破牢笼，方得始终。\n\n加勒比海盗中有个桥段：\n> 船长：扬帆！起航！\n> 水手：什么方向？\n> 船长：海盗需要什么方向？起航！\n\n2020，扬帆，起航！乘着梦想之风，向着星光的方向，找到最好的自己。","slug":"where-of-what-is-past-is-prologue","published":1,"updated":"2020-02-07T03:25:20.604Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9kyh07g000x1pdb4ekn4uee","content":"<p>凡所过往，皆是序章，新的一年，用4个关键词来讲一讲2019年自己心中的过往和序章。</p>\n<p><img src=\"/2020/02/05/where-of-what-is-past-is-prologue/1.jpg\" alt></p>\n<a id=\"more\"></a>\n\n<h2 id=\"变化\"><a href=\"#变化\" class=\"headerlink\" title=\"变化\"></a>变化</h2><p>如果要用1个词来形容2019年的话，我觉得就是 变化了。无论内部，还是外部，都在发生着翻天覆地的变化。变化这种滋味起初很痛苦，但是却蕴含着无限的机会。</p>\n<p>未来，变化会是常态。工作方式要变，工具平台要变，开发语言要变……所有的事情都在向着好的方向变化、发展。</p>\n<p>在这个变化的过程中，不能随波逐流，也不能固守己见。需要顺应变化，找到其中的机会，顺势而为，乘风而起。</p>\n<h2 id=\"学习\"><a href=\"#学习\" class=\"headerlink\" title=\"学习\"></a>学习</h2><p>在快速的变化中，怎么才能乘风而起？秘诀也就是 学习了。</p>\n<p>2019年，我学习了opengl，kotlin；</p>\n<p>2019年，学习了数字视频，ffmpeg，opencv；</p>\n<p>2019年，学习了Android的相机系统，学习了逆向破解；</p>\n<p>2019年，学习了如何写作，学习了如何演讲，学习了如何系统化思考……</p>\n<p>在学校的时候，为了工作而学习。现在或者未来，要为了学习而工作，不断学习新的技能。</p>\n<p>学习是应对各种变化的不二法门。学习，在路上……</p>\n<h2 id=\"创新\"><a href=\"#创新\" class=\"headerlink\" title=\"创新\"></a>创新</h2><p>创新这个词呢，总会和 造轮子这个词关联起来。</p>\n<p>2019年， 造轮子这个词很频繁，频繁的有点令我不知所措。实际上，我个人不反对造轮子，但是反对 为了造轮子而造轮子。每种技术都有他的能力边界，在 造轮子的时候，要首先识别到能力的边界，然后找到一种快速有效的方式来解决我们的问题，这才是创新。</p>\n<p>技术的进步就是一个造轮子的过程。为什么有了c++还会有java，有了java还会有kotlin？为什么有了苹果还会有安卓？为什么有了快手还会有抖音？……</p>\n<p>未来，在创新的路上，需要更多的践行work smarter not work harder。</p>\n<p><img src=\"/2020/02/05/where-of-what-is-past-is-prologue/2.jpg\" alt></p>\n<h2 id=\"突破\"><a href=\"#突破\" class=\"headerlink\" title=\"突破\"></a>突破</h2><p>毫无疑问，未来肯定是新人的天下。</p>\n<p>未来需要更多的想，怎么能够让新人可以站在巨人的肩膀上成长；也要更多的想，怎们样才能够让自己成为人老心不老的老新人。</p>\n<p>工作中，更无须给自己设限，这个我可能不行呢，那个我可能也搞不定呀，这个我没接触过呀，那个我没学过呀。怕什么呢，不试试怎们知道不行，不搞一搞怎么知道搞不定，没接触过就接触嘛，没学过就学起来嘛，没有什么大不了的。</p>\n<p>工程师不就是要逢山开路遇水搭桥吗？要坚信，在我们可以想象的范围内，没有技术解决不了的问题。</p>\n<p>冲破牢笼，方得始终。</p>\n<p>加勒比海盗中有个桥段：</p>\n<blockquote>\n<p>船长：扬帆！起航！<br>水手：什么方向？<br>船长：海盗需要什么方向？起航！</p>\n</blockquote>\n<p>2020，扬帆，起航！乘着梦想之风，向着星光的方向，找到最好的自己。</p>\n","site":{"data":{}},"excerpt":"<p>凡所过往，皆是序章，新的一年，用4个关键词来讲一讲2019年自己心中的过往和序章。</p>\n<p><img src=\"/2020/02/05/where-of-what-is-past-is-prologue/1.jpg\" alt></p>","more":"<h2 id=\"变化\"><a href=\"#变化\" class=\"headerlink\" title=\"变化\"></a>变化</h2><p>如果要用1个词来形容2019年的话，我觉得就是 变化了。无论内部，还是外部，都在发生着翻天覆地的变化。变化这种滋味起初很痛苦，但是却蕴含着无限的机会。</p>\n<p>未来，变化会是常态。工作方式要变，工具平台要变，开发语言要变……所有的事情都在向着好的方向变化、发展。</p>\n<p>在这个变化的过程中，不能随波逐流，也不能固守己见。需要顺应变化，找到其中的机会，顺势而为，乘风而起。</p>\n<h2 id=\"学习\"><a href=\"#学习\" class=\"headerlink\" title=\"学习\"></a>学习</h2><p>在快速的变化中，怎么才能乘风而起？秘诀也就是 学习了。</p>\n<p>2019年，我学习了opengl，kotlin；</p>\n<p>2019年，学习了数字视频，ffmpeg，opencv；</p>\n<p>2019年，学习了Android的相机系统，学习了逆向破解；</p>\n<p>2019年，学习了如何写作，学习了如何演讲，学习了如何系统化思考……</p>\n<p>在学校的时候，为了工作而学习。现在或者未来，要为了学习而工作，不断学习新的技能。</p>\n<p>学习是应对各种变化的不二法门。学习，在路上……</p>\n<h2 id=\"创新\"><a href=\"#创新\" class=\"headerlink\" title=\"创新\"></a>创新</h2><p>创新这个词呢，总会和 造轮子这个词关联起来。</p>\n<p>2019年， 造轮子这个词很频繁，频繁的有点令我不知所措。实际上，我个人不反对造轮子，但是反对 为了造轮子而造轮子。每种技术都有他的能力边界，在 造轮子的时候，要首先识别到能力的边界，然后找到一种快速有效的方式来解决我们的问题，这才是创新。</p>\n<p>技术的进步就是一个造轮子的过程。为什么有了c++还会有java，有了java还会有kotlin？为什么有了苹果还会有安卓？为什么有了快手还会有抖音？……</p>\n<p>未来，在创新的路上，需要更多的践行work smarter not work harder。</p>\n<p><img src=\"/2020/02/05/where-of-what-is-past-is-prologue/2.jpg\" alt></p>\n<h2 id=\"突破\"><a href=\"#突破\" class=\"headerlink\" title=\"突破\"></a>突破</h2><p>毫无疑问，未来肯定是新人的天下。</p>\n<p>未来需要更多的想，怎么能够让新人可以站在巨人的肩膀上成长；也要更多的想，怎们样才能够让自己成为人老心不老的老新人。</p>\n<p>工作中，更无须给自己设限，这个我可能不行呢，那个我可能也搞不定呀，这个我没接触过呀，那个我没学过呀。怕什么呢，不试试怎们知道不行，不搞一搞怎么知道搞不定，没接触过就接触嘛，没学过就学起来嘛，没有什么大不了的。</p>\n<p>工程师不就是要逢山开路遇水搭桥吗？要坚信，在我们可以想象的范围内，没有技术解决不了的问题。</p>\n<p>冲破牢笼，方得始终。</p>\n<p>加勒比海盗中有个桥段：</p>\n<blockquote>\n<p>船长：扬帆！起航！<br>水手：什么方向？<br>船长：海盗需要什么方向？起航！</p>\n</blockquote>\n<p>2020，扬帆，起航！乘着梦想之风，向着星光的方向，找到最好的自己。</p>"},{"title":"数字视频技术导论","reward":false,"top":false,"date":"2020-02-28T00:49:46.000Z","_content":"\n# 背景\n因为工作关系需要了解数字视频相关技术，在学习的过程中找到了这份托管在github上的**数字视频导论**的材料。\n\n这份材料介绍了基本的数字视频相关技术，言简意赅但又不枯燥无味，即有理论又有丰富的实践操作，在我学习的所有材料中算是比较上乘的材料。\n\n基于如上的原因，将该材料的相关内容转载到此处。大家可以直接访问该材料的github仓库[**digital_video_introduction**](https://github.com/leandromoreira/digital_video_introduction)获取相关内容。如下的所有内容皆来自[**digital_video_introduction**](https://github.com/leandromoreira/digital_video_introduction)，特此标注。\n\n[![license](https://img.shields.io/badge/license-BSD--3--Clause-blue.svg)](https://img.shields.io/badge/license-BSD--3--Clause-blue.svg)\n\n<!--more-->\n\n# 介绍\n这是一份循序渐进的视频技术的介绍。尽管它面向的是软件开发人员/工程师，但我们希望**对任何人而言**，这份文档都能简单易学。这个点子产生于一个[视频技术新手小型研讨会](https://docs.google.com/presentation/d/17Z31kEkl_NGJ0M66reqr9_uTG6tI5EDDVXpdPKVuIrs/edit#slide=id.p)期间。\n\n本文档旨在尽可能使用**浅显的词语，丰富的图像和实际例子**介绍数字视频概念，使这些知识能适用于各种场合。你可以随时反馈意见或建议，以改进这篇文档。\n\n# 目录\n\n- [介绍](#介绍)\n- [目录](#目录)\n- [基本术语](#基本术语)\n  * [编码彩色图像的其它方法](#编码彩色图像的其它方法)\n  * [自己动手：玩转图像和颜色](#自己动手：玩转图像和颜色)\n  * [DVD 的 DAR 是 4:3](#DVD-的-DAR-是-4-3)\n  * [自己动手：检查视频属性](#自己动手：检查视频属性)\n- [消除冗余](#消除冗余)\n  * [颜色，亮度和我们的眼睛](#颜色，亮度和我们的眼睛)\n    + [颜色模型](#颜色模型)\n    + [YCbCr 和 RGB 之间的转换](#YCbCr-和-RGB-之间的转换)\n    + [色度子采样](#色度子采样)\n    + [自己动手：检查 YCbCr 直方图](#自己动手：检查-YCbCr-直方图)\n  * [帧类型](#帧类型)\n    + [I 帧（内部，关键帧）](#I-帧（帧内编码，关键帧）)\n    + [P 帧（预测）](#P-帧（预测）)\n      - [自己动手：具有单个 I 帧的视频](#自己动手：具有单个-I-帧的视频)\n    + [B 帧（双向预测）](#B-帧（双向预测）)\n      - [自己动手：使用 B 帧比较视频](#自己动手：使用-B-帧比较视频)\n    + [小结](#小结)\n  * [时间冗余（帧间预测）](#时间冗余（帧间预测）)\n      - [自己动手：查看运动向量](#自己动手：查看运动向量)\n  * [空间冗余（帧内预测）](#空间冗余（帧内预测）)\n      - [自己动手：查看帧内预测](#自己动手：查看帧内预测)\n- [视频编解码器是如何工作的？](#视频编解码器是如何工作的？)\n  * [是什么？为什么？怎么做？](#是什么？为什么？怎么做？)\n  * [历史](#历史)\n    + [AV1 的诞生](#AV1-的诞生)\n  * [通用编解码器](#通用编解码器)\n  * [第一步 - 图片分区](#第一步-图片分区)\n    + [自己动手：查看分区](#自己动手：查看分区)\n  * [第二步 - 预测](#第二步-预测)\n  * [第三步 - 转换](#第三步-转换)\n    + [自己动手：丢弃不同的系数](#自己动手：丢弃不同的系数)\n  * [第四步 - 量化](#第四步-量化)\n    + [自己动手：量化](#自己动手：量化)\n  * [第五步 - 熵编码](#第五步-熵编码)\n    + [VLC 编码](#VLC-编码：)\n    + [算术编码](#算术编码)\n    + [自己动手：CABAC vs CAVLC](#自己动手：CABAC-vs-CAVLC)\n  * [第六步 - 比特流格式](#第六步-比特流格式)\n    + [H.264 比特流](#H-264-比特流)\n    + [自己动手：检查 H.264 比特流](#自己动手：检查-H-264-比特流)\n  * [回顾](#回顾)\n  * [H.265 如何实现比 H.264 更好的压缩率?](#H-265-如何实现比-H-264-更好的压缩率)\n- [在线流媒体](#在线流媒体)\n  * [通用架构](#通用架构)\n  * [渐进式下载和自适应流](#渐进式下载和自适应流)\n  * [内容保护](#内容保护)\n- [如何使用 jupyter](#如何使用-jupyter)\n- [会议](#会议)\n- [参考](#参考)\n\n# 基本术语\n\n一个**图像**可以视作一个**二维矩阵**。如果将**色彩**考虑进来，我们可以做出推广：将这个图像视作一个**三维矩阵**——多出来的维度用于储存色彩信息。\n\n如果我们选择三原色（红、绿、蓝）代表这些色彩，这就定义了三个平面：第一个是红色平面，第二个是绿色平面，最后一个是蓝色平面。\n\n![an image is a 3d matrix RGB](i/image_3d_matrix_rgb.png \"An image is a 3D matrix\")\n\n我们把这个矩阵里的每一个点称为**像素**（图像元素）。像素的色彩由三原色的**强度**（通常用数值表示）表示。例如，一个**红色像素**是指强度为 0 的绿色，强度为 0 的蓝色和强度最大的红色。**粉色像素**可以通过三种颜色的组合表示。如果规定强度的取值范围是 0 到 255，**红色 255、绿色 192、蓝色 203** 则表示粉色。\n\n> ### 编码彩色图像的其它方法\n>\n> 还有许多其它模型也可以用来表示色彩，进而组成图像。例如，给每种颜色都标上序号（如下图），这样每个像素仅需一个字节就可以表示出来，而不是 RGB 模型通常所需的 3 个。在这样一个模型里我们可以用一个二维矩阵来代替三维矩阵去表示我们的色彩，这将节省存储空间，但色彩的数量将会受限。\n>\n> ![NES palette](i/nes-color-palette.png \"NES palette\")\n\n\n例如以下几张图片。第一张包含所有颜色平面。剩下的分别是红、绿、蓝色平面（显示为灰调）（译注：颜色强度高的地方显示为亮色，强度低为暗色）。\n\n\n![RGB channels intensity](i/rgb_channels_intensity.png \"RGB channels intensity\")\n\n我们可以看到，对于最终的成像，红色平面对强度的贡献更多（三个平面最亮的是红色平面），蓝色平面（最后一张图片）的贡献大多只在马里奥的眼睛和他衣服的一部分。所有颜色平面对马里奥的胡子（最暗的部分）均贡献较少。\n\n存储颜色的强度，需要占用一定大小的数据空间，这个大小被称为颜色深度。假如每个颜色（平面）的强度占用 8 bit（取值范围为 0 到 255），那么颜色深度就是 24（8*3）bit，我们还可以推导出我们可以使用 2 的 24 次方种不同的颜色。\n\n> 很棒的学习材料：[现实世界的照片是如何拍摄成 0 和 1 的](http://www.cambridgeincolour.com/tutorials/camera-sensors.htm)。\n\n图片的另一个属性是**分辨率**，即一个平面内像素的数量。通常表示成宽*高，例如下面这张 **4x4** 的图片。\n\n![image resolution](i/resolution.png \"image resolution\")\n\n> ### 自己动手：玩转图像和颜色\n>\n> 你可以使用 [jupyter](#如何使用-jupyter)（python, numpy, matplotlib 等等）[玩转图像](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/image_as_3d_array.ipynb)。\n>\n> 你也可以学习[图像滤镜（边缘检测，磨皮，模糊。。。）的原理](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/filters_are_easy.ipynb)。\n\n图像或视频还有一个属性是宽高比，它简单地描述了图像或像素的宽度和高度之间的比例关系。\n\n当人们说这个电影或照片是 16:9 时，通常是指显示宽高比（DAR），然而我们也可以有不同形状的单个像素，我们称为像素宽高比（PAR）。\n\n![display aspect ratio](i/DAR.png \"display aspect ratio\")\n\n![pixel aspect ratio](i/PAR.png \"pixel aspect ratio\")\n\n> ## DVD 的 DAR 是 4:3\n>\n> 虽然 DVD 的实际分辨率是 704x480，但它依然保持 4:3 的宽高比，因为它有一个 10:11（704x10／480x11）的 PAR。\n\n现在我们可以将**视频**定义为在**单位时间**内**连续的 n 帧**，这可以视作一个新的维度，n 即为帧率，若单位时间为秒，则等同于 FPS (每秒帧数 Frames Per Second)。\n\n![video](i/video.png \"video\")\n\n播放一段视频每秒所需的数据量就是它的**比特率**（即常说的码率）。\n> 比特率 = 宽 * 高 * 颜色深度 * 帧每秒\n\n例如，一段每秒 30 帧，每像素 24 bits，分辨率是 480x240 的视频，如果我们不做任何压缩，它将需要 **82,944,000 比特每秒**或 82.944 Mbps (30x480x240x24)。\n\n当**比特率**几乎恒定时称为恒定比特率（**CBR**）；但它也可以变化，称为可变比特率（**VBR**）。\n\n> 这个图形显示了一个受限的 VBR，当帧为黑色时不会花费太多的数据量。\n>\n> ![constrained vbr](i/vbr.png \"constrained vbr\")\n\n在早期，工程师们想出了一项技术能将视频的感官帧率加倍而**没有消耗额外带宽**。这项技术被称为**隔行扫描**；总的来说，它在一个时间点发送一个画面——画面用于填充屏幕的一半，而下一个时间点发送的画面用于填充屏幕的另一半。\n\n如今的屏幕渲染大多使用**逐行扫描技术**。这是一种显示、存储、传输运动图像的方法，每帧中的所有行都会被依次绘制。\n\n![interlaced vs progressive](i/interlaced_vs_progressive.png \"interlaced vs progressive\")\n\n现在我们知道了数字化**图像**的原理；它的**颜色**的编排方式；给定**帧率**和**分辨率**时，展示一个视频需要花费多少**比特率**；它是恒定的（CBR）还是可变的（VBR）；还有很多其它内容，如隔行扫描和 PAR。\n\n> ## 自己动手：检查视频属性\n> 你可以[使用 ffmpeg 或 mediainfo 检查大多数属性的解释](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#inspect-stream)。\n\n# 消除冗余\n\n我们认识到，不对视频进行压缩是不行的；**一个单独的一小时长的视频**，分辨率为 720p 和 30fps 时将**需要 278GB<sup>\\*</sup>**。仅仅使用无损数据压缩算法——如 DEFLATE（被PKZIP, Gzip, 和 PNG 使用）——也无法充分减少视频所需的带宽，我们需要找到其它压缩视频的方法。\n\n> <sup>*</sup>我们使用乘积得出这个数字 1280 x 720 x 24 x 30 x 3600 （宽，高，每像素比特数，fps 和秒数）\n\n为此，我们可以**利用视觉特性**：和区分颜色相比，我们区分亮度要更加敏锐。**时间上的重复**：一段视频包含很多只有一点小小改变的图像。**图像内的重复**：每一帧也包含很多颜色相同或相似的区域。\n\n## 颜色，亮度和我们的眼睛\n\n我们的眼睛[对亮度比对颜色更敏感](http://vanseodesign.com/web-design/color-luminance/)，你可以看看下面的图片自己测试。\n\n![luminance vs color](i/luminance_vs_color.png \"luminance vs color\")\n\n如果你看不出左图的**方块 A 和方块 B** 的颜色是**相同的**，那么好，是我们的大脑玩了一个小把戏，这让我们更多的去注意光与暗，而不是颜色。右边这里有一个使用同样颜色的连接器，那么我们（的大脑）就能轻易分辨出事实，它们是同样的颜色。\n\n> **简单解释我们的眼睛工作的原理**\n>\n> [眼睛是一个复杂的器官](http://www.biologymad.com/nervoussystem/eyenotes.htm)，有许多部分组成，但我们最感兴趣的是视锥细胞和视杆细胞。眼睛有[大约1.2亿个视杆细胞和6百万个视锥细胞](https://en.wikipedia.org/wiki/Photoreceptor_cell)。\n>\n> **简单来说**，让我们把颜色和亮度放在眼睛的功能部位上。[视杆细胞](https://en.wikipedia.org/wiki/Rod_cell)**主要负责亮度**，而[视锥细胞](https://en.wikipedia.org/wiki/Cone_cell)**负责颜色**，有三种类型的视锥，每个都有不同的颜料，叫做：[S-视锥（蓝色），M-视锥（绿色）和L-视锥（红色）](https://upload.wikimedia.org/wikipedia/commons/1/1e/Cones_SMJ2_E.svg)。\n>\n> 既然我们的视杆细胞（亮度）比视锥细胞多很多，一个合理的推断是相比颜色，我们有更好的能力去区分黑暗和光亮。\n>\n> ![eyes composition](i/eyes.jpg \"eyes composition\")\n\n一旦我们知道我们对**亮度**（图像中的亮度）更敏感，我们就可以利用它。\n\n### 颜色模型\n\n我们最开始学习的[彩色图像的原理](#基本术语)使用的是 **RGB 模型**，但也有其他模型。有一种模型将亮度（光亮）和色度（颜色）分离开，它被称为 **YCbCr**<sup>*</sup>。\n\n> <sup>*</sup> 有很多种模型做同样的分离。\n\n这个颜色模型使用 **Y** 来表示亮度，还有两种颜色通道：Cb（蓝色色度） 和 Cr（红色色度）。YCbCr 可以由 RGB 转换得来，也可以转换回 RGB。使用这个模型我们可以创建拥有完整色彩的图像，如下图。\n\n![ycbcr 例子](i/ycbcr.png \"ycbcr 例子\")\n\n### YCbCr 和 RGB 之间的转换\n\n有人可能会问，在**不使用绿色（色度）**的情况下，我们如何表现出所有的色彩？\n\n为了回答这个问题，我们将介绍从 RGB 到 YCbCr 的转换。我们将使用 [ITU-R 小组](https://en.wikipedia.org/wiki/ITU-R)*建议的[标准 BT.601](https://en.wikipedia.org/wiki/Rec._601) 中的系数。\n\n第一步是计算亮度，我们将使用 ITU 建议的常量，并替换 RGB 值。\n\n```\nY = 0.299R + 0.587G + 0.114B\n```\n\n一旦我们有了亮度后，我们就可以拆分颜色（蓝色色度和红色色度）：\n\n```\nCb = 0.564(B - Y)\nCr = 0.713(R - Y)\n```\n\n并且我们也可以使用 YCbCr 转换回来，甚至得到绿色。\n\n```\nR = Y + 1.402Cr\nB = Y + 1.772Cb\nG = Y - 0.344Cb - 0.714Cr\n```\n\n> <sup>*</sup>组织和标准在数字视频领域中很常见，它们通常定义什么是标准，例如，[什么是 4K？我们应该使用什么帧率？分辨率？颜色模型？](https://en.wikipedia.org/wiki/Rec._2020)\n\n通常，**显示屏**（监视器，电视机，屏幕等等）**仅使用 RGB 模型**，并以不同的方式来组织，看看下面这些放大效果：\n\n![pixel geometry](i/new_pixel_geometry.jpg \"pixel geometry\")\n\n### 色度子采样\n\n一旦我们能从图像中分离出亮度和色度，我们就可以利用人类视觉系统对亮度比色度更敏感的特点，选择性地剔除信息。**色度子采样**是一种编码图像时，使**色度分辨率低于亮度**的技术。\n\n![ycbcr 子采样分辨率](i/ycbcr_subsampling_resolution.png \"ycbcr 子采样分辨率\")\n\n我们应该减少多少色度分辨率呢？已经有一些模式定义了如何处理分辨率和合并（`最终的颜色 = Y + Cb + Cr`）。\n\n这些模式称为子采样系统，并被表示为 3 部分的比率 - `a:x:y`，其定义了色度平面的分辨率，与亮度平面上的、分辨率为 `a x 2` 的小块之间的关系。\n* `a` 是水平采样参考 (通常是 4)，\n* `x` 是第一行的色度样本数（相对于 a 的水平分辨率），\n* `y` 是第二行的色度样本数。\n\n> 存在的一个例外是 4:1:0，其在每个亮度平面分辨率为 4 x 4 的块内提供一个色度样本。\n\n现代编解码器中使用的常用方案是： 4:4:4 (没有子采样)**, 4:2:2, 4:1:1, 4:2:0, 4:1:0 and 3:1:1。\n\n> YCbCr 4:2:0 合并\n>\n> 这是使用 YCbCr 4:2:0 合并的一个图像的一块，注意我们每像素只花费 12bit。\n>\n> ![YCbCr 4:2:0 合并](i/ycbcr_420_merge.png \"YCbCr 4:2:0 合并\")\n\n下图是同一张图片使用几种主要的色度子采样技术进行编码，第一行图像是最终的 YCbCr，而最后一行图像展示了色度的分辨率。这么小的损失确实是一个伟大的胜利。\n\n![色度子采样例子](i/chroma_subsampling_examples.jpg \"色度子采样例子\")\n\n前面我们计算过我们需要 [278GB 去存储一个一小时长，分辨率在720p和30fps的视频文件](#消除冗余)。如果我们使用 `YCbCr 4:2:0` 我们能剪掉`一半的大小（139GB）`<sup>*</sup>，但仍然不够理想。\n> <sup>*</sup> 我们通过将宽、高、颜色深度和 fps 相乘得出这个值。前面我们需要 24 bit，现在我们只需要 12 bit。\n\n> ### 自己动手：检查 YCbCr 直方图\n> 你可以[使用 ffmpeg 检查 YCbCr 直方图](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#generates-yuv-histogram)。这个场景有更多的蓝色贡献，由[直方图](https://en.wikipedia.org/wiki/Histogram)显示。\n>\n> ![ycbcr 颜色直方图](i/yuv_histogram.png \"ycbcr 颜色直方图\")\n\n## 帧类型\n\n现在我们进一步消除`时间冗余`，但在这之前让我们来确定一些基本术语。假设我们一段 30fps 的影片，这是最开始的 4 帧。\n\n![球 1](i/smw_background_ball_1.png \"球 1\") ![球 2](i/smw_background_ball_2.png \"球 2\") ![球 3](i/smw_background_ball_3.png \"球 3\")\n![球 4](i/smw_background_ball_4.png \"球 4\")\n\n我们可以在帧内看到**很多重复内容**，如**蓝色背景**，从 0 帧到第 3 帧它都没有变化。为了解决这个问题，我们可以将它们**抽象地分类**为三种类型的帧。\n\n### I 帧（帧内编码，关键帧）\n\nI 帧（可参考，关键帧，帧内编码）是一个**自足的帧**。它不依靠任何东西来渲染，I 帧与静态图片相似。第一帧通常是 I 帧，但我们将看到 I 帧被定期插入其它类型的帧之间。\n\n![球 1](i/smw_background_ball_1.png \"球 1\")\n\n### P 帧（预测）\n\nP 帧利用了一个事实：当前的画面几乎总能**使用之前的一帧进行渲染**。例如，在第二帧，唯一的改变是球向前移动了。仅仅使用（第二帧）对前一帧的引用和差值，我们就能重建前一帧。\n\n![球 1](i/smw_background_ball_1.png \"球 1\") <-  ![球 2](i/smw_background_ball_2_diff.png \"球 2\")\n\n> #### 自己动手：具有单个 I 帧的视频\n> 既然 P 帧使用较少的数据，为什么我们不能用[单个 I 帧和其余的 P 帧](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#1-i-frame-and-the-rest-p-frames)来编码整个视频？\n>\n> 编码完这个视频之后，开始观看它，并**快进到视频的末尾部分**，你会注意到**它需要花一些时间**才真正跳转到这部分。这是因为 **P 帧需要一个引用帧**（比如 I 帧）才能渲染。\n>\n> 你可以做的另一个快速试验，是使用单个 I 帧编码视频，然后[再次编码且每 2 秒插入一个 I 帧](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#1-i-frames-per-second-vs-05-i-frames-per-second)，并**比较成品的大小**。\n\n### B 帧（双向预测）\n\n如何引用前面和后面的帧去做更好的压缩？！简单地说 B 帧就是这么做的。\n\n ![球 1](i/smw_background_ball_1.png \"球 1\") <-  ![球 2](i/smw_background_ball_2_diff.png \"球 2\") -> ![球 3](i/smw_background_ball_3.png \"球 3\")\n\n> #### 自己动手：使用 B 帧比较视频\n> 你可以生成两个版本，一个使用 B 帧，另一个[全部不使用 B 帧](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#no-b-frames-at-all)，然后查看文件的大小以及画质。\n\n### 小结\n\n这些帧类型用于提供更好的压缩率，我们将在下一章看到这是如何发生的。现在，我们可以想到 I 帧是昂贵的，P 帧是便宜的，最便宜的是 B 帧。\n\n![帧类型例子](i/frame_types.png \"帧类型例子\")\n\n## 时间冗余（帧间预测）\n\n让我们探究去除**时间上的重复**，去除这一类冗余的技术就是**帧间预测**。\n\n我们将尝试**花费较少的数据量**去编码在时间上连续的 0 号帧和 1 号帧。\n\n![原始帧](i/original_frames.png \"原始帧\")\n\n我们可以做个减法，我们简单地**用 0 号帧减去 1 号帧**，得到残差，这样我们就只需要**对残差进行编码**。\n\n![残差帧](i/difference_frames.png \"残差帧\")\n\n但我们有一个**更好的方法**来节省数据量。首先，我们将`0 号帧` 视为一个个分块的集合，然后我们将尝试将 `帧 1` 和 `帧 0` 上的块相匹配。我们可以将这看作是**运动预测**。\n\n> ### 维基百科—块运动补偿\n> “运动补偿是一种描述相邻帧（相邻在这里表示在编码关系上相邻，在播放顺序上两帧未必相邻）差别的方法，具体来说是描述前面一帧（相邻在这里表示在编码关系上的前面，在播放顺序上未必在当前帧前面）的每个小块怎样移动到当前帧中的某个位置去。”\n\n![原始帧运动预测](i/original_frames_motion_estimation.png \"原始帧运动预测\")\n\n我们预计那个球会从 `x=0, y=25` 移动到 `x=6, y=26`，**x** 和 **y** 的值就是**运动向量**。**进一步**节省数据量的方法是，只编码这两者运动向量的差。所以，最终运动向量就是 `x=6 (6-0), y=1 (26-25)`。\n\n> 实际情况下，这个球会被切成 n 个分区，但处理过程是相同的。\n\n帧上的物体**以三维方式移动**，当球移动到背景时会变小。当我们尝试寻找匹配的块，**找不到完美匹配的块**是正常的。这是一张运动预测与实际值相叠加的图片。\n\n![运动预测](i/motion_estimation.png \"运动预测\")\n\n但我们能看到当我们使用**运动预测**时，**编码的数据量少于**使用简单的残差帧技术。\n\n![运动预测 vs 残差 ](i/comparison_delta_vs_motion_estimation.png \"运动预测 vs 残差\")\n\n你可以[使用 jupyter 玩转这些概念](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/frame_difference_vs_motion_estimation_plus_residual.ipynb)。\n\n> ### 自己动手：查看运动向量\n>\n> 我们可以[使用 ffmpeg 生成包含帧间预测（运动向量）的视频](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#generate-debug-video)。\n>\n> ![ffmpeg 帧间预测（运动向量）](i/motion_vectors_ffmpeg.png \"ffmpeg 帧间预测（运动向量）\")\n>\n> 或者我们也可使用 [Intel® Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer)（需要付费，但也有只能查看前 10 帧的免费试用版）。\n>\n> ![Intel® Video Pro Analyzer 使用帧间预测](i/inter_prediction_intel_video_pro_analyzer.png \"inter prediction intel video pro analyzer\")\n\n## 空间冗余（帧内预测）\n如果我们分析一个视频里的**每一帧**，我们会看到有**许多区域是相互关联的**。\n\n![空间内重复](i/repetitions_in_space.png \"空间内重复\")\n\n让我们举一个例子。这个场景大部分由蓝色和白色组成。\n\n![smw 背景](i/smw_bg.png \"smw 背景\")\n\n这是一个 `I 帧`，我们**不能使用前面的帧来预测**，但我们仍然可以压缩它。我们将编码我们选择的那块红色区域。如果我们**看看它的周围**，我们可以**估计它周围颜色的变化**。\n\n![smw 背景块](i/smw_bg_block.png \"smw 背景块\")\n\n我们预测:帧中的颜色在垂直方向上保持一致，这意味着**未知像素的颜色与临近的像素相同**。\n\n![smw 背景预测](i/smw_bg_prediction.png \"smw 背景预测\")\n\n我们的**预测会出错**，所以我们需要先利用这项技术（**帧内预测**），然后**减去实际值**，算出残差，得出的矩阵比原始数据更容易压缩。\n\n![smw 残差](i/smw_residual.png \"smw 残差\")\n\n> ### 自己动手：查看帧内预测\n> 你可以[使用 ffmpeg 生成包含宏块及预测的视频](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#generate-debug-video)。请查看 ffmpeg 文档以了解[每个块颜色的含义](https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors)。\n>\n> ![ffmpeg 帧内预测（宏块）](i/macro_blocks_ffmpeg.png \"ffmpeg 帧内预测（宏块）\")\n>\n> 或者我们也可使用 [Intel® Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer)（需要付费，但也有只能查看前 10 帧的免费试用版）。\n>\n> ![Intel® Video Pro Analyzer 帧内预测](i/intra_prediction_intel_video_pro_analyzer.png \"Intel® Video Pro Analyzer 帧内预测\")\n\n# 视频编解码器是如何工作的？\n\n## 是什么？为什么？怎么做？\n\n**是什么？** 就是用于压缩或解压数字视频的软件或硬件。**为什么？** 人们需要在有限带宽或存储空间下提高视频的质量。还记得当我们计算每秒 30 帧，每像素 24 bit，分辨率是 480x240 的视频[需要多少带宽](#基本术语)吗？没有压缩时是 **82.944 Mbps**。电视或互联网提供 HD/FullHD/4K 只能靠视频编解码器。**怎么做？** 我们将简单介绍一下主要的技术。\n\n> 视频编解码 vs 容器\n>\n> 初学者一个常见的错误是混淆数字视频编解码器和[数字视频容器](https://en.wikipedia.org/wiki/Digital_container_format)。我们可以将**容器**视为包含视频（也很可能包含音频）元数据的包装格式，**压缩过的视频**可以看成是它承载的内容。\n>\n> 通常，视频文件的格式定义其视频容器。例如，文件 `video.mp4` 可能是 [MPEG-4 Part 14](https://en.wikipedia.org/wiki/MPEG-4_Part_14) 容器，一个叫 `video.mkv` 的文件可能是 [matroska](https://en.wikipedia.org/wiki/Matroska)。我们可以使用 [ffmpeg 或 mediainfo](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#inspect-stream) 来完全确定编解码器和容器格式。\n\n## 历史\n\n在我们跳进通用编解码器内部工作之前，让我们回头了解一些旧的视频编解码器。\n\n视频编解码器 [H.261](https://en.wikipedia.org/wiki/H.261) 诞生在 1990（技术上是 1988），被设计为以 **64 kbit/s 的数据速率**工作。它已经使用如色度子采样、宏块，等等理念。在 1995 年，**H.263** 视频编解码器标准被发布，并继续延续到 2001 年。\n\n在 2003 年 **H.264/AVC** 的第一版被完成。在同一年，一家叫做 **TrueMotion** 的公司发布了他们的**免版税**有损视频压缩的视频编解码器，称为 **VP3**。在 2008 年，**Google 收购了**这家公司，在同一年发布 **VP8**。在 2012 年 12 月，Google 发布了 **VP9**，**市面上大约有 3/4 的浏览器**（包括手机）支持。\n\n[AV1](https://en.wikipedia.org/wiki/AOMedia_Video_1) 是由 **Google, Mozilla, Microsoft, Amazon, Netflix, AMD, ARM, NVidia, Intel, Cisco** 等公司组成的[开放媒体联盟（AOMedia）](http://aomedia.org/)设计的一种新的视频编解码器，免版税，开源。**第一版** 0.1.0 参考编解码器**发布于 2016 年 4 月 7 号**。\n\n![编解码器历史线路图](i/codec_history_timeline.png \"编解码器历史线路图\")\n\n> ### AV1 的诞生\n>\n> 2015 年早期，Google 正在 VP10 上工作，Xiph (Mozilla) 正在 Daala 上工作，Cisco 开源了它的称为 Thor 的免版税视频编解码器。\n>\n> 接着 MPEG LA 宣布了 HEVC (H.265) 每年版税的的上限，比 H.264 高 8 倍，但很快他们又再次改变了条款：\n> *\t**不设年度收费上限**\n> *\t**收取内容费**（收入的 0.5%）\n> *\t**每单位费用高于 h264 的 10 倍**\n>\n> [开放媒体联盟](http://aomedia.org/about-us/)由硬件厂商（Intel, AMD, ARM , Nvidia, Cisco），内容分发商（Google, Netflix, Amazon），浏览器维护者（Google, Mozilla），等公司创建。\n>\n> 这些公司有一个共同目标，一个免版税的视频编解码器，所以 AV1 诞生时使用了一个更[简单的专利许可证](http://aomedia.org/license/patent/)。**Timothy B. Terriberry** 做了一个精彩的介绍，[关于 AV1 的概念，许可证模式和它当前的状态](https://www.youtube.com/watch?v=lzPaldsmJbk)，就是本节的来源。\n>\n> 前往 [http://aomanalyzer.org/](http://aomanalyzer.org/)， 你会惊讶于**使用你的浏览器就可以分析 AV1 编解码器**。\n> ![av1 浏览器分析器](i/av1_browser_analyzer.png \"浏览器分析器\")\n>\n> 附：如果你想了解更多编解码器的历史，你需要了解[视频压缩专利](https://www.vcodex.com/video-compression-patents/)背后的基本知识。\n\n## 通用编解码器\n\n我们接下来要介绍**通用视频编解码器背后的主要机制**，大多数概念都很实用，并被现代编解码器如 VP9, AV1 和 HEVC 使用。需要注意：我们将简化许多内容。有时我们会使用真实的例子（主要是 H.264）来演示技术。\n\n## 第一步 - 图片分区\n\n第一步是**将帧**分成几个**分区**，**子分区**甚至更多。\n\n![图片分区](i/picture_partitioning.png \"图片分区\")\n\n**但是为什么呢？**有许多原因，比如，当我们分割图片时，我们可以更精确的处理预测，在微小移动的部分使用较小的分区，而在静态背景上使用较大的分区。\n\n通常，编解码器**将这些分区组织**成切片（或瓦片），宏（或编码树单元）和许多子分区。这些分区的最大大小有所不同，HEVC 设置成 64x64，而 AVC 使用 16x16，但子分区可以达到 4x4 的大小。\n\n还记得我们学过的**帧的分类**吗？你也可以**把这些概念应用到块**，因此我们可以有 I 切片，B 切片，I 宏块等等。\n\n> ### 自己动手：查看分区\n>\n> 我们也可以使用 [Intel® Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer)（需要付费，但也有只能查看前 10 帧的免费试用版）。这是 [VP9 分区](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#transcoding)的分析。\n>\n> ![Intel® Video Pro Analyzer VP9 分区视图 ](i/paritions_view_intel_video_pro_analyzer.png \"Intel® Video Pro Analyzer VP9 分区视图\")\n\n## 第二步 - 预测\n\n一旦我们有了分区，我们就可以在它们之上做出预测。对于[帧间预测](#时间冗余（帧间预测）)，我们需要**发送运动向量和残差**；至于[帧内预测](#空间冗余（帧内预测）)，我们需要**发送预测方向和残差**。\n\n## 第三步 - 转换\n\n在我们得到残差块（`预测分区-真实分区`）之后，我们可以用一种方式**变换**它，这样我们就知道**哪些像素我们应该丢弃**，还依然能保持**整体质量**。这个确切的行为有几种变换方式。\n\n尽管有[其它的变换方式](https://en.wikipedia.org/wiki/List_of_Fourier-related_transforms#Discrete_transforms)，但我们重点关注离散余弦变换（DCT）。[DCT](https://en.wikipedia.org/wiki/Discrete_cosine_transform) 的主要功能有：\n*\t将**像素**块**转换**为相同大小的**频率系数块**。\n*\t**压缩**能量，更容易消除空间冗余。\n*\t**可逆的**，也意味着你可以还原回像素。\n\n> 2017 年 2 月 2 号，F. M. Bayer 和 R. J. Cintra 发表了他们的论文：[图像压缩的 DCT 类变换只需要 14 个加法](https://arxiv.org/abs/1702.00817)。\n\n如果你不理解每个要点的好处，不用担心，我们会尝试进行一些实验，以便从中看到真正的价值。\n\n我们来看下面的**像素块**（8x8）：\n\n![像素值矩形](i/pixel_matrice.png \"像素值矩形\")\n\n下面是其渲染的块图像（8x8）：\n\n![像素值矩形](i/gray_image.png \"像素值矩形\")\n\n当我们对这个像素块**应用 DCT** 时， 得到如下**系数块**（8x8）：\n\n![系数值 values](i/dct_coefficient_values.png \"系数值\")\n\n接着如果我们渲染这个系数块，就会得到这张图片：\n\n![dct 系数图片](i/dct_coefficient_image.png \"dct 系数图片\")\n\n如你所见它看起来完全不像原图像，我们可能会注意到**第一个系数**与其它系数非常不同。第一个系数被称为直流分量，代表了输入数组中的**所有样本**，有点**类似于平均值**。\n\n这个系数块有一个有趣的属性：高频部分和低频部分是分离的。\n\n![dct 频率系数属性](i/dctfrequ.jpg \"dct 频率系数属性\")\n\n在一张图像中，**大多数能量**会集中在[低频部分](https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm)，所以如果我们将图像转换成频率系数，并**丢掉高频系数**，我们就能**减少描述图像所需的数据量**，而不会牺牲太多的图像质量。\n> 频率是指信号变化的速度。\n\n让我们通过实验学习这点，我们将使用 DCT 把原始图像转换为频率（系数块），然后丢掉最不重要的系数。\n\n首先，我们将它转换为其**频域**。\n\n![系数值](i/dct_coefficient_values.png \"系数值\")\n\n然后我们丢弃部分（67%）系数，主要是它的右下角部分。\n\n![系数清零](i/dct_coefficient_zeroed.png \"系数清零\")\n\n然后我们从丢弃的系数块重构图像（记住，这需要可逆），并与原始图像相比较。\n\n![原始 vs 量化](i/original_vs_quantized.png \"原始 vs 量化\")\n\n如我们所见它酷似原始图像，但它引入了许多与原来的不同，我们**丢弃了67.1875%**，但我们仍然得到至少类似于原来的东西。我们可以更加智能的丢弃系数去得到更好的图像质量，但这是下一个主题。\n\n> ### 使用全部像素形成每个系数\n>  \n> 重要的是要注意，每个系数并不直接映射到单个像素，但它是所有像素的加权和。这个神奇的图形展示了如何计算出第一和第二个系数，使用每个唯一的索引做权重。\n>\n> ![dct 计算](i/applicat.jpg \"dct 计算\")\n>\n> 来源：[https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm](https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm)\n>  \n> 你也可以尝试[通过查看在 DCT 基础上形成的简单图片来可视化 DCT](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/dct_better_explained.ipynb)。例如，这是使用每个系数权重[形成的字符 A](https://en.wikipedia.org/wiki/Discrete_cosine_transform#Example_of_IDCT)。\n>\n> ![](https://upload.wikimedia.org/wikipedia/commons/5/5e/Idct-animation.gif )\n\n<br/>\n\n> ### 自己动手：丢弃不同的系数\n> 你可以玩转 [DCT 变换](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/uniform_quantization_experience.ipynb)\n\n## 第四步 - 量化\n\n当我们丢弃一些系数时，在最后一步（变换），我们做了一些形式的量化。这一步，我们选择性地剔除信息（**有损部分**）或者简单来说，我们将**量化系数以实现压缩**。\n\n我们如何量化一个系数块？一个简单的方法是均匀量化，我们取一个块并**将其除以单个的值**（10），并舍入值。\n\n![量化](i/quantize.png \"量化\")\n\n我们如何**逆转**（重新量化）这个系数块？我们可以通过**乘以我们先前除以的相同的值**（10）来做到。\n\n![逆转量化](i/re-quantize.png \"逆转量化\")\n\n这**不是最好的方法**，因为它没有考虑到每个系数的重要性，我们可以使用一个**量化矩阵**来代替单个值，这个矩阵可以利用 DCT 的属性，多量化右下部，而少（量化）左上部，[JPEG 使用了类似的方法](https://www.hdm-stuttgart.de/~maucher/Python/MMCodecs/html/jpegUpToQuant.html)，你可以通过[查看源码看看这个矩阵](https://github.com/google/guetzli/blob/master/guetzli/jpeg_data.h#L40)。\n\n> ### 自己动手：量化\n> 你可以玩转[量化](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/dct_experiences.ipynb)\n\n## 第五步 - 熵编码\n\n在我们量化数据（图像块／切片／帧）之后，我们仍然可以以无损的方式来压缩它。有许多方法（算法）可用来压缩数据。我们将简单体验其中几个，你可以阅读这本很棒的书去深入理解：[Understanding Compression: Data Compression for Modern Developers](https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/)。\n\n### VLC 编码：\n\n让我们假设我们有一个符号流：**a**, **e**, **r** 和 **t**，它们的概率（从0到1）由下表所示。\n\n|     | a   | e   | r    | t   |\n|-----|-----|-----|------|-----|\n| 概率 | 0.3 | 0.3 | 0.2 |  0.2 |\n\n\n我们可以分配不同的二进制码，（最好是）小的码给最可能（出现的字符），大些的码给最少可能（出现的字符）。\n\n|        | a   | e   | r    | t   |\n|--------|-----|-----|------|-----|\n|   概率  | 0.3 | 0.3 | 0.2 | 0.2 |\n| 二进制码 | 0 | 10 | 110 | 1110 |\n\n\n让我们压缩 **eat** 流，假设我们为每个字符花费 8 bit，在没有做任何压缩时我们将花费 **24 bit**。但是在这种情况下，我们使用各自的代码来替换每个字符，我们就能节省空间。\n\n第一步是编码字符 **e** 为 `10`，第二个字符是 **a**，追加（不是数学加法）后是 `[10][0]`，最后是第三个字符 **t**，最终组成已压缩的比特流 `[10][0][1110]` 或 `1001110`，这只需 **7 bit**（比原来的空间少 3.4 倍）。\n\n请注意每个代码必须是唯一的前缀码，[Huffman 能帮你找到这些数字](https://en.wikipedia.org/wiki/Huffman_coding)。虽然它有一些问题，但是[视频编解码器仍然提供该方法](https://en.wikipedia.org/wiki/Context-adaptive_variable-length_coding)，它也是很多应用程序的压缩算法。\n\n编码器和解码器都**必须知道**这个（包含编码的）字符表，因此，你也需要传送这个表。\n\n### 算术编码\n\n让我们假设我们有一个符号流：**a**, **e**, **r**, **s** 和 **t**，它们的概率由下表所示。\n\n|     | a   | e   | r    | s    | t   |\n|-----|-----|-----|------|------|-----|\n| 概率 | 0.3 | 0.3 | 0.15 | 0.05 | 0.2 |\n\n\n考虑到这个表，我们可以构建一个区间，区间包含了所有可能的字符，字符按出现概率排序。\n\n![初始算法区间](i/range.png \"初始算法区间\")\n\n让我们编码 **eat** 流，我们选择第一个字符 **e** 位于 **0.3 到 0.6** （但不包括 0.6）的子区间，我们选择这个子区间，按照之前同等的比例再次分割。\n\n![第二个子区间](i/second_subrange.png \"第二个子区间\")\n\n让我们继续编码我们的流 **eat**，现在使第二个 **a** 字符位于 **0.3 到 0.39** 的区间里，接着再次用同样的方法编码最后的字符 **t**，得到最后的子区间 **0.354 到 0.372**。\n\n![最终算法区间](i/arithimetic_range.png \"最终算法区间\")\n\n我们只需从最后的子区间 0.354 到 0.372 里选择一个数，让我们选择 0.36，不过我们可以选择这个子区间里的任何数。仅靠这个数，我们将可以恢复原始流 **eat**。就像我们在区间的区间里画了一根线来编码我们的流。\n\n![最终区间横断面](i/range_show.png \"最终区间横断面\")\n\n**反向过程**（又名解码）一样简单，用数字 **0.36** 和我们原始区间，我们可以进行同样的操作，不过现在是使用这个数字来还原被编码的流。\n\n在第一个区间，我们发现数字落入了一个子区间，因此，这个子区间是我们的第一个字符，现在我们再次切分这个子区间，像之前一样做同样的过程。我们会注意到 **0.36** 落入了 **a** 的区间，然后我们重复这一过程直到得到最后一个字符 **t**（形成我们原始编码过的流 eat）。\n\n编码器和解码器都**必须知道**字符概率表，因此，你也需要传送这个表。\n\n非常巧妙，不是吗？人们能想出这样的解决方案实在是太聪明了，一些[视频编解码器使用](https://en.wikipedia.org/wiki/Context-adaptive_binary_arithmetic_coding)这项技术（或至少提供这一选择）。\n\n关于无损压缩量化比特流的办法，这篇文章无疑缺少了很多细节、原因、权衡等等。作为一个开发者你[应该学习更多](https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/)。刚入门视频编码的人可以尝试使用不同的[熵编码算法，如ANS](https://en.wikipedia.org/wiki/Asymmetric_Numeral_Systems)。\n\n> ### 自己动手：CABAC vs CAVLC\n> 你可以[生成两个流，一个使用 CABAC，另一个使用 CAVLC](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#cabac-vs-cavlc)，并比较生成每一个的时间以及最终的大小。\n\n## 第六步 - 比特流格式\n\n完成所有这些步之后，我们需要将**压缩过的帧和内容打包进去**。需要明确告知解码器**编码定义**，如颜色深度，颜色空间，分辨率，预测信息（运动向量，帧内预测方向），配置<sup>\\*</sup>，层级<sup>\\*</sup>，帧率，帧类型，帧号等等更多信息。\n> <sup>*</sup> 译注：原文为 profile 和 level，没有通用的译名\n\n我们将简单地学习 H.264 比特流。第一步是[生成一个小的 H.264<sup>\\*</sup> 比特流](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#generate-a-single-frame-h264-bitstream)，可以使用本 repo 和 [ffmpeg](http://ffmpeg.org/) 来做。\n\n```\n./s/ffmpeg -i /files/i/minimal.png -pix_fmt yuv420p /files/v/minimal_yuv420.h264\n```\n\n> <sup>*</sup> ffmpeg 默认将所有参数添加为 **SEI NAL**，很快我们会定义什么是 NAL。\n\n这个命令会使用下面的图片作为帧，生成一个具有**单个帧**，64x64 和颜色空间为 yuv420 的原始 h264 比特流。\n> ![使用帧来生成极简 h264 比特流](i/minimal.png \"使用帧来生成极简 h264 比特流\")\n\n### H.264 比特流\n\nAVC (H.264) 标准规定信息将在宏帧（网络概念上的）内传输，称为 [NAL](https://en.wikipedia.org/wiki/Network_Abstraction_Layer)（网络抽象层）。NAL 的主要目标是提供“网络友好”的视频呈现方式，该标准必须适用于电视（基于流），互联网（基于数据包）等。\n\n![H.264 NAL 单元](i/nal_units.png \"H.264 NAL 单元\")\n\n[同步标记](https://en.wikipedia.org/wiki/Frame_synchronization)用来定义 NAL 单元的边界。每个同步标记的值固定为  `0x00 0x00 0x01` ，最开头的标记例外，它的值是  `0x00 0x00 0x00 0x01` 。如果我们在生成的 h264 比特流上运行 **hexdump**，我们可以在文件的开头识别至少三个 NAL。\n\n![NAL 单元上的同步标记](i/minimal_yuv420_hex.png \"NAL 单元上的同步标记\")\n\n我们之前说过，解码器需要知道不仅仅是图片数据，还有视频的详细信息，如：帧、颜色、使用的参数等。每个 NAL 的**第一位**定义了其分类和**类型**。\n\n| NAL type id  | 描述  |\n|---  |---|\n| 0  |  Undefined |\n| 1  |  Coded slice of a non-IDR picture |\n| 2  |  Coded slice data partition A |\n| 3  |  Coded slice data partition B |\n| 4  |  Coded slice data partition C |\n| 5  |  **IDR** Coded slice of an IDR picture |\n| 6  |  **SEI** Supplemental enhancement information |\n| 7  |  **SPS** Sequence parameter set |\n| 8  |  **PPS** Picture parameter set |\n| 9  |  Access unit delimiter |\n| 10 |  End of sequence |\n| 11 |  End of stream |\n| ... |  ... |\n\n通常，比特流的第一个 NAL 是 **SPS**，这个类型的 NAL 负责传达通用编码参数，如**配置，层级，分辨率**等。\n\n如果我们跳过第一个同步标记，就可以通过解码**第一个字节**来了解第一个 **NAL 的类型**。\n\n例如同步标记之后的第一个字节是 `01100111`，第一位（`0`）是 **forbidden_zero_bit** 字段，接下来的两位（`11`）告诉我们是 **nal_ref_idc** 字段，其表示该 NAL 是否是参考字段，其余 5 位（`00111`）告诉我们是 **nal_unit_type** 字段，在这个例子里是 NAL 单元 **SPS** (7)。\n\nSPS NAL 的第 2 位 (`binary=01100100, hex=0x64, dec=100`) 是 **profile_idc** 字段，显示编码器使用的配置，在这个例子里，我们使用[受限高配置](https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Profiles)，一种没有 B（双向预测） 切片支持的高配置。\n\n![SPS 二进制视图](i/minimal_yuv420_bin.png \"SPS 二进制视图\")\n\n当我们阅读 SPS NAL 的 H.264 比特流规范时，会为**参数名称**，**分类**和**描述**找到许多值，例如，看看字段 `pic_width_in_mbs_minus_1` 和 `pic_height_in_map_units_minus_1`。\n\n| 参数名称  | 分类  |  描述  |\n|---  |---|---|\n| pic_width_in_mbs_minus_1 |  0 | ue(v) |\n| pic_height_in_map_units_minus_1 |  0 | ue(v) |\n\n> **ue(v)**: 无符号整形 [Exp-Golomb-coded](https://pythonhosted.org/bitstring/exp-golomb.html)\n\n如果我们对这些字段的值进行一些计算，将最终得出**分辨率**。我们可以使用值为 `119（ (119 + 1) * macroblock_size = 120 * 16 = 1920）`的 `pic_width_in_mbs_minus_1` 表示 `1920 x 1080`，再次为了减少空间，我们使用 `119` 来代替编码 `1920`。\n\n如果我们再次使用二进制视图检查我们创建的视频 (ex: `xxd -b -c 11 v/minimal_yuv420.h264`)，可以跳到帧自身上一个 NAL。\n\n![h264 idr 切片头](i/slice_nal_idr_bin.png \"h264 idr 切片头\")\n\n我们可以看到最开始的 6 个字节：`01100101 10001000 10000100 00000000 00100001 11111111`。我们已经知道第一个字节告诉我们 NAL 的类型，在这个例子里， (`00101`) 是 **IDR 切片 (5)**，可以进一步检查它：\n\n![h264 切片头规格](i/slice_header.png \"h264 切片头规格\")\n\n对照规范，我们能解码切片的类型（**slice_type**），帧号（**frame_num**）等重要字段。\n\n为了获得一些字段（`ue(v), me(v), se(v) 或 te(v)`）的值，我们需要称为 [Exponential-Golomb](https://pythonhosted.org/bitstring/exp-golomb.html) 的特定解码器来解码它。当存在很多默认值时，这个方法编码变量值特别高效。\n\n> 这个视频里 **slice_type** 和 **frame_num** 的值是 7（I 切片）和 0（第一帧）。\n\n我们可以将**比特流视为一个协议**，如果你想学习更多关于比特流的内容，请参考 [ITU H.264 规范](http://www.itu.int/rec/T-REC-H.264-201610-I)。这个宏观图展示了图片数据（压缩过的 YUV）所在的位置。\n\n![h264 比特流宏观图](i/h264_bitstream_macro_diagram.png \"h264 比特流宏观图\")\n\n我们可以探究其它比特流，如 [VP9 比特流](https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf)，[H.265（HEVC）](http://handle.itu.int/11.1002/1000/11885-en?locatt=format:pdf)或是我们的新朋友 [AV1 比特流](https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8)，[他们很相似吗？不](http://www.gpac-licensing.com/2016/07/12/vp9-av1-bitstream-format/)，但只要学习了其中之一，学习其他的就简单多了。\n\n> ### 自己动手：检查 H.264 比特流\n> \n> 我们可以[生成一个单帧视频](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#generate-a-single-frame-video)，使用 [mediainfo](https://en.wikipedia.org/wiki/MediaInfo) 检查它的 H.264 比特流。事实上，你甚至可以查看[解析 h264(AVC) 视频流的源代码](https://github.com/MediaArea/MediaInfoLib/blob/master/Source/MediaInfo/Video/File_Avc.cpp)。\n>\n> ![mediainfo h264 比特流的详情 ](i/mediainfo_details_1.png \"mediainfo h264 比特流的详情\")\n>  \n> 我们也可使用 [Intel® Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer)，需要付费，但也有只能查看前 10 帧的免费试用版，这已经够达成学习目的了。\n>\n> ![Intel® Video Pro Analyzer h264 比特流的详情](i/intel-video-pro-analyzer.png \"Intel® Video Pro Analyzer h264 比特流的详情\")\n\n## 回顾\n\n我们可以看到我们学了许多**使用相同模型的现代编解码器**。事实上，让我们看看 Thor 视频编解码器框图，它包含所有我们学过的步骤。你现在应该能更好地理解数字视频领域内的创新和论文。\n![thor 编解码器块图](i/thor_codec_block_diagram.png \"thor 编解码器块图\")\n\n之前我们计算过我们[需要 139GB 来保存一个一小时，720p 分辨率和30fps的视频文件](#色度子采样)，如果我们使用在这里学过的技术，如**帧间和帧内预测，转换，量化，熵编码和其它**我们能实现——假设我们**每像素花费 0.031 bit**——同样观感质量的视频，**对比 139GB 的存储，只需 367.82MB**。\n> 我们根据这里提供的示例视频选择**每像素使用 0.031 bit**。\n\n## H.265 如何实现比 H.264 更好的压缩率\n\n我们已经更多地了解了编解码器的工作原理，那么就容易理解新的编解码器如何使用更少的数据量传输更高分辨率的视频。\n\n我们将比较 AVC 和 HEVC，要记住的是：我们几乎总是要在压缩率和更多的 CPU 周期（复杂度）之间作权衡。\n\nHEVC 比 AVC 有更大和更多的**分区**（和**子分区**）选项，更多**帧内预测方向**，**改进的熵编码**等，所有这些改进使得 H.265 比 H.264 的压缩率提升 50%。\n\n![h264 vs h265](i/avc_vs_hevc.png \"H.264 vs H.265\")\n\n# 在线流媒体\n## 通用架构\n\n![general_architecture](i/general_architecture.png)\n\n[TODO]\n\n## 渐进式下载和自适应流\n\n![progressive_download](i/progressive_download.png)\n\n![adaptive_streaming](i/adaptive_streaming.png)\n\n[TODO]\n\n## 内容保护\n\n我们可以用一个简单的令牌认证系统来保护视频。用户需要拥有一个有效的令牌才可以播放视频，CDN 会拒绝没有令牌的用户的请求。它与大多数网站的身份认证系统非常相似。\n\n![token_protection](i/token_protection.png)\n\n仅仅使用令牌认证系统，用户仍然可以下载并重新分发视频。DRM 系统可以用来避免这种情况。\n\n![drm](i/drm.png)\n\n实际情况下，人们通常同时使用这两种技术提供授权和认证。\n\n### DRM\n#### 主要系统\n\n* FPS - [**FairPlay Streaming**](https://developer.apple.com/streaming/fps/)\n* PR - [**PlayReady**](https://www.microsoft.com/playready/)\n* WV - [**Widevine**](http://www.widevine.com/)\n\n#### 是什么\n\nDRM 指的是数字版权管理，是一种**为数字媒体提供版权保护**的方法，例如数字视频和音频。尽管用在了很多场合，但它并[没有被普遍接受](https://en.wikipedia.org/wiki/Digital_rights_management#DRM-free_works).\n\n#### 为什么\n\n内容的创作者（大多是工作室/制片厂）希望保护他们的知识产权，使他们的数字媒体免遭未经授权的分发。\n\n#### 怎么做\n\n我们将用一种简单的、抽象的方式描述 DRM\n\n现有一份**内容 C1**（如 HLS 或 DASH 视频流），一个**播放器 P1**（如 shaka-clappr, exo-player 或 iOS），装在**设备 D1**（如智能手机、电视或台式机/笔记本）上，使用 **DRM 系统 DRM1**（如 FairPlay Streaming, PlayReady, Widevine）\n\n内容 C1 由 DRM1 用一个**对称密钥 K1** 加密，生成**加密内容 C'1**\n\n![DRM 一般流程](i/drm_general_flow.jpeg \"DRM 一般流程\")\n\n设备 D1 上的播放器 P1 有一个非对称密钥对，密钥对包含一个**私钥 PRK1**（这个密钥是受保护的<sup>1</sup>，只有 **D1** 知道密钥内容），和一个**公钥 PUK1**\n\n> **<sup>1</sup>受保护的**: 这种保护可以**通过硬件**进行保护，例如, 将这个密钥存储在一个特殊的芯片（只读）中，芯片的工作方式就像一个用来解密的[黑箱]。 或**通过软件**进行保护（较低的安全系数）。DRM 系统提供了识别设备所使用的保护类型的方法。\n\n当 **播放器 P1 希望播放****加密内容 C'1** 时，它需要与 **DRM1** 协商，将公钥 **PUK1** 发送给 DRM1, DRM1 会返回一个被公钥 **PUK1** **加密过的 K1**。按照推论，结果就是**只有 D1 能够解密**。\n\n`K1P1D1 = enc(K1, PUK1)`\n\n**P1** 使用它的本地 DRM 系统（这可以使用 [SoC](https://zh.wikipedia.org/wiki/系统芯片) ，一个专门的硬件和软件，这个系统可以使用它的私钥 PRK1 用来**解密**内容，它可以解密被加密过的**K1P1D1 的对称密钥 K1**。理想情况下，密钥不会被导出到内存以外的地方。\n\n```\n K1 = dec(K1P1D1, PRK1)\n\n P1.play(dec(C'1, K1))\n```\n\n![DRM 解码流程](i/drm_decoder_flow.jpeg \"DRM 解码流程\")\n\n# 如何使用 jupyter\n\n确保你已安装 docker，只需运行 `./s/start_jupyter.sh`，然后按照控制台的说明进行操作。\n\n# 会议\n\n*\t[DEMUXED](https://demuxed.com/) - 您可以[查看最近的2个活动演示](https://www.youtube.com/channel/UCIc_DkRxo9UgUSTvWVNCmpA)。\n\n# 参考\n\n这里有最丰富的资源，这篇文档包含的信息，均摘录、依据或受它们启发。你可以用这些精彩的链接，书籍，视频等深化你的知识。\n\n在线课程和教程：\n\n* [https://www.coursera.org/learn/digital/](https://www.coursera.org/learn/digital/)\n* [https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf](https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf)\n* [https://xiph.org/video/vid1.shtml](https://xiph.org/video/vid1.shtml)\n* [https://xiph.org/video/vid2.shtml](https://xiph.org/video/vid2.shtml)\n* [http://slhck.info/ffmpeg-encoding-course](http://slhck.info/ffmpeg-encoding-course)\n* [http://www.cambridgeincolour.com/tutorials/camera-sensors.htm](http://www.cambridgeincolour.com/tutorials/camera-sensors.htm)\n* [http://www.slideshare.net/vcodex/a-short-history-of-video-coding](http://www.slideshare.net/vcodex/a-short-history-of-video-coding)\n* [http://www.slideshare.net/vcodex/introduction-to-video-compression-1339433](http://www.slideshare.net/vcodex/introduction-to-video-compression-13394338)\n* [https://developer.android.com/guide/topics/media/media-formats.html](https://developer.android.com/guide/topics/media/media-formats.html)\n* [http://www.slideshare.net/MadhawaKasun/audio-compression-23398426](http://www.slideshare.net/MadhawaKasun/audio-compression-23398426)\n* [http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf](http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf)\n\n书籍:\n\n* [https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&ie=UTF8&qid=1486395327&sr=1-1](https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&ie=UTF8&qid=1486395327&sr=1-1)\n* [https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925](https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925)\n* [https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&ie=UTF8&qid=1486396914&sr=1-3&keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO](https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&ie=UTF8&qid=1486396914&sr=1-3&keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO)\n* [https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&ie=UTF8&qid=1486396940&sr=1-1&keywords=jan+ozer](https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&ie=UTF8&qid=1486396940&sr=1-1&keywords=jan+ozer)\n\n比特流规范:\n\n* [http://www.itu.int/rec/T-REC-H.264-201610-I](http://www.itu.int/rec/T-REC-H.264-201610-I)\n* [http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&lang=en](http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&lang=en)\n* [https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf](https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf)\n* [http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf](http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf)\n* [http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243](http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243)\n* [http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html](http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html)\n\n软件:\n\n* [https://ffmpeg.org/](https://ffmpeg.org/)\n* [https://ffmpeg.org/ffmpeg-all.html](https://ffmpeg.org/ffmpeg-all.html)\n* [https://ffmpeg.org/ffprobe.html](https://ffmpeg.org/ffprobe.html)\n* [https://trac.ffmpeg.org/wiki/](https://trac.ffmpeg.org/wiki/)\n* [https://software.intel.com/en-us/intel-video-pro-analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer)\n* [https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8](https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8)\n\n非-ITU 编解码器:\n\n* [https://aomedia.googlesource.com/](https://aomedia.googlesource.com/)\n* [https://github.com/webmproject/libvpx/tree/master/vp9](https://github.com/webmproject/libvpx/tree/master/vp9)\n* [https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml](https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml)\n* [https://people.xiph.org/~jm/daala/revisiting/](https://people.xiph.org/~jm/daala/revisiting/)\n* [https://www.youtube.com/watch?v=lzPaldsmJbk](https://www.youtube.com/watch?v=lzPaldsmJbk)\n* [https://fosdem.org/2017/schedule/event/om_av1/](https://fosdem.org/2017/schedule/event/om_av1/)\n\n编码概念:\n\n* [http://x265.org/hevc-h265/](http://x265.org/hevc-h265/)\n* [http://slhck.info/video/2017/03/01/rate-control.html](http://slhck.info/video/2017/03/01/rate-control.html)\n* [http://slhck.info/video/2017/02/24/vbr-settings.html](http://slhck.info/video/2017/02/24/vbr-settings.html)\n* [http://slhck.info/video/2017/02/24/crf-guide.html](http://slhck.info/video/2017/02/24/crf-guide.html)\n* [https://arxiv.org/pdf/1702.00817v1.pdf](https://arxiv.org/pdf/1702.00817v1.pdf)\n* [https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors](https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors)\n* [http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html](http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html)\n* [http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html](http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html)\n* [https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/](https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/)\n\n测试用视频序列:\n\n* [http://bbb3d.renderfarming.net/download.html](http://bbb3d.renderfarming.net/download.html)\n* [https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx](https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx)\n\n杂项:\n\n* [http://stackoverflow.com/a/24890903](http://stackoverflow.com/a/24890903)\n* [http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264](http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264)\n* [http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html](http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html)\n* [http://vanseodesign.com/web-design/color-luminance/](http://vanseodesign.com/web-design/color-luminance/)\n* [http://www.biologymad.com/nervoussystem/eyenotes.htm](http://www.biologymad.com/nervoussystem/eyenotes.htm)\n* [http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf](http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf)\n* [http://www.csc.villanova.edu/~rschumey/csc4800/dct.html](http://www.csc.villanova.edu/~rschumey/csc4800/dct.html)\n* [http://www.explainthatstuff.com/digitalcameras.html](http://www.explainthatstuff.com/digitalcameras.html)\n* [http://www.hkvstar.com](http://www.hkvstar.com)\n* [http://www.hometheatersound.com/](http://www.hometheatersound.com/)\n* [http://www.lighterra.com/papers/videoencodingh264/](http://www.lighterra.com/papers/videoencodingh264/)\n* [http://www.red.com/learn/red-101/video-chroma-subsampling](http://www.red.com/learn/red-101/video-chroma-subsampling)\n* [http://www.slideshare.net/ManoharKuse/hevc-intra-coding](http://www.slideshare.net/ManoharKuse/hevc-intra-coding)\n* [http://www.slideshare.net/mwalendo/h264vs-hevc](http://www.slideshare.net/mwalendo/h264vs-hevc)\n* [http://www.slideshare.net/rvarun7777/final-seminar-46117193](http://www.slideshare.net/rvarun7777/final-seminar-46117193)\n* [http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf](http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf)\n* [http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx](http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx)\n* [http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&PageNum=1](http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&PageNum=1)\n* [http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/](http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/)\n* [https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/](https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/)\n* [https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/](https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/)\n* [https://codesequoia.wordpress.com/category/video/](https://codesequoia.wordpress.com/category/video/)\n* [https://developer.apple.com/library/content/technotes/tn2224/_index.html](https://developer.apple.com/library/content/technotes/tn2224/_index.html)\n* [https://en.wikibooks.org/wiki/MeGUI/x264_Settings](https://en.wikibooks.org/wiki/MeGUI/x264_Settings)\n* [https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming](https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming)\n* [https://en.wikipedia.org/wiki/AOMedia_Video_1](https://en.wikipedia.org/wiki/AOMedia_Video_1)\n* [https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg](https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg)\n* [https://en.wikipedia.org/wiki/Cone_cell](https://en.wikipedia.org/wiki/Cone_cell)\n* [https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg](https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg)\n* [https://en.wikipedia.org/wiki/Inter_frame](https://en.wikipedia.org/wiki/Inter_frame)\n* [https://en.wikipedia.org/wiki/Intra-frame_coding](https://en.wikipedia.org/wiki/Intra-frame_coding)\n* [https://en.wikipedia.org/wiki/Photoreceptor_cell](https://en.wikipedia.org/wiki/Photoreceptor_cell)\n* [https://en.wikipedia.org/wiki/Pixel_aspect_ratio](https://en.wikipedia.org/wiki/Pixel_aspect_ratio)\n* [https://en.wikipedia.org/wiki/Presentation_timestamp](https://en.wikipedia.org/wiki/Presentation_timestamp)\n* [https://en.wikipedia.org/wiki/Rod_cell](https://en.wikipedia.org/wiki/Rod_cell)\n* [https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg](https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg)\n* [https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/](https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/)\n* [https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping](https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping)\n* [https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/](https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/)\n* [https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03](https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03)\n* [https://www.encoding.com/android/](https://www.encoding.com/android/)\n* [https://www.encoding.com/http-live-streaming-hls/](https://www.encoding.com/http-live-streaming-hls/)\n* [https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm](https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm)\n* [https://www.lifewire.com/cmos-image-sensor-493271](https://www.lifewire.com/cmos-image-sensor-493271)\n* [https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ](https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ)\n* [https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar](https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar)\n* [https://www.vcodex.com/h264avc-intra-precition/](https://www.vcodex.com/h264avc-intra-precition/)\n* [https://www.youtube.com/watch?v=9vgtJJ2wwMA](https://www.youtube.com/watch?v=9vgtJJ2wwMA)\n* [https://www.youtube.com/watch?v=LFXN9PiOGtY](https://www.youtube.com/watch?v=LFXN9PiOGtY)\n* [https://www.youtube.com/watch?v=Lto-ajuqW3w&list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6](https://www.youtube.com/watch?v=Lto-ajuqW3w&list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6)\n* [https://www.youtube.com/watch?v=LWxu4rkZBLw](https://www.youtube.com/watch?v=LWxu4rkZBLw)\n\n","source":"_posts/Introduction-to-digital-video-technology.md","raw":"---\ntitle: 数字视频技术导论\nreward: false\ntop: false\ndate: 2020-02-28 08:49:46\ncategories: 视频技术\ntags: \n  - 数字视频\n---\n\n# 背景\n因为工作关系需要了解数字视频相关技术，在学习的过程中找到了这份托管在github上的**数字视频导论**的材料。\n\n这份材料介绍了基本的数字视频相关技术，言简意赅但又不枯燥无味，即有理论又有丰富的实践操作，在我学习的所有材料中算是比较上乘的材料。\n\n基于如上的原因，将该材料的相关内容转载到此处。大家可以直接访问该材料的github仓库[**digital_video_introduction**](https://github.com/leandromoreira/digital_video_introduction)获取相关内容。如下的所有内容皆来自[**digital_video_introduction**](https://github.com/leandromoreira/digital_video_introduction)，特此标注。\n\n[![license](https://img.shields.io/badge/license-BSD--3--Clause-blue.svg)](https://img.shields.io/badge/license-BSD--3--Clause-blue.svg)\n\n<!--more-->\n\n# 介绍\n这是一份循序渐进的视频技术的介绍。尽管它面向的是软件开发人员/工程师，但我们希望**对任何人而言**，这份文档都能简单易学。这个点子产生于一个[视频技术新手小型研讨会](https://docs.google.com/presentation/d/17Z31kEkl_NGJ0M66reqr9_uTG6tI5EDDVXpdPKVuIrs/edit#slide=id.p)期间。\n\n本文档旨在尽可能使用**浅显的词语，丰富的图像和实际例子**介绍数字视频概念，使这些知识能适用于各种场合。你可以随时反馈意见或建议，以改进这篇文档。\n\n# 目录\n\n- [介绍](#介绍)\n- [目录](#目录)\n- [基本术语](#基本术语)\n  * [编码彩色图像的其它方法](#编码彩色图像的其它方法)\n  * [自己动手：玩转图像和颜色](#自己动手：玩转图像和颜色)\n  * [DVD 的 DAR 是 4:3](#DVD-的-DAR-是-4-3)\n  * [自己动手：检查视频属性](#自己动手：检查视频属性)\n- [消除冗余](#消除冗余)\n  * [颜色，亮度和我们的眼睛](#颜色，亮度和我们的眼睛)\n    + [颜色模型](#颜色模型)\n    + [YCbCr 和 RGB 之间的转换](#YCbCr-和-RGB-之间的转换)\n    + [色度子采样](#色度子采样)\n    + [自己动手：检查 YCbCr 直方图](#自己动手：检查-YCbCr-直方图)\n  * [帧类型](#帧类型)\n    + [I 帧（内部，关键帧）](#I-帧（帧内编码，关键帧）)\n    + [P 帧（预测）](#P-帧（预测）)\n      - [自己动手：具有单个 I 帧的视频](#自己动手：具有单个-I-帧的视频)\n    + [B 帧（双向预测）](#B-帧（双向预测）)\n      - [自己动手：使用 B 帧比较视频](#自己动手：使用-B-帧比较视频)\n    + [小结](#小结)\n  * [时间冗余（帧间预测）](#时间冗余（帧间预测）)\n      - [自己动手：查看运动向量](#自己动手：查看运动向量)\n  * [空间冗余（帧内预测）](#空间冗余（帧内预测）)\n      - [自己动手：查看帧内预测](#自己动手：查看帧内预测)\n- [视频编解码器是如何工作的？](#视频编解码器是如何工作的？)\n  * [是什么？为什么？怎么做？](#是什么？为什么？怎么做？)\n  * [历史](#历史)\n    + [AV1 的诞生](#AV1-的诞生)\n  * [通用编解码器](#通用编解码器)\n  * [第一步 - 图片分区](#第一步-图片分区)\n    + [自己动手：查看分区](#自己动手：查看分区)\n  * [第二步 - 预测](#第二步-预测)\n  * [第三步 - 转换](#第三步-转换)\n    + [自己动手：丢弃不同的系数](#自己动手：丢弃不同的系数)\n  * [第四步 - 量化](#第四步-量化)\n    + [自己动手：量化](#自己动手：量化)\n  * [第五步 - 熵编码](#第五步-熵编码)\n    + [VLC 编码](#VLC-编码：)\n    + [算术编码](#算术编码)\n    + [自己动手：CABAC vs CAVLC](#自己动手：CABAC-vs-CAVLC)\n  * [第六步 - 比特流格式](#第六步-比特流格式)\n    + [H.264 比特流](#H-264-比特流)\n    + [自己动手：检查 H.264 比特流](#自己动手：检查-H-264-比特流)\n  * [回顾](#回顾)\n  * [H.265 如何实现比 H.264 更好的压缩率?](#H-265-如何实现比-H-264-更好的压缩率)\n- [在线流媒体](#在线流媒体)\n  * [通用架构](#通用架构)\n  * [渐进式下载和自适应流](#渐进式下载和自适应流)\n  * [内容保护](#内容保护)\n- [如何使用 jupyter](#如何使用-jupyter)\n- [会议](#会议)\n- [参考](#参考)\n\n# 基本术语\n\n一个**图像**可以视作一个**二维矩阵**。如果将**色彩**考虑进来，我们可以做出推广：将这个图像视作一个**三维矩阵**——多出来的维度用于储存色彩信息。\n\n如果我们选择三原色（红、绿、蓝）代表这些色彩，这就定义了三个平面：第一个是红色平面，第二个是绿色平面，最后一个是蓝色平面。\n\n![an image is a 3d matrix RGB](i/image_3d_matrix_rgb.png \"An image is a 3D matrix\")\n\n我们把这个矩阵里的每一个点称为**像素**（图像元素）。像素的色彩由三原色的**强度**（通常用数值表示）表示。例如，一个**红色像素**是指强度为 0 的绿色，强度为 0 的蓝色和强度最大的红色。**粉色像素**可以通过三种颜色的组合表示。如果规定强度的取值范围是 0 到 255，**红色 255、绿色 192、蓝色 203** 则表示粉色。\n\n> ### 编码彩色图像的其它方法\n>\n> 还有许多其它模型也可以用来表示色彩，进而组成图像。例如，给每种颜色都标上序号（如下图），这样每个像素仅需一个字节就可以表示出来，而不是 RGB 模型通常所需的 3 个。在这样一个模型里我们可以用一个二维矩阵来代替三维矩阵去表示我们的色彩，这将节省存储空间，但色彩的数量将会受限。\n>\n> ![NES palette](i/nes-color-palette.png \"NES palette\")\n\n\n例如以下几张图片。第一张包含所有颜色平面。剩下的分别是红、绿、蓝色平面（显示为灰调）（译注：颜色强度高的地方显示为亮色，强度低为暗色）。\n\n\n![RGB channels intensity](i/rgb_channels_intensity.png \"RGB channels intensity\")\n\n我们可以看到，对于最终的成像，红色平面对强度的贡献更多（三个平面最亮的是红色平面），蓝色平面（最后一张图片）的贡献大多只在马里奥的眼睛和他衣服的一部分。所有颜色平面对马里奥的胡子（最暗的部分）均贡献较少。\n\n存储颜色的强度，需要占用一定大小的数据空间，这个大小被称为颜色深度。假如每个颜色（平面）的强度占用 8 bit（取值范围为 0 到 255），那么颜色深度就是 24（8*3）bit，我们还可以推导出我们可以使用 2 的 24 次方种不同的颜色。\n\n> 很棒的学习材料：[现实世界的照片是如何拍摄成 0 和 1 的](http://www.cambridgeincolour.com/tutorials/camera-sensors.htm)。\n\n图片的另一个属性是**分辨率**，即一个平面内像素的数量。通常表示成宽*高，例如下面这张 **4x4** 的图片。\n\n![image resolution](i/resolution.png \"image resolution\")\n\n> ### 自己动手：玩转图像和颜色\n>\n> 你可以使用 [jupyter](#如何使用-jupyter)（python, numpy, matplotlib 等等）[玩转图像](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/image_as_3d_array.ipynb)。\n>\n> 你也可以学习[图像滤镜（边缘检测，磨皮，模糊。。。）的原理](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/filters_are_easy.ipynb)。\n\n图像或视频还有一个属性是宽高比，它简单地描述了图像或像素的宽度和高度之间的比例关系。\n\n当人们说这个电影或照片是 16:9 时，通常是指显示宽高比（DAR），然而我们也可以有不同形状的单个像素，我们称为像素宽高比（PAR）。\n\n![display aspect ratio](i/DAR.png \"display aspect ratio\")\n\n![pixel aspect ratio](i/PAR.png \"pixel aspect ratio\")\n\n> ## DVD 的 DAR 是 4:3\n>\n> 虽然 DVD 的实际分辨率是 704x480，但它依然保持 4:3 的宽高比，因为它有一个 10:11（704x10／480x11）的 PAR。\n\n现在我们可以将**视频**定义为在**单位时间**内**连续的 n 帧**，这可以视作一个新的维度，n 即为帧率，若单位时间为秒，则等同于 FPS (每秒帧数 Frames Per Second)。\n\n![video](i/video.png \"video\")\n\n播放一段视频每秒所需的数据量就是它的**比特率**（即常说的码率）。\n> 比特率 = 宽 * 高 * 颜色深度 * 帧每秒\n\n例如，一段每秒 30 帧，每像素 24 bits，分辨率是 480x240 的视频，如果我们不做任何压缩，它将需要 **82,944,000 比特每秒**或 82.944 Mbps (30x480x240x24)。\n\n当**比特率**几乎恒定时称为恒定比特率（**CBR**）；但它也可以变化，称为可变比特率（**VBR**）。\n\n> 这个图形显示了一个受限的 VBR，当帧为黑色时不会花费太多的数据量。\n>\n> ![constrained vbr](i/vbr.png \"constrained vbr\")\n\n在早期，工程师们想出了一项技术能将视频的感官帧率加倍而**没有消耗额外带宽**。这项技术被称为**隔行扫描**；总的来说，它在一个时间点发送一个画面——画面用于填充屏幕的一半，而下一个时间点发送的画面用于填充屏幕的另一半。\n\n如今的屏幕渲染大多使用**逐行扫描技术**。这是一种显示、存储、传输运动图像的方法，每帧中的所有行都会被依次绘制。\n\n![interlaced vs progressive](i/interlaced_vs_progressive.png \"interlaced vs progressive\")\n\n现在我们知道了数字化**图像**的原理；它的**颜色**的编排方式；给定**帧率**和**分辨率**时，展示一个视频需要花费多少**比特率**；它是恒定的（CBR）还是可变的（VBR）；还有很多其它内容，如隔行扫描和 PAR。\n\n> ## 自己动手：检查视频属性\n> 你可以[使用 ffmpeg 或 mediainfo 检查大多数属性的解释](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#inspect-stream)。\n\n# 消除冗余\n\n我们认识到，不对视频进行压缩是不行的；**一个单独的一小时长的视频**，分辨率为 720p 和 30fps 时将**需要 278GB<sup>\\*</sup>**。仅仅使用无损数据压缩算法——如 DEFLATE（被PKZIP, Gzip, 和 PNG 使用）——也无法充分减少视频所需的带宽，我们需要找到其它压缩视频的方法。\n\n> <sup>*</sup>我们使用乘积得出这个数字 1280 x 720 x 24 x 30 x 3600 （宽，高，每像素比特数，fps 和秒数）\n\n为此，我们可以**利用视觉特性**：和区分颜色相比，我们区分亮度要更加敏锐。**时间上的重复**：一段视频包含很多只有一点小小改变的图像。**图像内的重复**：每一帧也包含很多颜色相同或相似的区域。\n\n## 颜色，亮度和我们的眼睛\n\n我们的眼睛[对亮度比对颜色更敏感](http://vanseodesign.com/web-design/color-luminance/)，你可以看看下面的图片自己测试。\n\n![luminance vs color](i/luminance_vs_color.png \"luminance vs color\")\n\n如果你看不出左图的**方块 A 和方块 B** 的颜色是**相同的**，那么好，是我们的大脑玩了一个小把戏，这让我们更多的去注意光与暗，而不是颜色。右边这里有一个使用同样颜色的连接器，那么我们（的大脑）就能轻易分辨出事实，它们是同样的颜色。\n\n> **简单解释我们的眼睛工作的原理**\n>\n> [眼睛是一个复杂的器官](http://www.biologymad.com/nervoussystem/eyenotes.htm)，有许多部分组成，但我们最感兴趣的是视锥细胞和视杆细胞。眼睛有[大约1.2亿个视杆细胞和6百万个视锥细胞](https://en.wikipedia.org/wiki/Photoreceptor_cell)。\n>\n> **简单来说**，让我们把颜色和亮度放在眼睛的功能部位上。[视杆细胞](https://en.wikipedia.org/wiki/Rod_cell)**主要负责亮度**，而[视锥细胞](https://en.wikipedia.org/wiki/Cone_cell)**负责颜色**，有三种类型的视锥，每个都有不同的颜料，叫做：[S-视锥（蓝色），M-视锥（绿色）和L-视锥（红色）](https://upload.wikimedia.org/wikipedia/commons/1/1e/Cones_SMJ2_E.svg)。\n>\n> 既然我们的视杆细胞（亮度）比视锥细胞多很多，一个合理的推断是相比颜色，我们有更好的能力去区分黑暗和光亮。\n>\n> ![eyes composition](i/eyes.jpg \"eyes composition\")\n\n一旦我们知道我们对**亮度**（图像中的亮度）更敏感，我们就可以利用它。\n\n### 颜色模型\n\n我们最开始学习的[彩色图像的原理](#基本术语)使用的是 **RGB 模型**，但也有其他模型。有一种模型将亮度（光亮）和色度（颜色）分离开，它被称为 **YCbCr**<sup>*</sup>。\n\n> <sup>*</sup> 有很多种模型做同样的分离。\n\n这个颜色模型使用 **Y** 来表示亮度，还有两种颜色通道：Cb（蓝色色度） 和 Cr（红色色度）。YCbCr 可以由 RGB 转换得来，也可以转换回 RGB。使用这个模型我们可以创建拥有完整色彩的图像，如下图。\n\n![ycbcr 例子](i/ycbcr.png \"ycbcr 例子\")\n\n### YCbCr 和 RGB 之间的转换\n\n有人可能会问，在**不使用绿色（色度）**的情况下，我们如何表现出所有的色彩？\n\n为了回答这个问题，我们将介绍从 RGB 到 YCbCr 的转换。我们将使用 [ITU-R 小组](https://en.wikipedia.org/wiki/ITU-R)*建议的[标准 BT.601](https://en.wikipedia.org/wiki/Rec._601) 中的系数。\n\n第一步是计算亮度，我们将使用 ITU 建议的常量，并替换 RGB 值。\n\n```\nY = 0.299R + 0.587G + 0.114B\n```\n\n一旦我们有了亮度后，我们就可以拆分颜色（蓝色色度和红色色度）：\n\n```\nCb = 0.564(B - Y)\nCr = 0.713(R - Y)\n```\n\n并且我们也可以使用 YCbCr 转换回来，甚至得到绿色。\n\n```\nR = Y + 1.402Cr\nB = Y + 1.772Cb\nG = Y - 0.344Cb - 0.714Cr\n```\n\n> <sup>*</sup>组织和标准在数字视频领域中很常见，它们通常定义什么是标准，例如，[什么是 4K？我们应该使用什么帧率？分辨率？颜色模型？](https://en.wikipedia.org/wiki/Rec._2020)\n\n通常，**显示屏**（监视器，电视机，屏幕等等）**仅使用 RGB 模型**，并以不同的方式来组织，看看下面这些放大效果：\n\n![pixel geometry](i/new_pixel_geometry.jpg \"pixel geometry\")\n\n### 色度子采样\n\n一旦我们能从图像中分离出亮度和色度，我们就可以利用人类视觉系统对亮度比色度更敏感的特点，选择性地剔除信息。**色度子采样**是一种编码图像时，使**色度分辨率低于亮度**的技术。\n\n![ycbcr 子采样分辨率](i/ycbcr_subsampling_resolution.png \"ycbcr 子采样分辨率\")\n\n我们应该减少多少色度分辨率呢？已经有一些模式定义了如何处理分辨率和合并（`最终的颜色 = Y + Cb + Cr`）。\n\n这些模式称为子采样系统，并被表示为 3 部分的比率 - `a:x:y`，其定义了色度平面的分辨率，与亮度平面上的、分辨率为 `a x 2` 的小块之间的关系。\n* `a` 是水平采样参考 (通常是 4)，\n* `x` 是第一行的色度样本数（相对于 a 的水平分辨率），\n* `y` 是第二行的色度样本数。\n\n> 存在的一个例外是 4:1:0，其在每个亮度平面分辨率为 4 x 4 的块内提供一个色度样本。\n\n现代编解码器中使用的常用方案是： 4:4:4 (没有子采样)**, 4:2:2, 4:1:1, 4:2:0, 4:1:0 and 3:1:1。\n\n> YCbCr 4:2:0 合并\n>\n> 这是使用 YCbCr 4:2:0 合并的一个图像的一块，注意我们每像素只花费 12bit。\n>\n> ![YCbCr 4:2:0 合并](i/ycbcr_420_merge.png \"YCbCr 4:2:0 合并\")\n\n下图是同一张图片使用几种主要的色度子采样技术进行编码，第一行图像是最终的 YCbCr，而最后一行图像展示了色度的分辨率。这么小的损失确实是一个伟大的胜利。\n\n![色度子采样例子](i/chroma_subsampling_examples.jpg \"色度子采样例子\")\n\n前面我们计算过我们需要 [278GB 去存储一个一小时长，分辨率在720p和30fps的视频文件](#消除冗余)。如果我们使用 `YCbCr 4:2:0` 我们能剪掉`一半的大小（139GB）`<sup>*</sup>，但仍然不够理想。\n> <sup>*</sup> 我们通过将宽、高、颜色深度和 fps 相乘得出这个值。前面我们需要 24 bit，现在我们只需要 12 bit。\n\n> ### 自己动手：检查 YCbCr 直方图\n> 你可以[使用 ffmpeg 检查 YCbCr 直方图](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#generates-yuv-histogram)。这个场景有更多的蓝色贡献，由[直方图](https://en.wikipedia.org/wiki/Histogram)显示。\n>\n> ![ycbcr 颜色直方图](i/yuv_histogram.png \"ycbcr 颜色直方图\")\n\n## 帧类型\n\n现在我们进一步消除`时间冗余`，但在这之前让我们来确定一些基本术语。假设我们一段 30fps 的影片，这是最开始的 4 帧。\n\n![球 1](i/smw_background_ball_1.png \"球 1\") ![球 2](i/smw_background_ball_2.png \"球 2\") ![球 3](i/smw_background_ball_3.png \"球 3\")\n![球 4](i/smw_background_ball_4.png \"球 4\")\n\n我们可以在帧内看到**很多重复内容**，如**蓝色背景**，从 0 帧到第 3 帧它都没有变化。为了解决这个问题，我们可以将它们**抽象地分类**为三种类型的帧。\n\n### I 帧（帧内编码，关键帧）\n\nI 帧（可参考，关键帧，帧内编码）是一个**自足的帧**。它不依靠任何东西来渲染，I 帧与静态图片相似。第一帧通常是 I 帧，但我们将看到 I 帧被定期插入其它类型的帧之间。\n\n![球 1](i/smw_background_ball_1.png \"球 1\")\n\n### P 帧（预测）\n\nP 帧利用了一个事实：当前的画面几乎总能**使用之前的一帧进行渲染**。例如，在第二帧，唯一的改变是球向前移动了。仅仅使用（第二帧）对前一帧的引用和差值，我们就能重建前一帧。\n\n![球 1](i/smw_background_ball_1.png \"球 1\") <-  ![球 2](i/smw_background_ball_2_diff.png \"球 2\")\n\n> #### 自己动手：具有单个 I 帧的视频\n> 既然 P 帧使用较少的数据，为什么我们不能用[单个 I 帧和其余的 P 帧](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#1-i-frame-and-the-rest-p-frames)来编码整个视频？\n>\n> 编码完这个视频之后，开始观看它，并**快进到视频的末尾部分**，你会注意到**它需要花一些时间**才真正跳转到这部分。这是因为 **P 帧需要一个引用帧**（比如 I 帧）才能渲染。\n>\n> 你可以做的另一个快速试验，是使用单个 I 帧编码视频，然后[再次编码且每 2 秒插入一个 I 帧](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#1-i-frames-per-second-vs-05-i-frames-per-second)，并**比较成品的大小**。\n\n### B 帧（双向预测）\n\n如何引用前面和后面的帧去做更好的压缩？！简单地说 B 帧就是这么做的。\n\n ![球 1](i/smw_background_ball_1.png \"球 1\") <-  ![球 2](i/smw_background_ball_2_diff.png \"球 2\") -> ![球 3](i/smw_background_ball_3.png \"球 3\")\n\n> #### 自己动手：使用 B 帧比较视频\n> 你可以生成两个版本，一个使用 B 帧，另一个[全部不使用 B 帧](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#no-b-frames-at-all)，然后查看文件的大小以及画质。\n\n### 小结\n\n这些帧类型用于提供更好的压缩率，我们将在下一章看到这是如何发生的。现在，我们可以想到 I 帧是昂贵的，P 帧是便宜的，最便宜的是 B 帧。\n\n![帧类型例子](i/frame_types.png \"帧类型例子\")\n\n## 时间冗余（帧间预测）\n\n让我们探究去除**时间上的重复**，去除这一类冗余的技术就是**帧间预测**。\n\n我们将尝试**花费较少的数据量**去编码在时间上连续的 0 号帧和 1 号帧。\n\n![原始帧](i/original_frames.png \"原始帧\")\n\n我们可以做个减法，我们简单地**用 0 号帧减去 1 号帧**，得到残差，这样我们就只需要**对残差进行编码**。\n\n![残差帧](i/difference_frames.png \"残差帧\")\n\n但我们有一个**更好的方法**来节省数据量。首先，我们将`0 号帧` 视为一个个分块的集合，然后我们将尝试将 `帧 1` 和 `帧 0` 上的块相匹配。我们可以将这看作是**运动预测**。\n\n> ### 维基百科—块运动补偿\n> “运动补偿是一种描述相邻帧（相邻在这里表示在编码关系上相邻，在播放顺序上两帧未必相邻）差别的方法，具体来说是描述前面一帧（相邻在这里表示在编码关系上的前面，在播放顺序上未必在当前帧前面）的每个小块怎样移动到当前帧中的某个位置去。”\n\n![原始帧运动预测](i/original_frames_motion_estimation.png \"原始帧运动预测\")\n\n我们预计那个球会从 `x=0, y=25` 移动到 `x=6, y=26`，**x** 和 **y** 的值就是**运动向量**。**进一步**节省数据量的方法是，只编码这两者运动向量的差。所以，最终运动向量就是 `x=6 (6-0), y=1 (26-25)`。\n\n> 实际情况下，这个球会被切成 n 个分区，但处理过程是相同的。\n\n帧上的物体**以三维方式移动**，当球移动到背景时会变小。当我们尝试寻找匹配的块，**找不到完美匹配的块**是正常的。这是一张运动预测与实际值相叠加的图片。\n\n![运动预测](i/motion_estimation.png \"运动预测\")\n\n但我们能看到当我们使用**运动预测**时，**编码的数据量少于**使用简单的残差帧技术。\n\n![运动预测 vs 残差 ](i/comparison_delta_vs_motion_estimation.png \"运动预测 vs 残差\")\n\n你可以[使用 jupyter 玩转这些概念](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/frame_difference_vs_motion_estimation_plus_residual.ipynb)。\n\n> ### 自己动手：查看运动向量\n>\n> 我们可以[使用 ffmpeg 生成包含帧间预测（运动向量）的视频](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#generate-debug-video)。\n>\n> ![ffmpeg 帧间预测（运动向量）](i/motion_vectors_ffmpeg.png \"ffmpeg 帧间预测（运动向量）\")\n>\n> 或者我们也可使用 [Intel® Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer)（需要付费，但也有只能查看前 10 帧的免费试用版）。\n>\n> ![Intel® Video Pro Analyzer 使用帧间预测](i/inter_prediction_intel_video_pro_analyzer.png \"inter prediction intel video pro analyzer\")\n\n## 空间冗余（帧内预测）\n如果我们分析一个视频里的**每一帧**，我们会看到有**许多区域是相互关联的**。\n\n![空间内重复](i/repetitions_in_space.png \"空间内重复\")\n\n让我们举一个例子。这个场景大部分由蓝色和白色组成。\n\n![smw 背景](i/smw_bg.png \"smw 背景\")\n\n这是一个 `I 帧`，我们**不能使用前面的帧来预测**，但我们仍然可以压缩它。我们将编码我们选择的那块红色区域。如果我们**看看它的周围**，我们可以**估计它周围颜色的变化**。\n\n![smw 背景块](i/smw_bg_block.png \"smw 背景块\")\n\n我们预测:帧中的颜色在垂直方向上保持一致，这意味着**未知像素的颜色与临近的像素相同**。\n\n![smw 背景预测](i/smw_bg_prediction.png \"smw 背景预测\")\n\n我们的**预测会出错**，所以我们需要先利用这项技术（**帧内预测**），然后**减去实际值**，算出残差，得出的矩阵比原始数据更容易压缩。\n\n![smw 残差](i/smw_residual.png \"smw 残差\")\n\n> ### 自己动手：查看帧内预测\n> 你可以[使用 ffmpeg 生成包含宏块及预测的视频](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#generate-debug-video)。请查看 ffmpeg 文档以了解[每个块颜色的含义](https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors)。\n>\n> ![ffmpeg 帧内预测（宏块）](i/macro_blocks_ffmpeg.png \"ffmpeg 帧内预测（宏块）\")\n>\n> 或者我们也可使用 [Intel® Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer)（需要付费，但也有只能查看前 10 帧的免费试用版）。\n>\n> ![Intel® Video Pro Analyzer 帧内预测](i/intra_prediction_intel_video_pro_analyzer.png \"Intel® Video Pro Analyzer 帧内预测\")\n\n# 视频编解码器是如何工作的？\n\n## 是什么？为什么？怎么做？\n\n**是什么？** 就是用于压缩或解压数字视频的软件或硬件。**为什么？** 人们需要在有限带宽或存储空间下提高视频的质量。还记得当我们计算每秒 30 帧，每像素 24 bit，分辨率是 480x240 的视频[需要多少带宽](#基本术语)吗？没有压缩时是 **82.944 Mbps**。电视或互联网提供 HD/FullHD/4K 只能靠视频编解码器。**怎么做？** 我们将简单介绍一下主要的技术。\n\n> 视频编解码 vs 容器\n>\n> 初学者一个常见的错误是混淆数字视频编解码器和[数字视频容器](https://en.wikipedia.org/wiki/Digital_container_format)。我们可以将**容器**视为包含视频（也很可能包含音频）元数据的包装格式，**压缩过的视频**可以看成是它承载的内容。\n>\n> 通常，视频文件的格式定义其视频容器。例如，文件 `video.mp4` 可能是 [MPEG-4 Part 14](https://en.wikipedia.org/wiki/MPEG-4_Part_14) 容器，一个叫 `video.mkv` 的文件可能是 [matroska](https://en.wikipedia.org/wiki/Matroska)。我们可以使用 [ffmpeg 或 mediainfo](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#inspect-stream) 来完全确定编解码器和容器格式。\n\n## 历史\n\n在我们跳进通用编解码器内部工作之前，让我们回头了解一些旧的视频编解码器。\n\n视频编解码器 [H.261](https://en.wikipedia.org/wiki/H.261) 诞生在 1990（技术上是 1988），被设计为以 **64 kbit/s 的数据速率**工作。它已经使用如色度子采样、宏块，等等理念。在 1995 年，**H.263** 视频编解码器标准被发布，并继续延续到 2001 年。\n\n在 2003 年 **H.264/AVC** 的第一版被完成。在同一年，一家叫做 **TrueMotion** 的公司发布了他们的**免版税**有损视频压缩的视频编解码器，称为 **VP3**。在 2008 年，**Google 收购了**这家公司，在同一年发布 **VP8**。在 2012 年 12 月，Google 发布了 **VP9**，**市面上大约有 3/4 的浏览器**（包括手机）支持。\n\n[AV1](https://en.wikipedia.org/wiki/AOMedia_Video_1) 是由 **Google, Mozilla, Microsoft, Amazon, Netflix, AMD, ARM, NVidia, Intel, Cisco** 等公司组成的[开放媒体联盟（AOMedia）](http://aomedia.org/)设计的一种新的视频编解码器，免版税，开源。**第一版** 0.1.0 参考编解码器**发布于 2016 年 4 月 7 号**。\n\n![编解码器历史线路图](i/codec_history_timeline.png \"编解码器历史线路图\")\n\n> ### AV1 的诞生\n>\n> 2015 年早期，Google 正在 VP10 上工作，Xiph (Mozilla) 正在 Daala 上工作，Cisco 开源了它的称为 Thor 的免版税视频编解码器。\n>\n> 接着 MPEG LA 宣布了 HEVC (H.265) 每年版税的的上限，比 H.264 高 8 倍，但很快他们又再次改变了条款：\n> *\t**不设年度收费上限**\n> *\t**收取内容费**（收入的 0.5%）\n> *\t**每单位费用高于 h264 的 10 倍**\n>\n> [开放媒体联盟](http://aomedia.org/about-us/)由硬件厂商（Intel, AMD, ARM , Nvidia, Cisco），内容分发商（Google, Netflix, Amazon），浏览器维护者（Google, Mozilla），等公司创建。\n>\n> 这些公司有一个共同目标，一个免版税的视频编解码器，所以 AV1 诞生时使用了一个更[简单的专利许可证](http://aomedia.org/license/patent/)。**Timothy B. Terriberry** 做了一个精彩的介绍，[关于 AV1 的概念，许可证模式和它当前的状态](https://www.youtube.com/watch?v=lzPaldsmJbk)，就是本节的来源。\n>\n> 前往 [http://aomanalyzer.org/](http://aomanalyzer.org/)， 你会惊讶于**使用你的浏览器就可以分析 AV1 编解码器**。\n> ![av1 浏览器分析器](i/av1_browser_analyzer.png \"浏览器分析器\")\n>\n> 附：如果你想了解更多编解码器的历史，你需要了解[视频压缩专利](https://www.vcodex.com/video-compression-patents/)背后的基本知识。\n\n## 通用编解码器\n\n我们接下来要介绍**通用视频编解码器背后的主要机制**，大多数概念都很实用，并被现代编解码器如 VP9, AV1 和 HEVC 使用。需要注意：我们将简化许多内容。有时我们会使用真实的例子（主要是 H.264）来演示技术。\n\n## 第一步 - 图片分区\n\n第一步是**将帧**分成几个**分区**，**子分区**甚至更多。\n\n![图片分区](i/picture_partitioning.png \"图片分区\")\n\n**但是为什么呢？**有许多原因，比如，当我们分割图片时，我们可以更精确的处理预测，在微小移动的部分使用较小的分区，而在静态背景上使用较大的分区。\n\n通常，编解码器**将这些分区组织**成切片（或瓦片），宏（或编码树单元）和许多子分区。这些分区的最大大小有所不同，HEVC 设置成 64x64，而 AVC 使用 16x16，但子分区可以达到 4x4 的大小。\n\n还记得我们学过的**帧的分类**吗？你也可以**把这些概念应用到块**，因此我们可以有 I 切片，B 切片，I 宏块等等。\n\n> ### 自己动手：查看分区\n>\n> 我们也可以使用 [Intel® Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer)（需要付费，但也有只能查看前 10 帧的免费试用版）。这是 [VP9 分区](https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#transcoding)的分析。\n>\n> ![Intel® Video Pro Analyzer VP9 分区视图 ](i/paritions_view_intel_video_pro_analyzer.png \"Intel® Video Pro Analyzer VP9 分区视图\")\n\n## 第二步 - 预测\n\n一旦我们有了分区，我们就可以在它们之上做出预测。对于[帧间预测](#时间冗余（帧间预测）)，我们需要**发送运动向量和残差**；至于[帧内预测](#空间冗余（帧内预测）)，我们需要**发送预测方向和残差**。\n\n## 第三步 - 转换\n\n在我们得到残差块（`预测分区-真实分区`）之后，我们可以用一种方式**变换**它，这样我们就知道**哪些像素我们应该丢弃**，还依然能保持**整体质量**。这个确切的行为有几种变换方式。\n\n尽管有[其它的变换方式](https://en.wikipedia.org/wiki/List_of_Fourier-related_transforms#Discrete_transforms)，但我们重点关注离散余弦变换（DCT）。[DCT](https://en.wikipedia.org/wiki/Discrete_cosine_transform) 的主要功能有：\n*\t将**像素**块**转换**为相同大小的**频率系数块**。\n*\t**压缩**能量，更容易消除空间冗余。\n*\t**可逆的**，也意味着你可以还原回像素。\n\n> 2017 年 2 月 2 号，F. M. Bayer 和 R. J. Cintra 发表了他们的论文：[图像压缩的 DCT 类变换只需要 14 个加法](https://arxiv.org/abs/1702.00817)。\n\n如果你不理解每个要点的好处，不用担心，我们会尝试进行一些实验，以便从中看到真正的价值。\n\n我们来看下面的**像素块**（8x8）：\n\n![像素值矩形](i/pixel_matrice.png \"像素值矩形\")\n\n下面是其渲染的块图像（8x8）：\n\n![像素值矩形](i/gray_image.png \"像素值矩形\")\n\n当我们对这个像素块**应用 DCT** 时， 得到如下**系数块**（8x8）：\n\n![系数值 values](i/dct_coefficient_values.png \"系数值\")\n\n接着如果我们渲染这个系数块，就会得到这张图片：\n\n![dct 系数图片](i/dct_coefficient_image.png \"dct 系数图片\")\n\n如你所见它看起来完全不像原图像，我们可能会注意到**第一个系数**与其它系数非常不同。第一个系数被称为直流分量，代表了输入数组中的**所有样本**，有点**类似于平均值**。\n\n这个系数块有一个有趣的属性：高频部分和低频部分是分离的。\n\n![dct 频率系数属性](i/dctfrequ.jpg \"dct 频率系数属性\")\n\n在一张图像中，**大多数能量**会集中在[低频部分](https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm)，所以如果我们将图像转换成频率系数，并**丢掉高频系数**，我们就能**减少描述图像所需的数据量**，而不会牺牲太多的图像质量。\n> 频率是指信号变化的速度。\n\n让我们通过实验学习这点，我们将使用 DCT 把原始图像转换为频率（系数块），然后丢掉最不重要的系数。\n\n首先，我们将它转换为其**频域**。\n\n![系数值](i/dct_coefficient_values.png \"系数值\")\n\n然后我们丢弃部分（67%）系数，主要是它的右下角部分。\n\n![系数清零](i/dct_coefficient_zeroed.png \"系数清零\")\n\n然后我们从丢弃的系数块重构图像（记住，这需要可逆），并与原始图像相比较。\n\n![原始 vs 量化](i/original_vs_quantized.png \"原始 vs 量化\")\n\n如我们所见它酷似原始图像，但它引入了许多与原来的不同，我们**丢弃了67.1875%**，但我们仍然得到至少类似于原来的东西。我们可以更加智能的丢弃系数去得到更好的图像质量，但这是下一个主题。\n\n> ### 使用全部像素形成每个系数\n>  \n> 重要的是要注意，每个系数并不直接映射到单个像素，但它是所有像素的加权和。这个神奇的图形展示了如何计算出第一和第二个系数，使用每个唯一的索引做权重。\n>\n> ![dct 计算](i/applicat.jpg \"dct 计算\")\n>\n> 来源：[https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm](https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm)\n>  \n> 你也可以尝试[通过查看在 DCT 基础上形成的简单图片来可视化 DCT](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/dct_better_explained.ipynb)。例如，这是使用每个系数权重[形成的字符 A](https://en.wikipedia.org/wiki/Discrete_cosine_transform#Example_of_IDCT)。\n>\n> ![](https://upload.wikimedia.org/wikipedia/commons/5/5e/Idct-animation.gif )\n\n<br/>\n\n> ### 自己动手：丢弃不同的系数\n> 你可以玩转 [DCT 变换](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/uniform_quantization_experience.ipynb)\n\n## 第四步 - 量化\n\n当我们丢弃一些系数时，在最后一步（变换），我们做了一些形式的量化。这一步，我们选择性地剔除信息（**有损部分**）或者简单来说，我们将**量化系数以实现压缩**。\n\n我们如何量化一个系数块？一个简单的方法是均匀量化，我们取一个块并**将其除以单个的值**（10），并舍入值。\n\n![量化](i/quantize.png \"量化\")\n\n我们如何**逆转**（重新量化）这个系数块？我们可以通过**乘以我们先前除以的相同的值**（10）来做到。\n\n![逆转量化](i/re-quantize.png \"逆转量化\")\n\n这**不是最好的方法**，因为它没有考虑到每个系数的重要性，我们可以使用一个**量化矩阵**来代替单个值，这个矩阵可以利用 DCT 的属性，多量化右下部，而少（量化）左上部，[JPEG 使用了类似的方法](https://www.hdm-stuttgart.de/~maucher/Python/MMCodecs/html/jpegUpToQuant.html)，你可以通过[查看源码看看这个矩阵](https://github.com/google/guetzli/blob/master/guetzli/jpeg_data.h#L40)。\n\n> ### 自己动手：量化\n> 你可以玩转[量化](https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/dct_experiences.ipynb)\n\n## 第五步 - 熵编码\n\n在我们量化数据（图像块／切片／帧）之后，我们仍然可以以无损的方式来压缩它。有许多方法（算法）可用来压缩数据。我们将简单体验其中几个，你可以阅读这本很棒的书去深入理解：[Understanding Compression: Data Compression for Modern Developers](https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/)。\n\n### VLC 编码：\n\n让我们假设我们有一个符号流：**a**, **e**, **r** 和 **t**，它们的概率（从0到1）由下表所示。\n\n|     | a   | e   | r    | t   |\n|-----|-----|-----|------|-----|\n| 概率 | 0.3 | 0.3 | 0.2 |  0.2 |\n\n\n我们可以分配不同的二进制码，（最好是）小的码给最可能（出现的字符），大些的码给最少可能（出现的字符）。\n\n|        | a   | e   | r    | t   |\n|--------|-----|-----|------|-----|\n|   概率  | 0.3 | 0.3 | 0.2 | 0.2 |\n| 二进制码 | 0 | 10 | 110 | 1110 |\n\n\n让我们压缩 **eat** 流，假设我们为每个字符花费 8 bit，在没有做任何压缩时我们将花费 **24 bit**。但是在这种情况下，我们使用各自的代码来替换每个字符，我们就能节省空间。\n\n第一步是编码字符 **e** 为 `10`，第二个字符是 **a**，追加（不是数学加法）后是 `[10][0]`，最后是第三个字符 **t**，最终组成已压缩的比特流 `[10][0][1110]` 或 `1001110`，这只需 **7 bit**（比原来的空间少 3.4 倍）。\n\n请注意每个代码必须是唯一的前缀码，[Huffman 能帮你找到这些数字](https://en.wikipedia.org/wiki/Huffman_coding)。虽然它有一些问题，但是[视频编解码器仍然提供该方法](https://en.wikipedia.org/wiki/Context-adaptive_variable-length_coding)，它也是很多应用程序的压缩算法。\n\n编码器和解码器都**必须知道**这个（包含编码的）字符表，因此，你也需要传送这个表。\n\n### 算术编码\n\n让我们假设我们有一个符号流：**a**, **e**, **r**, **s** 和 **t**，它们的概率由下表所示。\n\n|     | a   | e   | r    | s    | t   |\n|-----|-----|-----|------|------|-----|\n| 概率 | 0.3 | 0.3 | 0.15 | 0.05 | 0.2 |\n\n\n考虑到这个表，我们可以构建一个区间，区间包含了所有可能的字符，字符按出现概率排序。\n\n![初始算法区间](i/range.png \"初始算法区间\")\n\n让我们编码 **eat** 流，我们选择第一个字符 **e** 位于 **0.3 到 0.6** （但不包括 0.6）的子区间，我们选择这个子区间，按照之前同等的比例再次分割。\n\n![第二个子区间](i/second_subrange.png \"第二个子区间\")\n\n让我们继续编码我们的流 **eat**，现在使第二个 **a** 字符位于 **0.3 到 0.39** 的区间里，接着再次用同样的方法编码最后的字符 **t**，得到最后的子区间 **0.354 到 0.372**。\n\n![最终算法区间](i/arithimetic_range.png \"最终算法区间\")\n\n我们只需从最后的子区间 0.354 到 0.372 里选择一个数，让我们选择 0.36，不过我们可以选择这个子区间里的任何数。仅靠这个数，我们将可以恢复原始流 **eat**。就像我们在区间的区间里画了一根线来编码我们的流。\n\n![最终区间横断面](i/range_show.png \"最终区间横断面\")\n\n**反向过程**（又名解码）一样简单，用数字 **0.36** 和我们原始区间，我们可以进行同样的操作，不过现在是使用这个数字来还原被编码的流。\n\n在第一个区间，我们发现数字落入了一个子区间，因此，这个子区间是我们的第一个字符，现在我们再次切分这个子区间，像之前一样做同样的过程。我们会注意到 **0.36** 落入了 **a** 的区间，然后我们重复这一过程直到得到最后一个字符 **t**（形成我们原始编码过的流 eat）。\n\n编码器和解码器都**必须知道**字符概率表，因此，你也需要传送这个表。\n\n非常巧妙，不是吗？人们能想出这样的解决方案实在是太聪明了，一些[视频编解码器使用](https://en.wikipedia.org/wiki/Context-adaptive_binary_arithmetic_coding)这项技术（或至少提供这一选择）。\n\n关于无损压缩量化比特流的办法，这篇文章无疑缺少了很多细节、原因、权衡等等。作为一个开发者你[应该学习更多](https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/)。刚入门视频编码的人可以尝试使用不同的[熵编码算法，如ANS](https://en.wikipedia.org/wiki/Asymmetric_Numeral_Systems)。\n\n> ### 自己动手：CABAC vs CAVLC\n> 你可以[生成两个流，一个使用 CABAC，另一个使用 CAVLC](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#cabac-vs-cavlc)，并比较生成每一个的时间以及最终的大小。\n\n## 第六步 - 比特流格式\n\n完成所有这些步之后，我们需要将**压缩过的帧和内容打包进去**。需要明确告知解码器**编码定义**，如颜色深度，颜色空间，分辨率，预测信息（运动向量，帧内预测方向），配置<sup>\\*</sup>，层级<sup>\\*</sup>，帧率，帧类型，帧号等等更多信息。\n> <sup>*</sup> 译注：原文为 profile 和 level，没有通用的译名\n\n我们将简单地学习 H.264 比特流。第一步是[生成一个小的 H.264<sup>\\*</sup> 比特流](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#generate-a-single-frame-h264-bitstream)，可以使用本 repo 和 [ffmpeg](http://ffmpeg.org/) 来做。\n\n```\n./s/ffmpeg -i /files/i/minimal.png -pix_fmt yuv420p /files/v/minimal_yuv420.h264\n```\n\n> <sup>*</sup> ffmpeg 默认将所有参数添加为 **SEI NAL**，很快我们会定义什么是 NAL。\n\n这个命令会使用下面的图片作为帧，生成一个具有**单个帧**，64x64 和颜色空间为 yuv420 的原始 h264 比特流。\n> ![使用帧来生成极简 h264 比特流](i/minimal.png \"使用帧来生成极简 h264 比特流\")\n\n### H.264 比特流\n\nAVC (H.264) 标准规定信息将在宏帧（网络概念上的）内传输，称为 [NAL](https://en.wikipedia.org/wiki/Network_Abstraction_Layer)（网络抽象层）。NAL 的主要目标是提供“网络友好”的视频呈现方式，该标准必须适用于电视（基于流），互联网（基于数据包）等。\n\n![H.264 NAL 单元](i/nal_units.png \"H.264 NAL 单元\")\n\n[同步标记](https://en.wikipedia.org/wiki/Frame_synchronization)用来定义 NAL 单元的边界。每个同步标记的值固定为  `0x00 0x00 0x01` ，最开头的标记例外，它的值是  `0x00 0x00 0x00 0x01` 。如果我们在生成的 h264 比特流上运行 **hexdump**，我们可以在文件的开头识别至少三个 NAL。\n\n![NAL 单元上的同步标记](i/minimal_yuv420_hex.png \"NAL 单元上的同步标记\")\n\n我们之前说过，解码器需要知道不仅仅是图片数据，还有视频的详细信息，如：帧、颜色、使用的参数等。每个 NAL 的**第一位**定义了其分类和**类型**。\n\n| NAL type id  | 描述  |\n|---  |---|\n| 0  |  Undefined |\n| 1  |  Coded slice of a non-IDR picture |\n| 2  |  Coded slice data partition A |\n| 3  |  Coded slice data partition B |\n| 4  |  Coded slice data partition C |\n| 5  |  **IDR** Coded slice of an IDR picture |\n| 6  |  **SEI** Supplemental enhancement information |\n| 7  |  **SPS** Sequence parameter set |\n| 8  |  **PPS** Picture parameter set |\n| 9  |  Access unit delimiter |\n| 10 |  End of sequence |\n| 11 |  End of stream |\n| ... |  ... |\n\n通常，比特流的第一个 NAL 是 **SPS**，这个类型的 NAL 负责传达通用编码参数，如**配置，层级，分辨率**等。\n\n如果我们跳过第一个同步标记，就可以通过解码**第一个字节**来了解第一个 **NAL 的类型**。\n\n例如同步标记之后的第一个字节是 `01100111`，第一位（`0`）是 **forbidden_zero_bit** 字段，接下来的两位（`11`）告诉我们是 **nal_ref_idc** 字段，其表示该 NAL 是否是参考字段，其余 5 位（`00111`）告诉我们是 **nal_unit_type** 字段，在这个例子里是 NAL 单元 **SPS** (7)。\n\nSPS NAL 的第 2 位 (`binary=01100100, hex=0x64, dec=100`) 是 **profile_idc** 字段，显示编码器使用的配置，在这个例子里，我们使用[受限高配置](https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Profiles)，一种没有 B（双向预测） 切片支持的高配置。\n\n![SPS 二进制视图](i/minimal_yuv420_bin.png \"SPS 二进制视图\")\n\n当我们阅读 SPS NAL 的 H.264 比特流规范时，会为**参数名称**，**分类**和**描述**找到许多值，例如，看看字段 `pic_width_in_mbs_minus_1` 和 `pic_height_in_map_units_minus_1`。\n\n| 参数名称  | 分类  |  描述  |\n|---  |---|---|\n| pic_width_in_mbs_minus_1 |  0 | ue(v) |\n| pic_height_in_map_units_minus_1 |  0 | ue(v) |\n\n> **ue(v)**: 无符号整形 [Exp-Golomb-coded](https://pythonhosted.org/bitstring/exp-golomb.html)\n\n如果我们对这些字段的值进行一些计算，将最终得出**分辨率**。我们可以使用值为 `119（ (119 + 1) * macroblock_size = 120 * 16 = 1920）`的 `pic_width_in_mbs_minus_1` 表示 `1920 x 1080`，再次为了减少空间，我们使用 `119` 来代替编码 `1920`。\n\n如果我们再次使用二进制视图检查我们创建的视频 (ex: `xxd -b -c 11 v/minimal_yuv420.h264`)，可以跳到帧自身上一个 NAL。\n\n![h264 idr 切片头](i/slice_nal_idr_bin.png \"h264 idr 切片头\")\n\n我们可以看到最开始的 6 个字节：`01100101 10001000 10000100 00000000 00100001 11111111`。我们已经知道第一个字节告诉我们 NAL 的类型，在这个例子里， (`00101`) 是 **IDR 切片 (5)**，可以进一步检查它：\n\n![h264 切片头规格](i/slice_header.png \"h264 切片头规格\")\n\n对照规范，我们能解码切片的类型（**slice_type**），帧号（**frame_num**）等重要字段。\n\n为了获得一些字段（`ue(v), me(v), se(v) 或 te(v)`）的值，我们需要称为 [Exponential-Golomb](https://pythonhosted.org/bitstring/exp-golomb.html) 的特定解码器来解码它。当存在很多默认值时，这个方法编码变量值特别高效。\n\n> 这个视频里 **slice_type** 和 **frame_num** 的值是 7（I 切片）和 0（第一帧）。\n\n我们可以将**比特流视为一个协议**，如果你想学习更多关于比特流的内容，请参考 [ITU H.264 规范](http://www.itu.int/rec/T-REC-H.264-201610-I)。这个宏观图展示了图片数据（压缩过的 YUV）所在的位置。\n\n![h264 比特流宏观图](i/h264_bitstream_macro_diagram.png \"h264 比特流宏观图\")\n\n我们可以探究其它比特流，如 [VP9 比特流](https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf)，[H.265（HEVC）](http://handle.itu.int/11.1002/1000/11885-en?locatt=format:pdf)或是我们的新朋友 [AV1 比特流](https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8)，[他们很相似吗？不](http://www.gpac-licensing.com/2016/07/12/vp9-av1-bitstream-format/)，但只要学习了其中之一，学习其他的就简单多了。\n\n> ### 自己动手：检查 H.264 比特流\n> \n> 我们可以[生成一个单帧视频](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#generate-a-single-frame-video)，使用 [mediainfo](https://en.wikipedia.org/wiki/MediaInfo) 检查它的 H.264 比特流。事实上，你甚至可以查看[解析 h264(AVC) 视频流的源代码](https://github.com/MediaArea/MediaInfoLib/blob/master/Source/MediaInfo/Video/File_Avc.cpp)。\n>\n> ![mediainfo h264 比特流的详情 ](i/mediainfo_details_1.png \"mediainfo h264 比特流的详情\")\n>  \n> 我们也可使用 [Intel® Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer)，需要付费，但也有只能查看前 10 帧的免费试用版，这已经够达成学习目的了。\n>\n> ![Intel® Video Pro Analyzer h264 比特流的详情](i/intel-video-pro-analyzer.png \"Intel® Video Pro Analyzer h264 比特流的详情\")\n\n## 回顾\n\n我们可以看到我们学了许多**使用相同模型的现代编解码器**。事实上，让我们看看 Thor 视频编解码器框图，它包含所有我们学过的步骤。你现在应该能更好地理解数字视频领域内的创新和论文。\n![thor 编解码器块图](i/thor_codec_block_diagram.png \"thor 编解码器块图\")\n\n之前我们计算过我们[需要 139GB 来保存一个一小时，720p 分辨率和30fps的视频文件](#色度子采样)，如果我们使用在这里学过的技术，如**帧间和帧内预测，转换，量化，熵编码和其它**我们能实现——假设我们**每像素花费 0.031 bit**——同样观感质量的视频，**对比 139GB 的存储，只需 367.82MB**。\n> 我们根据这里提供的示例视频选择**每像素使用 0.031 bit**。\n\n## H.265 如何实现比 H.264 更好的压缩率\n\n我们已经更多地了解了编解码器的工作原理，那么就容易理解新的编解码器如何使用更少的数据量传输更高分辨率的视频。\n\n我们将比较 AVC 和 HEVC，要记住的是：我们几乎总是要在压缩率和更多的 CPU 周期（复杂度）之间作权衡。\n\nHEVC 比 AVC 有更大和更多的**分区**（和**子分区**）选项，更多**帧内预测方向**，**改进的熵编码**等，所有这些改进使得 H.265 比 H.264 的压缩率提升 50%。\n\n![h264 vs h265](i/avc_vs_hevc.png \"H.264 vs H.265\")\n\n# 在线流媒体\n## 通用架构\n\n![general_architecture](i/general_architecture.png)\n\n[TODO]\n\n## 渐进式下载和自适应流\n\n![progressive_download](i/progressive_download.png)\n\n![adaptive_streaming](i/adaptive_streaming.png)\n\n[TODO]\n\n## 内容保护\n\n我们可以用一个简单的令牌认证系统来保护视频。用户需要拥有一个有效的令牌才可以播放视频，CDN 会拒绝没有令牌的用户的请求。它与大多数网站的身份认证系统非常相似。\n\n![token_protection](i/token_protection.png)\n\n仅仅使用令牌认证系统，用户仍然可以下载并重新分发视频。DRM 系统可以用来避免这种情况。\n\n![drm](i/drm.png)\n\n实际情况下，人们通常同时使用这两种技术提供授权和认证。\n\n### DRM\n#### 主要系统\n\n* FPS - [**FairPlay Streaming**](https://developer.apple.com/streaming/fps/)\n* PR - [**PlayReady**](https://www.microsoft.com/playready/)\n* WV - [**Widevine**](http://www.widevine.com/)\n\n#### 是什么\n\nDRM 指的是数字版权管理，是一种**为数字媒体提供版权保护**的方法，例如数字视频和音频。尽管用在了很多场合，但它并[没有被普遍接受](https://en.wikipedia.org/wiki/Digital_rights_management#DRM-free_works).\n\n#### 为什么\n\n内容的创作者（大多是工作室/制片厂）希望保护他们的知识产权，使他们的数字媒体免遭未经授权的分发。\n\n#### 怎么做\n\n我们将用一种简单的、抽象的方式描述 DRM\n\n现有一份**内容 C1**（如 HLS 或 DASH 视频流），一个**播放器 P1**（如 shaka-clappr, exo-player 或 iOS），装在**设备 D1**（如智能手机、电视或台式机/笔记本）上，使用 **DRM 系统 DRM1**（如 FairPlay Streaming, PlayReady, Widevine）\n\n内容 C1 由 DRM1 用一个**对称密钥 K1** 加密，生成**加密内容 C'1**\n\n![DRM 一般流程](i/drm_general_flow.jpeg \"DRM 一般流程\")\n\n设备 D1 上的播放器 P1 有一个非对称密钥对，密钥对包含一个**私钥 PRK1**（这个密钥是受保护的<sup>1</sup>，只有 **D1** 知道密钥内容），和一个**公钥 PUK1**\n\n> **<sup>1</sup>受保护的**: 这种保护可以**通过硬件**进行保护，例如, 将这个密钥存储在一个特殊的芯片（只读）中，芯片的工作方式就像一个用来解密的[黑箱]。 或**通过软件**进行保护（较低的安全系数）。DRM 系统提供了识别设备所使用的保护类型的方法。\n\n当 **播放器 P1 希望播放****加密内容 C'1** 时，它需要与 **DRM1** 协商，将公钥 **PUK1** 发送给 DRM1, DRM1 会返回一个被公钥 **PUK1** **加密过的 K1**。按照推论，结果就是**只有 D1 能够解密**。\n\n`K1P1D1 = enc(K1, PUK1)`\n\n**P1** 使用它的本地 DRM 系统（这可以使用 [SoC](https://zh.wikipedia.org/wiki/系统芯片) ，一个专门的硬件和软件，这个系统可以使用它的私钥 PRK1 用来**解密**内容，它可以解密被加密过的**K1P1D1 的对称密钥 K1**。理想情况下，密钥不会被导出到内存以外的地方。\n\n```\n K1 = dec(K1P1D1, PRK1)\n\n P1.play(dec(C'1, K1))\n```\n\n![DRM 解码流程](i/drm_decoder_flow.jpeg \"DRM 解码流程\")\n\n# 如何使用 jupyter\n\n确保你已安装 docker，只需运行 `./s/start_jupyter.sh`，然后按照控制台的说明进行操作。\n\n# 会议\n\n*\t[DEMUXED](https://demuxed.com/) - 您可以[查看最近的2个活动演示](https://www.youtube.com/channel/UCIc_DkRxo9UgUSTvWVNCmpA)。\n\n# 参考\n\n这里有最丰富的资源，这篇文档包含的信息，均摘录、依据或受它们启发。你可以用这些精彩的链接，书籍，视频等深化你的知识。\n\n在线课程和教程：\n\n* [https://www.coursera.org/learn/digital/](https://www.coursera.org/learn/digital/)\n* [https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf](https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf)\n* [https://xiph.org/video/vid1.shtml](https://xiph.org/video/vid1.shtml)\n* [https://xiph.org/video/vid2.shtml](https://xiph.org/video/vid2.shtml)\n* [http://slhck.info/ffmpeg-encoding-course](http://slhck.info/ffmpeg-encoding-course)\n* [http://www.cambridgeincolour.com/tutorials/camera-sensors.htm](http://www.cambridgeincolour.com/tutorials/camera-sensors.htm)\n* [http://www.slideshare.net/vcodex/a-short-history-of-video-coding](http://www.slideshare.net/vcodex/a-short-history-of-video-coding)\n* [http://www.slideshare.net/vcodex/introduction-to-video-compression-1339433](http://www.slideshare.net/vcodex/introduction-to-video-compression-13394338)\n* [https://developer.android.com/guide/topics/media/media-formats.html](https://developer.android.com/guide/topics/media/media-formats.html)\n* [http://www.slideshare.net/MadhawaKasun/audio-compression-23398426](http://www.slideshare.net/MadhawaKasun/audio-compression-23398426)\n* [http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf](http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf)\n\n书籍:\n\n* [https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&ie=UTF8&qid=1486395327&sr=1-1](https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&ie=UTF8&qid=1486395327&sr=1-1)\n* [https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925](https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925)\n* [https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&ie=UTF8&qid=1486396914&sr=1-3&keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO](https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&ie=UTF8&qid=1486396914&sr=1-3&keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO)\n* [https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&ie=UTF8&qid=1486396940&sr=1-1&keywords=jan+ozer](https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&ie=UTF8&qid=1486396940&sr=1-1&keywords=jan+ozer)\n\n比特流规范:\n\n* [http://www.itu.int/rec/T-REC-H.264-201610-I](http://www.itu.int/rec/T-REC-H.264-201610-I)\n* [http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&lang=en](http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&lang=en)\n* [https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf](https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf)\n* [http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf](http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf)\n* [http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243](http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243)\n* [http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html](http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html)\n\n软件:\n\n* [https://ffmpeg.org/](https://ffmpeg.org/)\n* [https://ffmpeg.org/ffmpeg-all.html](https://ffmpeg.org/ffmpeg-all.html)\n* [https://ffmpeg.org/ffprobe.html](https://ffmpeg.org/ffprobe.html)\n* [https://trac.ffmpeg.org/wiki/](https://trac.ffmpeg.org/wiki/)\n* [https://software.intel.com/en-us/intel-video-pro-analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer)\n* [https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8](https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8)\n\n非-ITU 编解码器:\n\n* [https://aomedia.googlesource.com/](https://aomedia.googlesource.com/)\n* [https://github.com/webmproject/libvpx/tree/master/vp9](https://github.com/webmproject/libvpx/tree/master/vp9)\n* [https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml](https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml)\n* [https://people.xiph.org/~jm/daala/revisiting/](https://people.xiph.org/~jm/daala/revisiting/)\n* [https://www.youtube.com/watch?v=lzPaldsmJbk](https://www.youtube.com/watch?v=lzPaldsmJbk)\n* [https://fosdem.org/2017/schedule/event/om_av1/](https://fosdem.org/2017/schedule/event/om_av1/)\n\n编码概念:\n\n* [http://x265.org/hevc-h265/](http://x265.org/hevc-h265/)\n* [http://slhck.info/video/2017/03/01/rate-control.html](http://slhck.info/video/2017/03/01/rate-control.html)\n* [http://slhck.info/video/2017/02/24/vbr-settings.html](http://slhck.info/video/2017/02/24/vbr-settings.html)\n* [http://slhck.info/video/2017/02/24/crf-guide.html](http://slhck.info/video/2017/02/24/crf-guide.html)\n* [https://arxiv.org/pdf/1702.00817v1.pdf](https://arxiv.org/pdf/1702.00817v1.pdf)\n* [https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors](https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors)\n* [http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html](http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html)\n* [http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html](http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html)\n* [https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/](https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/)\n\n测试用视频序列:\n\n* [http://bbb3d.renderfarming.net/download.html](http://bbb3d.renderfarming.net/download.html)\n* [https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx](https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx)\n\n杂项:\n\n* [http://stackoverflow.com/a/24890903](http://stackoverflow.com/a/24890903)\n* [http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264](http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264)\n* [http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html](http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html)\n* [http://vanseodesign.com/web-design/color-luminance/](http://vanseodesign.com/web-design/color-luminance/)\n* [http://www.biologymad.com/nervoussystem/eyenotes.htm](http://www.biologymad.com/nervoussystem/eyenotes.htm)\n* [http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf](http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf)\n* [http://www.csc.villanova.edu/~rschumey/csc4800/dct.html](http://www.csc.villanova.edu/~rschumey/csc4800/dct.html)\n* [http://www.explainthatstuff.com/digitalcameras.html](http://www.explainthatstuff.com/digitalcameras.html)\n* [http://www.hkvstar.com](http://www.hkvstar.com)\n* [http://www.hometheatersound.com/](http://www.hometheatersound.com/)\n* [http://www.lighterra.com/papers/videoencodingh264/](http://www.lighterra.com/papers/videoencodingh264/)\n* [http://www.red.com/learn/red-101/video-chroma-subsampling](http://www.red.com/learn/red-101/video-chroma-subsampling)\n* [http://www.slideshare.net/ManoharKuse/hevc-intra-coding](http://www.slideshare.net/ManoharKuse/hevc-intra-coding)\n* [http://www.slideshare.net/mwalendo/h264vs-hevc](http://www.slideshare.net/mwalendo/h264vs-hevc)\n* [http://www.slideshare.net/rvarun7777/final-seminar-46117193](http://www.slideshare.net/rvarun7777/final-seminar-46117193)\n* [http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf](http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf)\n* [http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx](http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx)\n* [http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&PageNum=1](http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&PageNum=1)\n* [http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/](http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/)\n* [https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/](https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/)\n* [https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/](https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/)\n* [https://codesequoia.wordpress.com/category/video/](https://codesequoia.wordpress.com/category/video/)\n* [https://developer.apple.com/library/content/technotes/tn2224/_index.html](https://developer.apple.com/library/content/technotes/tn2224/_index.html)\n* [https://en.wikibooks.org/wiki/MeGUI/x264_Settings](https://en.wikibooks.org/wiki/MeGUI/x264_Settings)\n* [https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming](https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming)\n* [https://en.wikipedia.org/wiki/AOMedia_Video_1](https://en.wikipedia.org/wiki/AOMedia_Video_1)\n* [https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg](https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg)\n* [https://en.wikipedia.org/wiki/Cone_cell](https://en.wikipedia.org/wiki/Cone_cell)\n* [https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg](https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg)\n* [https://en.wikipedia.org/wiki/Inter_frame](https://en.wikipedia.org/wiki/Inter_frame)\n* [https://en.wikipedia.org/wiki/Intra-frame_coding](https://en.wikipedia.org/wiki/Intra-frame_coding)\n* [https://en.wikipedia.org/wiki/Photoreceptor_cell](https://en.wikipedia.org/wiki/Photoreceptor_cell)\n* [https://en.wikipedia.org/wiki/Pixel_aspect_ratio](https://en.wikipedia.org/wiki/Pixel_aspect_ratio)\n* [https://en.wikipedia.org/wiki/Presentation_timestamp](https://en.wikipedia.org/wiki/Presentation_timestamp)\n* [https://en.wikipedia.org/wiki/Rod_cell](https://en.wikipedia.org/wiki/Rod_cell)\n* [https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg](https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg)\n* [https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/](https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/)\n* [https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping](https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping)\n* [https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/](https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/)\n* [https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03](https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03)\n* [https://www.encoding.com/android/](https://www.encoding.com/android/)\n* [https://www.encoding.com/http-live-streaming-hls/](https://www.encoding.com/http-live-streaming-hls/)\n* [https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm](https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm)\n* [https://www.lifewire.com/cmos-image-sensor-493271](https://www.lifewire.com/cmos-image-sensor-493271)\n* [https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ](https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ)\n* [https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar](https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar)\n* [https://www.vcodex.com/h264avc-intra-precition/](https://www.vcodex.com/h264avc-intra-precition/)\n* [https://www.youtube.com/watch?v=9vgtJJ2wwMA](https://www.youtube.com/watch?v=9vgtJJ2wwMA)\n* [https://www.youtube.com/watch?v=LFXN9PiOGtY](https://www.youtube.com/watch?v=LFXN9PiOGtY)\n* [https://www.youtube.com/watch?v=Lto-ajuqW3w&list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6](https://www.youtube.com/watch?v=Lto-ajuqW3w&list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6)\n* [https://www.youtube.com/watch?v=LWxu4rkZBLw](https://www.youtube.com/watch?v=LWxu4rkZBLw)\n\n","slug":"Introduction-to-digital-video-technology","published":1,"updated":"2020-03-30T09:16:55.380Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9kyh0cl002l1pdb0upwejyk","content":"<h1 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h1><p>因为工作关系需要了解数字视频相关技术，在学习的过程中找到了这份托管在github上的<strong>数字视频导论</strong>的材料。</p>\n<p>这份材料介绍了基本的数字视频相关技术，言简意赅但又不枯燥无味，即有理论又有丰富的实践操作，在我学习的所有材料中算是比较上乘的材料。</p>\n<p>基于如上的原因，将该材料的相关内容转载到此处。大家可以直接访问该材料的github仓库<a href=\"https://github.com/leandromoreira/digital_video_introduction\" target=\"_blank\" rel=\"noopener\"><strong>digital_video_introduction</strong></a>获取相关内容。如下的所有内容皆来自<a href=\"https://github.com/leandromoreira/digital_video_introduction\" target=\"_blank\" rel=\"noopener\"><strong>digital_video_introduction</strong></a>，特此标注。</p>\n<p><a href=\"https://img.shields.io/badge/license-BSD--3--Clause-blue.svg\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/license-BSD--3--Clause-blue.svg\" alt=\"license\"></a></p>\n<a id=\"more\"></a>\n\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>这是一份循序渐进的视频技术的介绍。尽管它面向的是软件开发人员/工程师，但我们希望<strong>对任何人而言</strong>，这份文档都能简单易学。这个点子产生于一个<a href=\"https://docs.google.com/presentation/d/17Z31kEkl_NGJ0M66reqr9_uTG6tI5EDDVXpdPKVuIrs/edit#slide=id.p\" target=\"_blank\" rel=\"noopener\">视频技术新手小型研讨会</a>期间。</p>\n<p>本文档旨在尽可能使用<strong>浅显的词语，丰富的图像和实际例子</strong>介绍数字视频概念，使这些知识能适用于各种场合。你可以随时反馈意见或建议，以改进这篇文档。</p>\n<h1 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h1><ul>\n<li><a href=\"#介绍\">介绍</a></li>\n<li><a href=\"#目录\">目录</a></li>\n<li><a href=\"#基本术语\">基本术语</a><ul>\n<li><a href=\"#编码彩色图像的其它方法\">编码彩色图像的其它方法</a></li>\n<li><a href=\"#自己动手：玩转图像和颜色\">自己动手：玩转图像和颜色</a></li>\n<li><a href=\"#DVD-的-DAR-是-4-3\">DVD 的 DAR 是 4:3</a></li>\n<li><a href=\"#自己动手：检查视频属性\">自己动手：检查视频属性</a></li>\n</ul>\n</li>\n<li><a href=\"#消除冗余\">消除冗余</a><ul>\n<li><a href=\"#颜色，亮度和我们的眼睛\">颜色，亮度和我们的眼睛</a><ul>\n<li><a href=\"#颜色模型\">颜色模型</a></li>\n<li><a href=\"#YCbCr-和-RGB-之间的转换\">YCbCr 和 RGB 之间的转换</a></li>\n<li><a href=\"#色度子采样\">色度子采样</a></li>\n<li><a href=\"#自己动手：检查-YCbCr-直方图\">自己动手：检查 YCbCr 直方图</a></li>\n</ul>\n</li>\n<li><a href=\"#帧类型\">帧类型</a><ul>\n<li><a href=\"#I-帧（帧内编码，关键帧）\">I 帧（内部，关键帧）</a></li>\n<li><a href=\"#P-帧（预测）\">P 帧（预测）</a><ul>\n<li><a href=\"#自己动手：具有单个-I-帧的视频\">自己动手：具有单个 I 帧的视频</a></li>\n</ul>\n</li>\n<li><a href=\"#B-帧（双向预测）\">B 帧（双向预测）</a><ul>\n<li><a href=\"#自己动手：使用-B-帧比较视频\">自己动手：使用 B 帧比较视频</a></li>\n</ul>\n</li>\n<li><a href=\"#小结\">小结</a></li>\n</ul>\n</li>\n<li><a href=\"#时间冗余（帧间预测）\">时间冗余（帧间预测）</a><ul>\n<li><a href=\"#自己动手：查看运动向量\">自己动手：查看运动向量</a></li>\n</ul>\n</li>\n<li><a href=\"#空间冗余（帧内预测）\">空间冗余（帧内预测）</a><ul>\n<li><a href=\"#自己动手：查看帧内预测\">自己动手：查看帧内预测</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#视频编解码器是如何工作的？\">视频编解码器是如何工作的？</a><ul>\n<li><a href=\"#是什么？为什么？怎么做？\">是什么？为什么？怎么做？</a></li>\n<li><a href=\"#历史\">历史</a><ul>\n<li><a href=\"#AV1-的诞生\">AV1 的诞生</a></li>\n</ul>\n</li>\n<li><a href=\"#通用编解码器\">通用编解码器</a></li>\n<li><a href=\"#第一步-图片分区\">第一步 - 图片分区</a><ul>\n<li><a href=\"#自己动手：查看分区\">自己动手：查看分区</a></li>\n</ul>\n</li>\n<li><a href=\"#第二步-预测\">第二步 - 预测</a></li>\n<li><a href=\"#第三步-转换\">第三步 - 转换</a><ul>\n<li><a href=\"#自己动手：丢弃不同的系数\">自己动手：丢弃不同的系数</a></li>\n</ul>\n</li>\n<li><a href=\"#第四步-量化\">第四步 - 量化</a><ul>\n<li><a href=\"#自己动手：量化\">自己动手：量化</a></li>\n</ul>\n</li>\n<li><a href=\"#第五步-熵编码\">第五步 - 熵编码</a><ul>\n<li><a href=\"#VLC-编码：\">VLC 编码</a></li>\n<li><a href=\"#算术编码\">算术编码</a></li>\n<li><a href=\"#自己动手：CABAC-vs-CAVLC\">自己动手：CABAC vs CAVLC</a></li>\n</ul>\n</li>\n<li><a href=\"#第六步-比特流格式\">第六步 - 比特流格式</a><ul>\n<li><a href=\"#H-264-比特流\">H.264 比特流</a></li>\n<li><a href=\"#自己动手：检查-H-264-比特流\">自己动手：检查 H.264 比特流</a></li>\n</ul>\n</li>\n<li><a href=\"#回顾\">回顾</a></li>\n<li><a href=\"#H-265-如何实现比-H-264-更好的压缩率\">H.265 如何实现比 H.264 更好的压缩率?</a></li>\n</ul>\n</li>\n<li><a href=\"#在线流媒体\">在线流媒体</a><ul>\n<li><a href=\"#通用架构\">通用架构</a></li>\n<li><a href=\"#渐进式下载和自适应流\">渐进式下载和自适应流</a></li>\n<li><a href=\"#内容保护\">内容保护</a></li>\n</ul>\n</li>\n<li><a href=\"#如何使用-jupyter\">如何使用 jupyter</a></li>\n<li><a href=\"#会议\">会议</a></li>\n<li><a href=\"#参考\">参考</a></li>\n</ul>\n<h1 id=\"基本术语\"><a href=\"#基本术语\" class=\"headerlink\" title=\"基本术语\"></a>基本术语</h1><p>一个<strong>图像</strong>可以视作一个<strong>二维矩阵</strong>。如果将<strong>色彩</strong>考虑进来，我们可以做出推广：将这个图像视作一个<strong>三维矩阵</strong>——多出来的维度用于储存色彩信息。</p>\n<p>如果我们选择三原色（红、绿、蓝）代表这些色彩，这就定义了三个平面：第一个是红色平面，第二个是绿色平面，最后一个是蓝色平面。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/image_3d_matrix_rgb.png\" alt=\"an image is a 3d matrix RGB\" title=\"An image is a 3D matrix\"></p>\n<p>我们把这个矩阵里的每一个点称为<strong>像素</strong>（图像元素）。像素的色彩由三原色的<strong>强度</strong>（通常用数值表示）表示。例如，一个<strong>红色像素</strong>是指强度为 0 的绿色，强度为 0 的蓝色和强度最大的红色。<strong>粉色像素</strong>可以通过三种颜色的组合表示。如果规定强度的取值范围是 0 到 255，<strong>红色 255、绿色 192、蓝色 203</strong> 则表示粉色。</p>\n<blockquote>\n<h3 id=\"编码彩色图像的其它方法\"><a href=\"#编码彩色图像的其它方法\" class=\"headerlink\" title=\"编码彩色图像的其它方法\"></a>编码彩色图像的其它方法</h3><p>还有许多其它模型也可以用来表示色彩，进而组成图像。例如，给每种颜色都标上序号（如下图），这样每个像素仅需一个字节就可以表示出来，而不是 RGB 模型通常所需的 3 个。在这样一个模型里我们可以用一个二维矩阵来代替三维矩阵去表示我们的色彩，这将节省存储空间，但色彩的数量将会受限。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/nes-color-palette.png\" alt=\"NES palette\" title=\"NES palette\"></p>\n</blockquote>\n<p>例如以下几张图片。第一张包含所有颜色平面。剩下的分别是红、绿、蓝色平面（显示为灰调）（译注：颜色强度高的地方显示为亮色，强度低为暗色）。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/rgb_channels_intensity.png\" alt=\"RGB channels intensity\" title=\"RGB channels intensity\"></p>\n<p>我们可以看到，对于最终的成像，红色平面对强度的贡献更多（三个平面最亮的是红色平面），蓝色平面（最后一张图片）的贡献大多只在马里奥的眼睛和他衣服的一部分。所有颜色平面对马里奥的胡子（最暗的部分）均贡献较少。</p>\n<p>存储颜色的强度，需要占用一定大小的数据空间，这个大小被称为颜色深度。假如每个颜色（平面）的强度占用 8 bit（取值范围为 0 到 255），那么颜色深度就是 24（8*3）bit，我们还可以推导出我们可以使用 2 的 24 次方种不同的颜色。</p>\n<blockquote>\n<p>很棒的学习材料：<a href=\"http://www.cambridgeincolour.com/tutorials/camera-sensors.htm\" target=\"_blank\" rel=\"noopener\">现实世界的照片是如何拍摄成 0 和 1 的</a>。</p>\n</blockquote>\n<p>图片的另一个属性是<strong>分辨率</strong>，即一个平面内像素的数量。通常表示成宽<em>高，例如下面这张 *</em>4x4** 的图片。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/resolution.png\" alt=\"image resolution\" title=\"image resolution\"></p>\n<blockquote>\n<h3 id=\"自己动手：玩转图像和颜色\"><a href=\"#自己动手：玩转图像和颜色\" class=\"headerlink\" title=\"自己动手：玩转图像和颜色\"></a>自己动手：玩转图像和颜色</h3><p>你可以使用 <a href=\"#如何使用-jupyter\">jupyter</a>（python, numpy, matplotlib 等等）<a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/image_as_3d_array.ipynb\" target=\"_blank\" rel=\"noopener\">玩转图像</a>。</p>\n<p>你也可以学习<a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/filters_are_easy.ipynb\" target=\"_blank\" rel=\"noopener\">图像滤镜（边缘检测，磨皮，模糊。。。）的原理</a>。</p>\n</blockquote>\n<p>图像或视频还有一个属性是宽高比，它简单地描述了图像或像素的宽度和高度之间的比例关系。</p>\n<p>当人们说这个电影或照片是 16:9 时，通常是指显示宽高比（DAR），然而我们也可以有不同形状的单个像素，我们称为像素宽高比（PAR）。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/DAR.png\" alt=\"display aspect ratio\" title=\"display aspect ratio\"></p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/PAR.png\" alt=\"pixel aspect ratio\" title=\"pixel aspect ratio\"></p>\n<blockquote>\n<h2 id=\"DVD-的-DAR-是-4-3\"><a href=\"#DVD-的-DAR-是-4-3\" class=\"headerlink\" title=\"DVD 的 DAR 是 4:3\"></a>DVD 的 DAR 是 4:3</h2><p>虽然 DVD 的实际分辨率是 704x480，但它依然保持 4:3 的宽高比，因为它有一个 10:11（704x10／480x11）的 PAR。</p>\n</blockquote>\n<p>现在我们可以将<strong>视频</strong>定义为在<strong>单位时间</strong>内<strong>连续的 n 帧</strong>，这可以视作一个新的维度，n 即为帧率，若单位时间为秒，则等同于 FPS (每秒帧数 Frames Per Second)。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/video.png\" alt=\"video\" title=\"video\"></p>\n<p>播放一段视频每秒所需的数据量就是它的<strong>比特率</strong>（即常说的码率）。</p>\n<blockquote>\n<p>比特率 = 宽 * 高 * 颜色深度 * 帧每秒</p>\n</blockquote>\n<p>例如，一段每秒 30 帧，每像素 24 bits，分辨率是 480x240 的视频，如果我们不做任何压缩，它将需要 <strong>82,944,000 比特每秒</strong>或 82.944 Mbps (30x480x240x24)。</p>\n<p>当<strong>比特率</strong>几乎恒定时称为恒定比特率（<strong>CBR</strong>）；但它也可以变化，称为可变比特率（<strong>VBR</strong>）。</p>\n<blockquote>\n<p>这个图形显示了一个受限的 VBR，当帧为黑色时不会花费太多的数据量。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/vbr.png\" alt=\"constrained vbr\" title=\"constrained vbr\"></p>\n</blockquote>\n<p>在早期，工程师们想出了一项技术能将视频的感官帧率加倍而<strong>没有消耗额外带宽</strong>。这项技术被称为<strong>隔行扫描</strong>；总的来说，它在一个时间点发送一个画面——画面用于填充屏幕的一半，而下一个时间点发送的画面用于填充屏幕的另一半。</p>\n<p>如今的屏幕渲染大多使用<strong>逐行扫描技术</strong>。这是一种显示、存储、传输运动图像的方法，每帧中的所有行都会被依次绘制。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/interlaced_vs_progressive.png\" alt=\"interlaced vs progressive\" title=\"interlaced vs progressive\"></p>\n<p>现在我们知道了数字化<strong>图像</strong>的原理；它的<strong>颜色</strong>的编排方式；给定<strong>帧率</strong>和<strong>分辨率</strong>时，展示一个视频需要花费多少<strong>比特率</strong>；它是恒定的（CBR）还是可变的（VBR）；还有很多其它内容，如隔行扫描和 PAR。</p>\n<blockquote>\n<h2 id=\"自己动手：检查视频属性\"><a href=\"#自己动手：检查视频属性\" class=\"headerlink\" title=\"自己动手：检查视频属性\"></a>自己动手：检查视频属性</h2><p>你可以<a href=\"https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#inspect-stream\" target=\"_blank\" rel=\"noopener\">使用 ffmpeg 或 mediainfo 检查大多数属性的解释</a>。</p>\n</blockquote>\n<h1 id=\"消除冗余\"><a href=\"#消除冗余\" class=\"headerlink\" title=\"消除冗余\"></a>消除冗余</h1><p>我们认识到，不对视频进行压缩是不行的；<strong>一个单独的一小时长的视频</strong>，分辨率为 720p 和 30fps 时将<strong>需要 278GB<sup>*</sup></strong>。仅仅使用无损数据压缩算法——如 DEFLATE（被PKZIP, Gzip, 和 PNG 使用）——也无法充分减少视频所需的带宽，我们需要找到其它压缩视频的方法。</p>\n<blockquote>\n<p><sup>*</sup>我们使用乘积得出这个数字 1280 x 720 x 24 x 30 x 3600 （宽，高，每像素比特数，fps 和秒数）</p>\n</blockquote>\n<p>为此，我们可以<strong>利用视觉特性</strong>：和区分颜色相比，我们区分亮度要更加敏锐。<strong>时间上的重复</strong>：一段视频包含很多只有一点小小改变的图像。<strong>图像内的重复</strong>：每一帧也包含很多颜色相同或相似的区域。</p>\n<h2 id=\"颜色，亮度和我们的眼睛\"><a href=\"#颜色，亮度和我们的眼睛\" class=\"headerlink\" title=\"颜色，亮度和我们的眼睛\"></a>颜色，亮度和我们的眼睛</h2><p>我们的眼睛<a href=\"http://vanseodesign.com/web-design/color-luminance/\" target=\"_blank\" rel=\"noopener\">对亮度比对颜色更敏感</a>，你可以看看下面的图片自己测试。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/luminance_vs_color.png\" alt=\"luminance vs color\" title=\"luminance vs color\"></p>\n<p>如果你看不出左图的<strong>方块 A 和方块 B</strong> 的颜色是<strong>相同的</strong>，那么好，是我们的大脑玩了一个小把戏，这让我们更多的去注意光与暗，而不是颜色。右边这里有一个使用同样颜色的连接器，那么我们（的大脑）就能轻易分辨出事实，它们是同样的颜色。</p>\n<blockquote>\n<p><strong>简单解释我们的眼睛工作的原理</strong></p>\n<p><a href=\"http://www.biologymad.com/nervoussystem/eyenotes.htm\" target=\"_blank\" rel=\"noopener\">眼睛是一个复杂的器官</a>，有许多部分组成，但我们最感兴趣的是视锥细胞和视杆细胞。眼睛有<a href=\"https://en.wikipedia.org/wiki/Photoreceptor_cell\" target=\"_blank\" rel=\"noopener\">大约1.2亿个视杆细胞和6百万个视锥细胞</a>。</p>\n<p><strong>简单来说</strong>，让我们把颜色和亮度放在眼睛的功能部位上。<a href=\"https://en.wikipedia.org/wiki/Rod_cell\" target=\"_blank\" rel=\"noopener\">视杆细胞</a><strong>主要负责亮度</strong>，而<a href=\"https://en.wikipedia.org/wiki/Cone_cell\" target=\"_blank\" rel=\"noopener\">视锥细胞</a><strong>负责颜色</strong>，有三种类型的视锥，每个都有不同的颜料，叫做：<a href=\"https://upload.wikimedia.org/wikipedia/commons/1/1e/Cones_SMJ2_E.svg\" target=\"_blank\" rel=\"noopener\">S-视锥（蓝色），M-视锥（绿色）和L-视锥（红色）</a>。</p>\n<p>既然我们的视杆细胞（亮度）比视锥细胞多很多，一个合理的推断是相比颜色，我们有更好的能力去区分黑暗和光亮。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/eyes.jpg\" alt=\"eyes composition\" title=\"eyes composition\"></p>\n</blockquote>\n<p>一旦我们知道我们对<strong>亮度</strong>（图像中的亮度）更敏感，我们就可以利用它。</p>\n<h3 id=\"颜色模型\"><a href=\"#颜色模型\" class=\"headerlink\" title=\"颜色模型\"></a>颜色模型</h3><p>我们最开始学习的<a href=\"#基本术语\">彩色图像的原理</a>使用的是 <strong>RGB 模型</strong>，但也有其他模型。有一种模型将亮度（光亮）和色度（颜色）分离开，它被称为 <strong>YCbCr</strong><sup>*</sup>。</p>\n<blockquote>\n<p><sup>*</sup> 有很多种模型做同样的分离。</p>\n</blockquote>\n<p>这个颜色模型使用 <strong>Y</strong> 来表示亮度，还有两种颜色通道：Cb（蓝色色度） 和 Cr（红色色度）。YCbCr 可以由 RGB 转换得来，也可以转换回 RGB。使用这个模型我们可以创建拥有完整色彩的图像，如下图。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/ycbcr.png\" alt=\"ycbcr 例子\" title=\"ycbcr 例子\"></p>\n<h3 id=\"YCbCr-和-RGB-之间的转换\"><a href=\"#YCbCr-和-RGB-之间的转换\" class=\"headerlink\" title=\"YCbCr 和 RGB 之间的转换\"></a>YCbCr 和 RGB 之间的转换</h3><p>有人可能会问，在<strong>不使用绿色（色度）</strong>的情况下，我们如何表现出所有的色彩？</p>\n<p>为了回答这个问题，我们将介绍从 RGB 到 YCbCr 的转换。我们将使用 <a href=\"https://en.wikipedia.org/wiki/ITU-R\" target=\"_blank\" rel=\"noopener\">ITU-R 小组</a>*建议的<a href=\"https://en.wikipedia.org/wiki/Rec._601\" target=\"_blank\" rel=\"noopener\">标准 BT.601</a> 中的系数。</p>\n<p>第一步是计算亮度，我们将使用 ITU 建议的常量，并替换 RGB 值。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Y &#x3D; 0.299R + 0.587G + 0.114B</span><br></pre></td></tr></table></figure>\n\n<p>一旦我们有了亮度后，我们就可以拆分颜色（蓝色色度和红色色度）：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Cb &#x3D; 0.564(B - Y)</span><br><span class=\"line\">Cr &#x3D; 0.713(R - Y)</span><br></pre></td></tr></table></figure>\n\n<p>并且我们也可以使用 YCbCr 转换回来，甚至得到绿色。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">R &#x3D; Y + 1.402Cr</span><br><span class=\"line\">B &#x3D; Y + 1.772Cb</span><br><span class=\"line\">G &#x3D; Y - 0.344Cb - 0.714Cr</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p><sup>*</sup>组织和标准在数字视频领域中很常见，它们通常定义什么是标准，例如，<a href=\"https://en.wikipedia.org/wiki/Rec._2020\" target=\"_blank\" rel=\"noopener\">什么是 4K？我们应该使用什么帧率？分辨率？颜色模型？</a></p>\n</blockquote>\n<p>通常，<strong>显示屏</strong>（监视器，电视机，屏幕等等）<strong>仅使用 RGB 模型</strong>，并以不同的方式来组织，看看下面这些放大效果：</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/new_pixel_geometry.jpg\" alt=\"pixel geometry\" title=\"pixel geometry\"></p>\n<h3 id=\"色度子采样\"><a href=\"#色度子采样\" class=\"headerlink\" title=\"色度子采样\"></a>色度子采样</h3><p>一旦我们能从图像中分离出亮度和色度，我们就可以利用人类视觉系统对亮度比色度更敏感的特点，选择性地剔除信息。<strong>色度子采样</strong>是一种编码图像时，使<strong>色度分辨率低于亮度</strong>的技术。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/ycbcr_subsampling_resolution.png\" alt=\"ycbcr 子采样分辨率\" title=\"ycbcr 子采样分辨率\"></p>\n<p>我们应该减少多少色度分辨率呢？已经有一些模式定义了如何处理分辨率和合并（<code>最终的颜色 = Y + Cb + Cr</code>）。</p>\n<p>这些模式称为子采样系统，并被表示为 3 部分的比率 - <code>a:x:y</code>，其定义了色度平面的分辨率，与亮度平面上的、分辨率为 <code>a x 2</code> 的小块之间的关系。</p>\n<ul>\n<li><code>a</code> 是水平采样参考 (通常是 4)，</li>\n<li><code>x</code> 是第一行的色度样本数（相对于 a 的水平分辨率），</li>\n<li><code>y</code> 是第二行的色度样本数。</li>\n</ul>\n<blockquote>\n<p>存在的一个例外是 4:1:0，其在每个亮度平面分辨率为 4 x 4 的块内提供一个色度样本。</p>\n</blockquote>\n<p>现代编解码器中使用的常用方案是： 4:4:4 (没有子采样)**, 4:2:2, 4:1:1, 4:2:0, 4:1:0 and 3:1:1。</p>\n<blockquote>\n<p>YCbCr 4:2:0 合并</p>\n<p>这是使用 YCbCr 4:2:0 合并的一个图像的一块，注意我们每像素只花费 12bit。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/ycbcr_420_merge.png\" alt=\"YCbCr 4:2:0 合并\" title=\"YCbCr 4:2:0 合并\"></p>\n</blockquote>\n<p>下图是同一张图片使用几种主要的色度子采样技术进行编码，第一行图像是最终的 YCbCr，而最后一行图像展示了色度的分辨率。这么小的损失确实是一个伟大的胜利。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/chroma_subsampling_examples.jpg\" alt=\"色度子采样例子\" title=\"色度子采样例子\"></p>\n<p>前面我们计算过我们需要 <a href=\"#消除冗余\">278GB 去存储一个一小时长，分辨率在720p和30fps的视频文件</a>。如果我们使用 <code>YCbCr 4:2:0</code> 我们能剪掉<code>一半的大小（139GB）</code><sup>*</sup>，但仍然不够理想。</p>\n<blockquote>\n<p><sup>*</sup> 我们通过将宽、高、颜色深度和 fps 相乘得出这个值。前面我们需要 24 bit，现在我们只需要 12 bit。</p>\n</blockquote>\n<blockquote>\n<h3 id=\"自己动手：检查-YCbCr-直方图\"><a href=\"#自己动手：检查-YCbCr-直方图\" class=\"headerlink\" title=\"自己动手：检查 YCbCr 直方图\"></a>自己动手：检查 YCbCr 直方图</h3><p>你可以<a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#generates-yuv-histogram\" target=\"_blank\" rel=\"noopener\">使用 ffmpeg 检查 YCbCr 直方图</a>。这个场景有更多的蓝色贡献，由<a href=\"https://en.wikipedia.org/wiki/Histogram\" target=\"_blank\" rel=\"noopener\">直方图</a>显示。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/yuv_histogram.png\" alt=\"ycbcr 颜色直方图\" title=\"ycbcr 颜色直方图\"></p>\n</blockquote>\n<h2 id=\"帧类型\"><a href=\"#帧类型\" class=\"headerlink\" title=\"帧类型\"></a>帧类型</h2><p>现在我们进一步消除<code>时间冗余</code>，但在这之前让我们来确定一些基本术语。假设我们一段 30fps 的影片，这是最开始的 4 帧。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_1.png\" alt=\"球 1\" title=\"球 1\"> <img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_2.png\" alt=\"球 2\" title=\"球 2\"> <img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_3.png\" alt=\"球 3\" title=\"球 3\"><br><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_4.png\" alt=\"球 4\" title=\"球 4\"></p>\n<p>我们可以在帧内看到<strong>很多重复内容</strong>，如<strong>蓝色背景</strong>，从 0 帧到第 3 帧它都没有变化。为了解决这个问题，我们可以将它们<strong>抽象地分类</strong>为三种类型的帧。</p>\n<h3 id=\"I-帧（帧内编码，关键帧）\"><a href=\"#I-帧（帧内编码，关键帧）\" class=\"headerlink\" title=\"I 帧（帧内编码，关键帧）\"></a>I 帧（帧内编码，关键帧）</h3><p>I 帧（可参考，关键帧，帧内编码）是一个<strong>自足的帧</strong>。它不依靠任何东西来渲染，I 帧与静态图片相似。第一帧通常是 I 帧，但我们将看到 I 帧被定期插入其它类型的帧之间。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_1.png\" alt=\"球 1\" title=\"球 1\"></p>\n<h3 id=\"P-帧（预测）\"><a href=\"#P-帧（预测）\" class=\"headerlink\" title=\"P 帧（预测）\"></a>P 帧（预测）</h3><p>P 帧利用了一个事实：当前的画面几乎总能<strong>使用之前的一帧进行渲染</strong>。例如，在第二帧，唯一的改变是球向前移动了。仅仅使用（第二帧）对前一帧的引用和差值，我们就能重建前一帧。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_1.png\" alt=\"球 1\" title=\"球 1\"> &lt;-  <img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_2_diff.png\" alt=\"球 2\" title=\"球 2\"></p>\n<blockquote>\n<h4 id=\"自己动手：具有单个-I-帧的视频\"><a href=\"#自己动手：具有单个-I-帧的视频\" class=\"headerlink\" title=\"自己动手：具有单个 I 帧的视频\"></a>自己动手：具有单个 I 帧的视频</h4><p>既然 P 帧使用较少的数据，为什么我们不能用<a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#1-i-frame-and-the-rest-p-frames\" target=\"_blank\" rel=\"noopener\">单个 I 帧和其余的 P 帧</a>来编码整个视频？</p>\n<p>编码完这个视频之后，开始观看它，并<strong>快进到视频的末尾部分</strong>，你会注意到<strong>它需要花一些时间</strong>才真正跳转到这部分。这是因为 <strong>P 帧需要一个引用帧</strong>（比如 I 帧）才能渲染。</p>\n<p>你可以做的另一个快速试验，是使用单个 I 帧编码视频，然后<a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#1-i-frames-per-second-vs-05-i-frames-per-second\" target=\"_blank\" rel=\"noopener\">再次编码且每 2 秒插入一个 I 帧</a>，并<strong>比较成品的大小</strong>。</p>\n</blockquote>\n<h3 id=\"B-帧（双向预测）\"><a href=\"#B-帧（双向预测）\" class=\"headerlink\" title=\"B 帧（双向预测）\"></a>B 帧（双向预测）</h3><p>如何引用前面和后面的帧去做更好的压缩？！简单地说 B 帧就是这么做的。</p>\n<p> <img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_1.png\" alt=\"球 1\" title=\"球 1\"> &lt;-  <img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_2_diff.png\" alt=\"球 2\" title=\"球 2\"> -&gt; <img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_3.png\" alt=\"球 3\" title=\"球 3\"></p>\n<blockquote>\n<h4 id=\"自己动手：使用-B-帧比较视频\"><a href=\"#自己动手：使用-B-帧比较视频\" class=\"headerlink\" title=\"自己动手：使用 B 帧比较视频\"></a>自己动手：使用 B 帧比较视频</h4><p>你可以生成两个版本，一个使用 B 帧，另一个<a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#no-b-frames-at-all\" target=\"_blank\" rel=\"noopener\">全部不使用 B 帧</a>，然后查看文件的大小以及画质。</p>\n</blockquote>\n<h3 id=\"小结\"><a href=\"#小结\" class=\"headerlink\" title=\"小结\"></a>小结</h3><p>这些帧类型用于提供更好的压缩率，我们将在下一章看到这是如何发生的。现在，我们可以想到 I 帧是昂贵的，P 帧是便宜的，最便宜的是 B 帧。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/frame_types.png\" alt=\"帧类型例子\" title=\"帧类型例子\"></p>\n<h2 id=\"时间冗余（帧间预测）\"><a href=\"#时间冗余（帧间预测）\" class=\"headerlink\" title=\"时间冗余（帧间预测）\"></a>时间冗余（帧间预测）</h2><p>让我们探究去除<strong>时间上的重复</strong>，去除这一类冗余的技术就是<strong>帧间预测</strong>。</p>\n<p>我们将尝试<strong>花费较少的数据量</strong>去编码在时间上连续的 0 号帧和 1 号帧。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/original_frames.png\" alt=\"原始帧\" title=\"原始帧\"></p>\n<p>我们可以做个减法，我们简单地<strong>用 0 号帧减去 1 号帧</strong>，得到残差，这样我们就只需要<strong>对残差进行编码</strong>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/difference_frames.png\" alt=\"残差帧\" title=\"残差帧\"></p>\n<p>但我们有一个<strong>更好的方法</strong>来节省数据量。首先，我们将<code>0 号帧</code> 视为一个个分块的集合，然后我们将尝试将 <code>帧 1</code> 和 <code>帧 0</code> 上的块相匹配。我们可以将这看作是<strong>运动预测</strong>。</p>\n<blockquote>\n<h3 id=\"维基百科—块运动补偿\"><a href=\"#维基百科—块运动补偿\" class=\"headerlink\" title=\"维基百科—块运动补偿\"></a>维基百科—块运动补偿</h3><p>“运动补偿是一种描述相邻帧（相邻在这里表示在编码关系上相邻，在播放顺序上两帧未必相邻）差别的方法，具体来说是描述前面一帧（相邻在这里表示在编码关系上的前面，在播放顺序上未必在当前帧前面）的每个小块怎样移动到当前帧中的某个位置去。”</p>\n</blockquote>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/original_frames_motion_estimation.png\" alt=\"原始帧运动预测\" title=\"原始帧运动预测\"></p>\n<p>我们预计那个球会从 <code>x=0, y=25</code> 移动到 <code>x=6, y=26</code>，<strong>x</strong> 和 <strong>y</strong> 的值就是<strong>运动向量</strong>。<strong>进一步</strong>节省数据量的方法是，只编码这两者运动向量的差。所以，最终运动向量就是 <code>x=6 (6-0), y=1 (26-25)</code>。</p>\n<blockquote>\n<p>实际情况下，这个球会被切成 n 个分区，但处理过程是相同的。</p>\n</blockquote>\n<p>帧上的物体<strong>以三维方式移动</strong>，当球移动到背景时会变小。当我们尝试寻找匹配的块，<strong>找不到完美匹配的块</strong>是正常的。这是一张运动预测与实际值相叠加的图片。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/motion_estimation.png\" alt=\"运动预测\" title=\"运动预测\"></p>\n<p>但我们能看到当我们使用<strong>运动预测</strong>时，<strong>编码的数据量少于</strong>使用简单的残差帧技术。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/comparison_delta_vs_motion_estimation.png\" alt=\"运动预测 vs 残差 \" title=\"运动预测 vs 残差\"></p>\n<p>你可以<a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/frame_difference_vs_motion_estimation_plus_residual.ipynb\" target=\"_blank\" rel=\"noopener\">使用 jupyter 玩转这些概念</a>。</p>\n<blockquote>\n<h3 id=\"自己动手：查看运动向量\"><a href=\"#自己动手：查看运动向量\" class=\"headerlink\" title=\"自己动手：查看运动向量\"></a>自己动手：查看运动向量</h3><p>我们可以<a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#generate-debug-video\" target=\"_blank\" rel=\"noopener\">使用 ffmpeg 生成包含帧间预测（运动向量）的视频</a>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/motion_vectors_ffmpeg.png\" alt=\"ffmpeg 帧间预测（运动向量）\" title=\"ffmpeg 帧间预测（运动向量）\"></p>\n<p>或者我们也可使用 <a href=\"https://software.intel.com/en-us/intel-video-pro-analyzer\" target=\"_blank\" rel=\"noopener\">Intel® Video Pro Analyzer</a>（需要付费，但也有只能查看前 10 帧的免费试用版）。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/inter_prediction_intel_video_pro_analyzer.png\" alt=\"Intel® Video Pro Analyzer 使用帧间预测\" title=\"inter prediction intel video pro analyzer\"></p>\n</blockquote>\n<h2 id=\"空间冗余（帧内预测）\"><a href=\"#空间冗余（帧内预测）\" class=\"headerlink\" title=\"空间冗余（帧内预测）\"></a>空间冗余（帧内预测）</h2><p>如果我们分析一个视频里的<strong>每一帧</strong>，我们会看到有<strong>许多区域是相互关联的</strong>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/repetitions_in_space.png\" alt=\"空间内重复\" title=\"空间内重复\"></p>\n<p>让我们举一个例子。这个场景大部分由蓝色和白色组成。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_bg.png\" alt=\"smw 背景\" title=\"smw 背景\"></p>\n<p>这是一个 <code>I 帧</code>，我们<strong>不能使用前面的帧来预测</strong>，但我们仍然可以压缩它。我们将编码我们选择的那块红色区域。如果我们<strong>看看它的周围</strong>，我们可以<strong>估计它周围颜色的变化</strong>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_bg_block.png\" alt=\"smw 背景块\" title=\"smw 背景块\"></p>\n<p>我们预测:帧中的颜色在垂直方向上保持一致，这意味着<strong>未知像素的颜色与临近的像素相同</strong>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_bg_prediction.png\" alt=\"smw 背景预测\" title=\"smw 背景预测\"></p>\n<p>我们的<strong>预测会出错</strong>，所以我们需要先利用这项技术（<strong>帧内预测</strong>），然后<strong>减去实际值</strong>，算出残差，得出的矩阵比原始数据更容易压缩。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_residual.png\" alt=\"smw 残差\" title=\"smw 残差\"></p>\n<blockquote>\n<h3 id=\"自己动手：查看帧内预测\"><a href=\"#自己动手：查看帧内预测\" class=\"headerlink\" title=\"自己动手：查看帧内预测\"></a>自己动手：查看帧内预测</h3><p>你可以<a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#generate-debug-video\" target=\"_blank\" rel=\"noopener\">使用 ffmpeg 生成包含宏块及预测的视频</a>。请查看 ffmpeg 文档以了解<a href=\"https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors\" target=\"_blank\" rel=\"noopener\">每个块颜色的含义</a>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/macro_blocks_ffmpeg.png\" alt=\"ffmpeg 帧内预测（宏块）\" title=\"ffmpeg 帧内预测（宏块）\"></p>\n<p>或者我们也可使用 <a href=\"https://software.intel.com/en-us/intel-video-pro-analyzer\" target=\"_blank\" rel=\"noopener\">Intel® Video Pro Analyzer</a>（需要付费，但也有只能查看前 10 帧的免费试用版）。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/intra_prediction_intel_video_pro_analyzer.png\" alt=\"Intel® Video Pro Analyzer 帧内预测\" title=\"Intel® Video Pro Analyzer 帧内预测\"></p>\n</blockquote>\n<h1 id=\"视频编解码器是如何工作的？\"><a href=\"#视频编解码器是如何工作的？\" class=\"headerlink\" title=\"视频编解码器是如何工作的？\"></a>视频编解码器是如何工作的？</h1><h2 id=\"是什么？为什么？怎么做？\"><a href=\"#是什么？为什么？怎么做？\" class=\"headerlink\" title=\"是什么？为什么？怎么做？\"></a>是什么？为什么？怎么做？</h2><p><strong>是什么？</strong> 就是用于压缩或解压数字视频的软件或硬件。<strong>为什么？</strong> 人们需要在有限带宽或存储空间下提高视频的质量。还记得当我们计算每秒 30 帧，每像素 24 bit，分辨率是 480x240 的视频<a href=\"#基本术语\">需要多少带宽</a>吗？没有压缩时是 <strong>82.944 Mbps</strong>。电视或互联网提供 HD/FullHD/4K 只能靠视频编解码器。<strong>怎么做？</strong> 我们将简单介绍一下主要的技术。</p>\n<blockquote>\n<p>视频编解码 vs 容器</p>\n<p>初学者一个常见的错误是混淆数字视频编解码器和<a href=\"https://en.wikipedia.org/wiki/Digital_container_format\" target=\"_blank\" rel=\"noopener\">数字视频容器</a>。我们可以将<strong>容器</strong>视为包含视频（也很可能包含音频）元数据的包装格式，<strong>压缩过的视频</strong>可以看成是它承载的内容。</p>\n<p>通常，视频文件的格式定义其视频容器。例如，文件 <code>video.mp4</code> 可能是 <a href=\"https://en.wikipedia.org/wiki/MPEG-4_Part_14\" target=\"_blank\" rel=\"noopener\">MPEG-4 Part 14</a> 容器，一个叫 <code>video.mkv</code> 的文件可能是 <a href=\"https://en.wikipedia.org/wiki/Matroska\" target=\"_blank\" rel=\"noopener\">matroska</a>。我们可以使用 <a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#inspect-stream\" target=\"_blank\" rel=\"noopener\">ffmpeg 或 mediainfo</a> 来完全确定编解码器和容器格式。</p>\n</blockquote>\n<h2 id=\"历史\"><a href=\"#历史\" class=\"headerlink\" title=\"历史\"></a>历史</h2><p>在我们跳进通用编解码器内部工作之前，让我们回头了解一些旧的视频编解码器。</p>\n<p>视频编解码器 <a href=\"https://en.wikipedia.org/wiki/H.261\" target=\"_blank\" rel=\"noopener\">H.261</a> 诞生在 1990（技术上是 1988），被设计为以 <strong>64 kbit/s 的数据速率</strong>工作。它已经使用如色度子采样、宏块，等等理念。在 1995 年，<strong>H.263</strong> 视频编解码器标准被发布，并继续延续到 2001 年。</p>\n<p>在 2003 年 <strong>H.264/AVC</strong> 的第一版被完成。在同一年，一家叫做 <strong>TrueMotion</strong> 的公司发布了他们的<strong>免版税</strong>有损视频压缩的视频编解码器，称为 <strong>VP3</strong>。在 2008 年，<strong>Google 收购了</strong>这家公司，在同一年发布 <strong>VP8</strong>。在 2012 年 12 月，Google 发布了 <strong>VP9</strong>，<strong>市面上大约有 3/4 的浏览器</strong>（包括手机）支持。</p>\n<p><a href=\"https://en.wikipedia.org/wiki/AOMedia_Video_1\" target=\"_blank\" rel=\"noopener\">AV1</a> 是由 <strong>Google, Mozilla, Microsoft, Amazon, Netflix, AMD, ARM, NVidia, Intel, Cisco</strong> 等公司组成的<a href=\"http://aomedia.org/\" target=\"_blank\" rel=\"noopener\">开放媒体联盟（AOMedia）</a>设计的一种新的视频编解码器，免版税，开源。<strong>第一版</strong> 0.1.0 参考编解码器<strong>发布于 2016 年 4 月 7 号</strong>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/codec_history_timeline.png\" alt=\"编解码器历史线路图\" title=\"编解码器历史线路图\"></p>\n<blockquote>\n<h3 id=\"AV1-的诞生\"><a href=\"#AV1-的诞生\" class=\"headerlink\" title=\"AV1 的诞生\"></a>AV1 的诞生</h3><p>2015 年早期，Google 正在 VP10 上工作，Xiph (Mozilla) 正在 Daala 上工作，Cisco 开源了它的称为 Thor 的免版税视频编解码器。</p>\n<p>接着 MPEG LA 宣布了 HEVC (H.265) 每年版税的的上限，比 H.264 高 8 倍，但很快他们又再次改变了条款：</p>\n<ul>\n<li><strong>不设年度收费上限</strong></li>\n<li><strong>收取内容费</strong>（收入的 0.5%）</li>\n<li><strong>每单位费用高于 h264 的 10 倍</strong></li>\n</ul>\n<p><a href=\"http://aomedia.org/about-us/\" target=\"_blank\" rel=\"noopener\">开放媒体联盟</a>由硬件厂商（Intel, AMD, ARM , Nvidia, Cisco），内容分发商（Google, Netflix, Amazon），浏览器维护者（Google, Mozilla），等公司创建。</p>\n<p>这些公司有一个共同目标，一个免版税的视频编解码器，所以 AV1 诞生时使用了一个更<a href=\"http://aomedia.org/license/patent/\" target=\"_blank\" rel=\"noopener\">简单的专利许可证</a>。<strong>Timothy B. Terriberry</strong> 做了一个精彩的介绍，<a href=\"https://www.youtube.com/watch?v=lzPaldsmJbk\" target=\"_blank\" rel=\"noopener\">关于 AV1 的概念，许可证模式和它当前的状态</a>，就是本节的来源。</p>\n<p>前往 <a href=\"http://aomanalyzer.org/\" target=\"_blank\" rel=\"noopener\">http://aomanalyzer.org/</a>， 你会惊讶于<strong>使用你的浏览器就可以分析 AV1 编解码器</strong>。<br><img src=\"/2020/02/28/Introduction-to-digital-video-technology/av1_browser_analyzer.png\" alt=\"av1 浏览器分析器\" title=\"浏览器分析器\"></p>\n<p>附：如果你想了解更多编解码器的历史，你需要了解<a href=\"https://www.vcodex.com/video-compression-patents/\" target=\"_blank\" rel=\"noopener\">视频压缩专利</a>背后的基本知识。</p>\n</blockquote>\n<h2 id=\"通用编解码器\"><a href=\"#通用编解码器\" class=\"headerlink\" title=\"通用编解码器\"></a>通用编解码器</h2><p>我们接下来要介绍<strong>通用视频编解码器背后的主要机制</strong>，大多数概念都很实用，并被现代编解码器如 VP9, AV1 和 HEVC 使用。需要注意：我们将简化许多内容。有时我们会使用真实的例子（主要是 H.264）来演示技术。</p>\n<h2 id=\"第一步-图片分区\"><a href=\"#第一步-图片分区\" class=\"headerlink\" title=\"第一步 - 图片分区\"></a>第一步 - 图片分区</h2><p>第一步是<strong>将帧</strong>分成几个<strong>分区</strong>，<strong>子分区</strong>甚至更多。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/picture_partitioning.png\" alt=\"图片分区\" title=\"图片分区\"></p>\n<p><strong>但是为什么呢？</strong>有许多原因，比如，当我们分割图片时，我们可以更精确的处理预测，在微小移动的部分使用较小的分区，而在静态背景上使用较大的分区。</p>\n<p>通常，编解码器<strong>将这些分区组织</strong>成切片（或瓦片），宏（或编码树单元）和许多子分区。这些分区的最大大小有所不同，HEVC 设置成 64x64，而 AVC 使用 16x16，但子分区可以达到 4x4 的大小。</p>\n<p>还记得我们学过的<strong>帧的分类</strong>吗？你也可以<strong>把这些概念应用到块</strong>，因此我们可以有 I 切片，B 切片，I 宏块等等。</p>\n<blockquote>\n<h3 id=\"自己动手：查看分区\"><a href=\"#自己动手：查看分区\" class=\"headerlink\" title=\"自己动手：查看分区\"></a>自己动手：查看分区</h3><p>我们也可以使用 <a href=\"https://software.intel.com/en-us/intel-video-pro-analyzer\" target=\"_blank\" rel=\"noopener\">Intel® Video Pro Analyzer</a>（需要付费，但也有只能查看前 10 帧的免费试用版）。这是 <a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#transcoding\" target=\"_blank\" rel=\"noopener\">VP9 分区</a>的分析。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/paritions_view_intel_video_pro_analyzer.png\" alt=\"Intel® Video Pro Analyzer VP9 分区视图 \" title=\"Intel® Video Pro Analyzer VP9 分区视图\"></p>\n</blockquote>\n<h2 id=\"第二步-预测\"><a href=\"#第二步-预测\" class=\"headerlink\" title=\"第二步 - 预测\"></a>第二步 - 预测</h2><p>一旦我们有了分区，我们就可以在它们之上做出预测。对于<a href=\"#时间冗余（帧间预测）\">帧间预测</a>，我们需要<strong>发送运动向量和残差</strong>；至于<a href=\"#空间冗余（帧内预测）\">帧内预测</a>，我们需要<strong>发送预测方向和残差</strong>。</p>\n<h2 id=\"第三步-转换\"><a href=\"#第三步-转换\" class=\"headerlink\" title=\"第三步 - 转换\"></a>第三步 - 转换</h2><p>在我们得到残差块（<code>预测分区-真实分区</code>）之后，我们可以用一种方式<strong>变换</strong>它，这样我们就知道<strong>哪些像素我们应该丢弃</strong>，还依然能保持<strong>整体质量</strong>。这个确切的行为有几种变换方式。</p>\n<p>尽管有<a href=\"https://en.wikipedia.org/wiki/List_of_Fourier-related_transforms#Discrete_transforms\" target=\"_blank\" rel=\"noopener\">其它的变换方式</a>，但我们重点关注离散余弦变换（DCT）。<a href=\"https://en.wikipedia.org/wiki/Discrete_cosine_transform\" target=\"_blank\" rel=\"noopener\">DCT</a> 的主要功能有：</p>\n<ul>\n<li>将<strong>像素</strong>块<strong>转换</strong>为相同大小的<strong>频率系数块</strong>。</li>\n<li><strong>压缩</strong>能量，更容易消除空间冗余。</li>\n<li><strong>可逆的</strong>，也意味着你可以还原回像素。</li>\n</ul>\n<blockquote>\n<p>2017 年 2 月 2 号，F. M. Bayer 和 R. J. Cintra 发表了他们的论文：<a href=\"https://arxiv.org/abs/1702.00817\" target=\"_blank\" rel=\"noopener\">图像压缩的 DCT 类变换只需要 14 个加法</a>。</p>\n</blockquote>\n<p>如果你不理解每个要点的好处，不用担心，我们会尝试进行一些实验，以便从中看到真正的价值。</p>\n<p>我们来看下面的<strong>像素块</strong>（8x8）：</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/pixel_matrice.png\" alt=\"像素值矩形\" title=\"像素值矩形\"></p>\n<p>下面是其渲染的块图像（8x8）：</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/gray_image.png\" alt=\"像素值矩形\" title=\"像素值矩形\"></p>\n<p>当我们对这个像素块<strong>应用 DCT</strong> 时， 得到如下<strong>系数块</strong>（8x8）：</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/dct_coefficient_values.png\" alt=\"系数值 values\" title=\"系数值\"></p>\n<p>接着如果我们渲染这个系数块，就会得到这张图片：</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/dct_coefficient_image.png\" alt=\"dct 系数图片\" title=\"dct 系数图片\"></p>\n<p>如你所见它看起来完全不像原图像，我们可能会注意到<strong>第一个系数</strong>与其它系数非常不同。第一个系数被称为直流分量，代表了输入数组中的<strong>所有样本</strong>，有点<strong>类似于平均值</strong>。</p>\n<p>这个系数块有一个有趣的属性：高频部分和低频部分是分离的。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/dctfrequ.jpg\" alt=\"dct 频率系数属性\" title=\"dct 频率系数属性\"></p>\n<p>在一张图像中，<strong>大多数能量</strong>会集中在<a href=\"https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm\" target=\"_blank\" rel=\"noopener\">低频部分</a>，所以如果我们将图像转换成频率系数，并<strong>丢掉高频系数</strong>，我们就能<strong>减少描述图像所需的数据量</strong>，而不会牺牲太多的图像质量。</p>\n<blockquote>\n<p>频率是指信号变化的速度。</p>\n</blockquote>\n<p>让我们通过实验学习这点，我们将使用 DCT 把原始图像转换为频率（系数块），然后丢掉最不重要的系数。</p>\n<p>首先，我们将它转换为其<strong>频域</strong>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/dct_coefficient_values.png\" alt=\"系数值\" title=\"系数值\"></p>\n<p>然后我们丢弃部分（67%）系数，主要是它的右下角部分。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/dct_coefficient_zeroed.png\" alt=\"系数清零\" title=\"系数清零\"></p>\n<p>然后我们从丢弃的系数块重构图像（记住，这需要可逆），并与原始图像相比较。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/original_vs_quantized.png\" alt=\"原始 vs 量化\" title=\"原始 vs 量化\"></p>\n<p>如我们所见它酷似原始图像，但它引入了许多与原来的不同，我们<strong>丢弃了67.1875%</strong>，但我们仍然得到至少类似于原来的东西。我们可以更加智能的丢弃系数去得到更好的图像质量，但这是下一个主题。</p>\n<blockquote>\n<h3 id=\"使用全部像素形成每个系数\"><a href=\"#使用全部像素形成每个系数\" class=\"headerlink\" title=\"使用全部像素形成每个系数\"></a>使用全部像素形成每个系数</h3><p>重要的是要注意，每个系数并不直接映射到单个像素，但它是所有像素的加权和。这个神奇的图形展示了如何计算出第一和第二个系数，使用每个唯一的索引做权重。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/applicat.jpg\" alt=\"dct 计算\" title=\"dct 计算\"></p>\n<p>来源：<a href=\"https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm\" target=\"_blank\" rel=\"noopener\">https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm</a></p>\n<p>你也可以尝试<a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/dct_better_explained.ipynb\" target=\"_blank\" rel=\"noopener\">通过查看在 DCT 基础上形成的简单图片来可视化 DCT</a>。例如，这是使用每个系数权重<a href=\"https://en.wikipedia.org/wiki/Discrete_cosine_transform#Example_of_IDCT\" target=\"_blank\" rel=\"noopener\">形成的字符 A</a>。</p>\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5e/Idct-animation.gif\" alt></p>\n</blockquote>\n<br>\n\n<blockquote>\n<h3 id=\"自己动手：丢弃不同的系数\"><a href=\"#自己动手：丢弃不同的系数\" class=\"headerlink\" title=\"自己动手：丢弃不同的系数\"></a>自己动手：丢弃不同的系数</h3><p>你可以玩转 <a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/uniform_quantization_experience.ipynb\" target=\"_blank\" rel=\"noopener\">DCT 变换</a></p>\n</blockquote>\n<h2 id=\"第四步-量化\"><a href=\"#第四步-量化\" class=\"headerlink\" title=\"第四步 - 量化\"></a>第四步 - 量化</h2><p>当我们丢弃一些系数时，在最后一步（变换），我们做了一些形式的量化。这一步，我们选择性地剔除信息（<strong>有损部分</strong>）或者简单来说，我们将<strong>量化系数以实现压缩</strong>。</p>\n<p>我们如何量化一个系数块？一个简单的方法是均匀量化，我们取一个块并<strong>将其除以单个的值</strong>（10），并舍入值。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/quantize.png\" alt=\"量化\" title=\"量化\"></p>\n<p>我们如何<strong>逆转</strong>（重新量化）这个系数块？我们可以通过<strong>乘以我们先前除以的相同的值</strong>（10）来做到。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/re-quantize.png\" alt=\"逆转量化\" title=\"逆转量化\"></p>\n<p>这<strong>不是最好的方法</strong>，因为它没有考虑到每个系数的重要性，我们可以使用一个<strong>量化矩阵</strong>来代替单个值，这个矩阵可以利用 DCT 的属性，多量化右下部，而少（量化）左上部，<a href=\"https://www.hdm-stuttgart.de/~maucher/Python/MMCodecs/html/jpegUpToQuant.html\" target=\"_blank\" rel=\"noopener\">JPEG 使用了类似的方法</a>，你可以通过<a href=\"https://github.com/google/guetzli/blob/master/guetzli/jpeg_data.h#L40\" target=\"_blank\" rel=\"noopener\">查看源码看看这个矩阵</a>。</p>\n<blockquote>\n<h3 id=\"自己动手：量化\"><a href=\"#自己动手：量化\" class=\"headerlink\" title=\"自己动手：量化\"></a>自己动手：量化</h3><p>你可以玩转<a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/dct_experiences.ipynb\" target=\"_blank\" rel=\"noopener\">量化</a></p>\n</blockquote>\n<h2 id=\"第五步-熵编码\"><a href=\"#第五步-熵编码\" class=\"headerlink\" title=\"第五步 - 熵编码\"></a>第五步 - 熵编码</h2><p>在我们量化数据（图像块／切片／帧）之后，我们仍然可以以无损的方式来压缩它。有许多方法（算法）可用来压缩数据。我们将简单体验其中几个，你可以阅读这本很棒的书去深入理解：<a href=\"https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/\" target=\"_blank\" rel=\"noopener\">Understanding Compression: Data Compression for Modern Developers</a>。</p>\n<h3 id=\"VLC-编码：\"><a href=\"#VLC-编码：\" class=\"headerlink\" title=\"VLC 编码：\"></a>VLC 编码：</h3><p>让我们假设我们有一个符号流：<strong>a</strong>, <strong>e</strong>, <strong>r</strong> 和 <strong>t</strong>，它们的概率（从0到1）由下表所示。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>a</th>\n<th>e</th>\n<th>r</th>\n<th>t</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>概率</td>\n<td>0.3</td>\n<td>0.3</td>\n<td>0.2</td>\n<td>0.2</td>\n</tr>\n</tbody></table>\n<p>我们可以分配不同的二进制码，（最好是）小的码给最可能（出现的字符），大些的码给最少可能（出现的字符）。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>a</th>\n<th>e</th>\n<th>r</th>\n<th>t</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>概率</td>\n<td>0.3</td>\n<td>0.3</td>\n<td>0.2</td>\n<td>0.2</td>\n</tr>\n<tr>\n<td>二进制码</td>\n<td>0</td>\n<td>10</td>\n<td>110</td>\n<td>1110</td>\n</tr>\n</tbody></table>\n<p>让我们压缩 <strong>eat</strong> 流，假设我们为每个字符花费 8 bit，在没有做任何压缩时我们将花费 <strong>24 bit</strong>。但是在这种情况下，我们使用各自的代码来替换每个字符，我们就能节省空间。</p>\n<p>第一步是编码字符 <strong>e</strong> 为 <code>10</code>，第二个字符是 <strong>a</strong>，追加（不是数学加法）后是 <code>[10][0]</code>，最后是第三个字符 <strong>t</strong>，最终组成已压缩的比特流 <code>[10][0][1110]</code> 或 <code>1001110</code>，这只需 <strong>7 bit</strong>（比原来的空间少 3.4 倍）。</p>\n<p>请注意每个代码必须是唯一的前缀码，<a href=\"https://en.wikipedia.org/wiki/Huffman_coding\" target=\"_blank\" rel=\"noopener\">Huffman 能帮你找到这些数字</a>。虽然它有一些问题，但是<a href=\"https://en.wikipedia.org/wiki/Context-adaptive_variable-length_coding\" target=\"_blank\" rel=\"noopener\">视频编解码器仍然提供该方法</a>，它也是很多应用程序的压缩算法。</p>\n<p>编码器和解码器都<strong>必须知道</strong>这个（包含编码的）字符表，因此，你也需要传送这个表。</p>\n<h3 id=\"算术编码\"><a href=\"#算术编码\" class=\"headerlink\" title=\"算术编码\"></a>算术编码</h3><p>让我们假设我们有一个符号流：<strong>a</strong>, <strong>e</strong>, <strong>r</strong>, <strong>s</strong> 和 <strong>t</strong>，它们的概率由下表所示。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>a</th>\n<th>e</th>\n<th>r</th>\n<th>s</th>\n<th>t</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>概率</td>\n<td>0.3</td>\n<td>0.3</td>\n<td>0.15</td>\n<td>0.05</td>\n<td>0.2</td>\n</tr>\n</tbody></table>\n<p>考虑到这个表，我们可以构建一个区间，区间包含了所有可能的字符，字符按出现概率排序。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/range.png\" alt=\"初始算法区间\" title=\"初始算法区间\"></p>\n<p>让我们编码 <strong>eat</strong> 流，我们选择第一个字符 <strong>e</strong> 位于 <strong>0.3 到 0.6</strong> （但不包括 0.6）的子区间，我们选择这个子区间，按照之前同等的比例再次分割。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/second_subrange.png\" alt=\"第二个子区间\" title=\"第二个子区间\"></p>\n<p>让我们继续编码我们的流 <strong>eat</strong>，现在使第二个 <strong>a</strong> 字符位于 <strong>0.3 到 0.39</strong> 的区间里，接着再次用同样的方法编码最后的字符 <strong>t</strong>，得到最后的子区间 <strong>0.354 到 0.372</strong>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/arithimetic_range.png\" alt=\"最终算法区间\" title=\"最终算法区间\"></p>\n<p>我们只需从最后的子区间 0.354 到 0.372 里选择一个数，让我们选择 0.36，不过我们可以选择这个子区间里的任何数。仅靠这个数，我们将可以恢复原始流 <strong>eat</strong>。就像我们在区间的区间里画了一根线来编码我们的流。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/range_show.png\" alt=\"最终区间横断面\" title=\"最终区间横断面\"></p>\n<p><strong>反向过程</strong>（又名解码）一样简单，用数字 <strong>0.36</strong> 和我们原始区间，我们可以进行同样的操作，不过现在是使用这个数字来还原被编码的流。</p>\n<p>在第一个区间，我们发现数字落入了一个子区间，因此，这个子区间是我们的第一个字符，现在我们再次切分这个子区间，像之前一样做同样的过程。我们会注意到 <strong>0.36</strong> 落入了 <strong>a</strong> 的区间，然后我们重复这一过程直到得到最后一个字符 <strong>t</strong>（形成我们原始编码过的流 eat）。</p>\n<p>编码器和解码器都<strong>必须知道</strong>字符概率表，因此，你也需要传送这个表。</p>\n<p>非常巧妙，不是吗？人们能想出这样的解决方案实在是太聪明了，一些<a href=\"https://en.wikipedia.org/wiki/Context-adaptive_binary_arithmetic_coding\" target=\"_blank\" rel=\"noopener\">视频编解码器使用</a>这项技术（或至少提供这一选择）。</p>\n<p>关于无损压缩量化比特流的办法，这篇文章无疑缺少了很多细节、原因、权衡等等。作为一个开发者你<a href=\"https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/\" target=\"_blank\" rel=\"noopener\">应该学习更多</a>。刚入门视频编码的人可以尝试使用不同的<a href=\"https://en.wikipedia.org/wiki/Asymmetric_Numeral_Systems\" target=\"_blank\" rel=\"noopener\">熵编码算法，如ANS</a>。</p>\n<blockquote>\n<h3 id=\"自己动手：CABAC-vs-CAVLC\"><a href=\"#自己动手：CABAC-vs-CAVLC\" class=\"headerlink\" title=\"自己动手：CABAC vs CAVLC\"></a>自己动手：CABAC vs CAVLC</h3><p>你可以<a href=\"https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#cabac-vs-cavlc\" target=\"_blank\" rel=\"noopener\">生成两个流，一个使用 CABAC，另一个使用 CAVLC</a>，并比较生成每一个的时间以及最终的大小。</p>\n</blockquote>\n<h2 id=\"第六步-比特流格式\"><a href=\"#第六步-比特流格式\" class=\"headerlink\" title=\"第六步 - 比特流格式\"></a>第六步 - 比特流格式</h2><p>完成所有这些步之后，我们需要将<strong>压缩过的帧和内容打包进去</strong>。需要明确告知解码器<strong>编码定义</strong>，如颜色深度，颜色空间，分辨率，预测信息（运动向量，帧内预测方向），配置<sup>*</sup>，层级<sup>*</sup>，帧率，帧类型，帧号等等更多信息。</p>\n<blockquote>\n<p><sup>*</sup> 译注：原文为 profile 和 level，没有通用的译名</p>\n</blockquote>\n<p>我们将简单地学习 H.264 比特流。第一步是<a href=\"https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#generate-a-single-frame-h264-bitstream\" target=\"_blank\" rel=\"noopener\">生成一个小的 H.264<sup>*</sup> 比特流</a>，可以使用本 repo 和 <a href=\"http://ffmpeg.org/\" target=\"_blank\" rel=\"noopener\">ffmpeg</a> 来做。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.&#x2F;s&#x2F;ffmpeg -i &#x2F;files&#x2F;i&#x2F;minimal.png -pix_fmt yuv420p &#x2F;files&#x2F;v&#x2F;minimal_yuv420.h264</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p><sup>*</sup> ffmpeg 默认将所有参数添加为 <strong>SEI NAL</strong>，很快我们会定义什么是 NAL。</p>\n</blockquote>\n<p>这个命令会使用下面的图片作为帧，生成一个具有<strong>单个帧</strong>，64x64 和颜色空间为 yuv420 的原始 h264 比特流。</p>\n<blockquote>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/minimal.png\" alt=\"使用帧来生成极简 h264 比特流\" title=\"使用帧来生成极简 h264 比特流\"></p>\n</blockquote>\n<h3 id=\"H-264-比特流\"><a href=\"#H-264-比特流\" class=\"headerlink\" title=\"H.264 比特流\"></a>H.264 比特流</h3><p>AVC (H.264) 标准规定信息将在宏帧（网络概念上的）内传输，称为 <a href=\"https://en.wikipedia.org/wiki/Network_Abstraction_Layer\" target=\"_blank\" rel=\"noopener\">NAL</a>（网络抽象层）。NAL 的主要目标是提供“网络友好”的视频呈现方式，该标准必须适用于电视（基于流），互联网（基于数据包）等。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/nal_units.png\" alt=\"H.264 NAL 单元\" title=\"H.264 NAL 单元\"></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Frame_synchronization\" target=\"_blank\" rel=\"noopener\">同步标记</a>用来定义 NAL 单元的边界。每个同步标记的值固定为  <code>0x00 0x00 0x01</code> ，最开头的标记例外，它的值是  <code>0x00 0x00 0x00 0x01</code> 。如果我们在生成的 h264 比特流上运行 <strong>hexdump</strong>，我们可以在文件的开头识别至少三个 NAL。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/minimal_yuv420_hex.png\" alt=\"NAL 单元上的同步标记\" title=\"NAL 单元上的同步标记\"></p>\n<p>我们之前说过，解码器需要知道不仅仅是图片数据，还有视频的详细信息，如：帧、颜色、使用的参数等。每个 NAL 的<strong>第一位</strong>定义了其分类和<strong>类型</strong>。</p>\n<table>\n<thead>\n<tr>\n<th>NAL type id</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>0</td>\n<td>Undefined</td>\n</tr>\n<tr>\n<td>1</td>\n<td>Coded slice of a non-IDR picture</td>\n</tr>\n<tr>\n<td>2</td>\n<td>Coded slice data partition A</td>\n</tr>\n<tr>\n<td>3</td>\n<td>Coded slice data partition B</td>\n</tr>\n<tr>\n<td>4</td>\n<td>Coded slice data partition C</td>\n</tr>\n<tr>\n<td>5</td>\n<td><strong>IDR</strong> Coded slice of an IDR picture</td>\n</tr>\n<tr>\n<td>6</td>\n<td><strong>SEI</strong> Supplemental enhancement information</td>\n</tr>\n<tr>\n<td>7</td>\n<td><strong>SPS</strong> Sequence parameter set</td>\n</tr>\n<tr>\n<td>8</td>\n<td><strong>PPS</strong> Picture parameter set</td>\n</tr>\n<tr>\n<td>9</td>\n<td>Access unit delimiter</td>\n</tr>\n<tr>\n<td>10</td>\n<td>End of sequence</td>\n</tr>\n<tr>\n<td>11</td>\n<td>End of stream</td>\n</tr>\n<tr>\n<td>…</td>\n<td>…</td>\n</tr>\n</tbody></table>\n<p>通常，比特流的第一个 NAL 是 <strong>SPS</strong>，这个类型的 NAL 负责传达通用编码参数，如<strong>配置，层级，分辨率</strong>等。</p>\n<p>如果我们跳过第一个同步标记，就可以通过解码<strong>第一个字节</strong>来了解第一个 <strong>NAL 的类型</strong>。</p>\n<p>例如同步标记之后的第一个字节是 <code>01100111</code>，第一位（<code>0</code>）是 <strong>forbidden_zero_bit</strong> 字段，接下来的两位（<code>11</code>）告诉我们是 <strong>nal_ref_idc</strong> 字段，其表示该 NAL 是否是参考字段，其余 5 位（<code>00111</code>）告诉我们是 <strong>nal_unit_type</strong> 字段，在这个例子里是 NAL 单元 <strong>SPS</strong> (7)。</p>\n<p>SPS NAL 的第 2 位 (<code>binary=01100100, hex=0x64, dec=100</code>) 是 <strong>profile_idc</strong> 字段，显示编码器使用的配置，在这个例子里，我们使用<a href=\"https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Profiles\" target=\"_blank\" rel=\"noopener\">受限高配置</a>，一种没有 B（双向预测） 切片支持的高配置。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/minimal_yuv420_bin.png\" alt=\"SPS 二进制视图\" title=\"SPS 二进制视图\"></p>\n<p>当我们阅读 SPS NAL 的 H.264 比特流规范时，会为<strong>参数名称</strong>，<strong>分类</strong>和<strong>描述</strong>找到许多值，例如，看看字段 <code>pic_width_in_mbs_minus_1</code> 和 <code>pic_height_in_map_units_minus_1</code>。</p>\n<table>\n<thead>\n<tr>\n<th>参数名称</th>\n<th>分类</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>pic_width_in_mbs_minus_1</td>\n<td>0</td>\n<td>ue(v)</td>\n</tr>\n<tr>\n<td>pic_height_in_map_units_minus_1</td>\n<td>0</td>\n<td>ue(v)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>ue(v)</strong>: 无符号整形 <a href=\"https://pythonhosted.org/bitstring/exp-golomb.html\" target=\"_blank\" rel=\"noopener\">Exp-Golomb-coded</a></p>\n</blockquote>\n<p>如果我们对这些字段的值进行一些计算，将最终得出<strong>分辨率</strong>。我们可以使用值为 <code>119（ (119 + 1) * macroblock_size = 120 * 16 = 1920）</code>的 <code>pic_width_in_mbs_minus_1</code> 表示 <code>1920 x 1080</code>，再次为了减少空间，我们使用 <code>119</code> 来代替编码 <code>1920</code>。</p>\n<p>如果我们再次使用二进制视图检查我们创建的视频 (ex: <code>xxd -b -c 11 v/minimal_yuv420.h264</code>)，可以跳到帧自身上一个 NAL。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/slice_nal_idr_bin.png\" alt=\"h264 idr 切片头\" title=\"h264 idr 切片头\"></p>\n<p>我们可以看到最开始的 6 个字节：<code>01100101 10001000 10000100 00000000 00100001 11111111</code>。我们已经知道第一个字节告诉我们 NAL 的类型，在这个例子里， (<code>00101</code>) 是 <strong>IDR 切片 (5)</strong>，可以进一步检查它：</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/slice_header.png\" alt=\"h264 切片头规格\" title=\"h264 切片头规格\"></p>\n<p>对照规范，我们能解码切片的类型（<strong>slice_type</strong>），帧号（<strong>frame_num</strong>）等重要字段。</p>\n<p>为了获得一些字段（<code>ue(v), me(v), se(v) 或 te(v)</code>）的值，我们需要称为 <a href=\"https://pythonhosted.org/bitstring/exp-golomb.html\" target=\"_blank\" rel=\"noopener\">Exponential-Golomb</a> 的特定解码器来解码它。当存在很多默认值时，这个方法编码变量值特别高效。</p>\n<blockquote>\n<p>这个视频里 <strong>slice_type</strong> 和 <strong>frame_num</strong> 的值是 7（I 切片）和 0（第一帧）。</p>\n</blockquote>\n<p>我们可以将<strong>比特流视为一个协议</strong>，如果你想学习更多关于比特流的内容，请参考 <a href=\"http://www.itu.int/rec/T-REC-H.264-201610-I\" target=\"_blank\" rel=\"noopener\">ITU H.264 规范</a>。这个宏观图展示了图片数据（压缩过的 YUV）所在的位置。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/h264_bitstream_macro_diagram.png\" alt=\"h264 比特流宏观图\" title=\"h264 比特流宏观图\"></p>\n<p>我们可以探究其它比特流，如 <a href=\"https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf\" target=\"_blank\" rel=\"noopener\">VP9 比特流</a>，<a href=\"http://handle.itu.int/11.1002/1000/11885-en?locatt=format:pdf\" target=\"_blank\" rel=\"noopener\">H.265（HEVC）</a>或是我们的新朋友 <a href=\"https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8\" target=\"_blank\" rel=\"noopener\">AV1 比特流</a>，<a href=\"http://www.gpac-licensing.com/2016/07/12/vp9-av1-bitstream-format/\" target=\"_blank\" rel=\"noopener\">他们很相似吗？不</a>，但只要学习了其中之一，学习其他的就简单多了。</p>\n<blockquote>\n<h3 id=\"自己动手：检查-H-264-比特流\"><a href=\"#自己动手：检查-H-264-比特流\" class=\"headerlink\" title=\"自己动手：检查 H.264 比特流\"></a>自己动手：检查 H.264 比特流</h3><p>我们可以<a href=\"https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#generate-a-single-frame-video\" target=\"_blank\" rel=\"noopener\">生成一个单帧视频</a>，使用 <a href=\"https://en.wikipedia.org/wiki/MediaInfo\" target=\"_blank\" rel=\"noopener\">mediainfo</a> 检查它的 H.264 比特流。事实上，你甚至可以查看<a href=\"https://github.com/MediaArea/MediaInfoLib/blob/master/Source/MediaInfo/Video/File_Avc.cpp\" target=\"_blank\" rel=\"noopener\">解析 h264(AVC) 视频流的源代码</a>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/mediainfo_details_1.png\" alt=\"mediainfo h264 比特流的详情 \" title=\"mediainfo h264 比特流的详情\"></p>\n<p>我们也可使用 <a href=\"https://software.intel.com/en-us/intel-video-pro-analyzer\" target=\"_blank\" rel=\"noopener\">Intel® Video Pro Analyzer</a>，需要付费，但也有只能查看前 10 帧的免费试用版，这已经够达成学习目的了。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/intel-video-pro-analyzer.png\" alt=\"Intel® Video Pro Analyzer h264 比特流的详情\" title=\"Intel® Video Pro Analyzer h264 比特流的详情\"></p>\n</blockquote>\n<h2 id=\"回顾\"><a href=\"#回顾\" class=\"headerlink\" title=\"回顾\"></a>回顾</h2><p>我们可以看到我们学了许多<strong>使用相同模型的现代编解码器</strong>。事实上，让我们看看 Thor 视频编解码器框图，它包含所有我们学过的步骤。你现在应该能更好地理解数字视频领域内的创新和论文。<br><img src=\"/2020/02/28/Introduction-to-digital-video-technology/thor_codec_block_diagram.png\" alt=\"thor 编解码器块图\" title=\"thor 编解码器块图\"></p>\n<p>之前我们计算过我们<a href=\"#色度子采样\">需要 139GB 来保存一个一小时，720p 分辨率和30fps的视频文件</a>，如果我们使用在这里学过的技术，如<strong>帧间和帧内预测，转换，量化，熵编码和其它</strong>我们能实现——假设我们<strong>每像素花费 0.031 bit</strong>——同样观感质量的视频，<strong>对比 139GB 的存储，只需 367.82MB</strong>。</p>\n<blockquote>\n<p>我们根据这里提供的示例视频选择<strong>每像素使用 0.031 bit</strong>。</p>\n</blockquote>\n<h2 id=\"H-265-如何实现比-H-264-更好的压缩率\"><a href=\"#H-265-如何实现比-H-264-更好的压缩率\" class=\"headerlink\" title=\"H.265 如何实现比 H.264 更好的压缩率\"></a>H.265 如何实现比 H.264 更好的压缩率</h2><p>我们已经更多地了解了编解码器的工作原理，那么就容易理解新的编解码器如何使用更少的数据量传输更高分辨率的视频。</p>\n<p>我们将比较 AVC 和 HEVC，要记住的是：我们几乎总是要在压缩率和更多的 CPU 周期（复杂度）之间作权衡。</p>\n<p>HEVC 比 AVC 有更大和更多的<strong>分区</strong>（和<strong>子分区</strong>）选项，更多<strong>帧内预测方向</strong>，<strong>改进的熵编码</strong>等，所有这些改进使得 H.265 比 H.264 的压缩率提升 50%。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/avc_vs_hevc.png\" alt=\"h264 vs h265\" title=\"H.264 vs H.265\"></p>\n<h1 id=\"在线流媒体\"><a href=\"#在线流媒体\" class=\"headerlink\" title=\"在线流媒体\"></a>在线流媒体</h1><h2 id=\"通用架构\"><a href=\"#通用架构\" class=\"headerlink\" title=\"通用架构\"></a>通用架构</h2><p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/general_architecture.png\" alt=\"general_architecture\"></p>\n<p>[TODO]</p>\n<h2 id=\"渐进式下载和自适应流\"><a href=\"#渐进式下载和自适应流\" class=\"headerlink\" title=\"渐进式下载和自适应流\"></a>渐进式下载和自适应流</h2><p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/progressive_download.png\" alt=\"progressive_download\"></p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/adaptive_streaming.png\" alt=\"adaptive_streaming\"></p>\n<p>[TODO]</p>\n<h2 id=\"内容保护\"><a href=\"#内容保护\" class=\"headerlink\" title=\"内容保护\"></a>内容保护</h2><p>我们可以用一个简单的令牌认证系统来保护视频。用户需要拥有一个有效的令牌才可以播放视频，CDN 会拒绝没有令牌的用户的请求。它与大多数网站的身份认证系统非常相似。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/token_protection.png\" alt=\"token_protection\"></p>\n<p>仅仅使用令牌认证系统，用户仍然可以下载并重新分发视频。DRM 系统可以用来避免这种情况。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/drm.png\" alt=\"drm\"></p>\n<p>实际情况下，人们通常同时使用这两种技术提供授权和认证。</p>\n<h3 id=\"DRM\"><a href=\"#DRM\" class=\"headerlink\" title=\"DRM\"></a>DRM</h3><h4 id=\"主要系统\"><a href=\"#主要系统\" class=\"headerlink\" title=\"主要系统\"></a>主要系统</h4><ul>\n<li>FPS - <a href=\"https://developer.apple.com/streaming/fps/\" target=\"_blank\" rel=\"noopener\"><strong>FairPlay Streaming</strong></a></li>\n<li>PR - <a href=\"https://www.microsoft.com/playready/\" target=\"_blank\" rel=\"noopener\"><strong>PlayReady</strong></a></li>\n<li>WV - <a href=\"http://www.widevine.com/\" target=\"_blank\" rel=\"noopener\"><strong>Widevine</strong></a></li>\n</ul>\n<h4 id=\"是什么\"><a href=\"#是什么\" class=\"headerlink\" title=\"是什么\"></a>是什么</h4><p>DRM 指的是数字版权管理，是一种<strong>为数字媒体提供版权保护</strong>的方法，例如数字视频和音频。尽管用在了很多场合，但它并<a href=\"https://en.wikipedia.org/wiki/Digital_rights_management#DRM-free_works\" target=\"_blank\" rel=\"noopener\">没有被普遍接受</a>.</p>\n<h4 id=\"为什么\"><a href=\"#为什么\" class=\"headerlink\" title=\"为什么\"></a>为什么</h4><p>内容的创作者（大多是工作室/制片厂）希望保护他们的知识产权，使他们的数字媒体免遭未经授权的分发。</p>\n<h4 id=\"怎么做\"><a href=\"#怎么做\" class=\"headerlink\" title=\"怎么做\"></a>怎么做</h4><p>我们将用一种简单的、抽象的方式描述 DRM</p>\n<p>现有一份<strong>内容 C1</strong>（如 HLS 或 DASH 视频流），一个<strong>播放器 P1</strong>（如 shaka-clappr, exo-player 或 iOS），装在<strong>设备 D1</strong>（如智能手机、电视或台式机/笔记本）上，使用 <strong>DRM 系统 DRM1</strong>（如 FairPlay Streaming, PlayReady, Widevine）</p>\n<p>内容 C1 由 DRM1 用一个<strong>对称密钥 K1</strong> 加密，生成<strong>加密内容 C’1</strong></p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/drm_general_flow.jpeg\" alt=\"DRM 一般流程\" title=\"DRM 一般流程\"></p>\n<p>设备 D1 上的播放器 P1 有一个非对称密钥对，密钥对包含一个<strong>私钥 PRK1</strong>（这个密钥是受保护的<sup>1</sup>，只有 <strong>D1</strong> 知道密钥内容），和一个<strong>公钥 PUK1</strong></p>\n<blockquote>\n<p><strong><sup>1</sup>受保护的</strong>: 这种保护可以<strong>通过硬件</strong>进行保护，例如, 将这个密钥存储在一个特殊的芯片（只读）中，芯片的工作方式就像一个用来解密的[黑箱]。 或<strong>通过软件</strong>进行保护（较低的安全系数）。DRM 系统提供了识别设备所使用的保护类型的方法。</p>\n</blockquote>\n<p>当 <strong>播放器 P1 希望播放**</strong>加密内容 C’1** 时，它需要与 <strong>DRM1</strong> 协商，将公钥 <strong>PUK1</strong> 发送给 DRM1, DRM1 会返回一个被公钥 <strong>PUK1</strong> <strong>加密过的 K1</strong>。按照推论，结果就是<strong>只有 D1 能够解密</strong>。</p>\n<p><code>K1P1D1 = enc(K1, PUK1)</code></p>\n<p><strong>P1</strong> 使用它的本地 DRM 系统（这可以使用 <a href=\"https://zh.wikipedia.org/wiki/系统芯片\" target=\"_blank\" rel=\"noopener\">SoC</a> ，一个专门的硬件和软件，这个系统可以使用它的私钥 PRK1 用来<strong>解密</strong>内容，它可以解密被加密过的<strong>K1P1D1 的对称密钥 K1</strong>。理想情况下，密钥不会被导出到内存以外的地方。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">K1 &#x3D; dec(K1P1D1, PRK1)</span><br><span class=\"line\"></span><br><span class=\"line\">P1.play(dec(C&#39;1, K1))</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/drm_decoder_flow.jpeg\" alt=\"DRM 解码流程\" title=\"DRM 解码流程\"></p>\n<h1 id=\"如何使用-jupyter\"><a href=\"#如何使用-jupyter\" class=\"headerlink\" title=\"如何使用 jupyter\"></a>如何使用 jupyter</h1><p>确保你已安装 docker，只需运行 <code>./s/start_jupyter.sh</code>，然后按照控制台的说明进行操作。</p>\n<h1 id=\"会议\"><a href=\"#会议\" class=\"headerlink\" title=\"会议\"></a>会议</h1><ul>\n<li><a href=\"https://demuxed.com/\" target=\"_blank\" rel=\"noopener\">DEMUXED</a> - 您可以<a href=\"https://www.youtube.com/channel/UCIc_DkRxo9UgUSTvWVNCmpA\" target=\"_blank\" rel=\"noopener\">查看最近的2个活动演示</a>。</li>\n</ul>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p>这里有最丰富的资源，这篇文档包含的信息，均摘录、依据或受它们启发。你可以用这些精彩的链接，书籍，视频等深化你的知识。</p>\n<p>在线课程和教程：</p>\n<ul>\n<li><a href=\"https://www.coursera.org/learn/digital/\" target=\"_blank\" rel=\"noopener\">https://www.coursera.org/learn/digital/</a></li>\n<li><a href=\"https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf\" target=\"_blank\" rel=\"noopener\">https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf</a></li>\n<li><a href=\"https://xiph.org/video/vid1.shtml\" target=\"_blank\" rel=\"noopener\">https://xiph.org/video/vid1.shtml</a></li>\n<li><a href=\"https://xiph.org/video/vid2.shtml\" target=\"_blank\" rel=\"noopener\">https://xiph.org/video/vid2.shtml</a></li>\n<li><a href=\"http://slhck.info/ffmpeg-encoding-course\" target=\"_blank\" rel=\"noopener\">http://slhck.info/ffmpeg-encoding-course</a></li>\n<li><a href=\"http://www.cambridgeincolour.com/tutorials/camera-sensors.htm\" target=\"_blank\" rel=\"noopener\">http://www.cambridgeincolour.com/tutorials/camera-sensors.htm</a></li>\n<li><a href=\"http://www.slideshare.net/vcodex/a-short-history-of-video-coding\" target=\"_blank\" rel=\"noopener\">http://www.slideshare.net/vcodex/a-short-history-of-video-coding</a></li>\n<li><a href=\"http://www.slideshare.net/vcodex/introduction-to-video-compression-13394338\" target=\"_blank\" rel=\"noopener\">http://www.slideshare.net/vcodex/introduction-to-video-compression-1339433</a></li>\n<li><a href=\"https://developer.android.com/guide/topics/media/media-formats.html\" target=\"_blank\" rel=\"noopener\">https://developer.android.com/guide/topics/media/media-formats.html</a></li>\n<li><a href=\"http://www.slideshare.net/MadhawaKasun/audio-compression-23398426\" target=\"_blank\" rel=\"noopener\">http://www.slideshare.net/MadhawaKasun/audio-compression-23398426</a></li>\n<li><a href=\"http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf\" target=\"_blank\" rel=\"noopener\">http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf</a></li>\n</ul>\n<p>书籍:</p>\n<ul>\n<li><a href=\"https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&ie=UTF8&qid=1486395327&sr=1-1\" target=\"_blank\" rel=\"noopener\">https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1486395327&amp;sr=1-1</a></li>\n<li><a href=\"https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925\" target=\"_blank\" rel=\"noopener\">https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925</a></li>\n<li><a href=\"https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&ie=UTF8&qid=1486396914&sr=1-3&keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO\" target=\"_blank\" rel=\"noopener\">https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&amp;ie=UTF8&amp;qid=1486396914&amp;sr=1-3&amp;keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO</a></li>\n<li><a href=\"https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&ie=UTF8&qid=1486396940&sr=1-1&keywords=jan+ozer\" target=\"_blank\" rel=\"noopener\">https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1486396940&amp;sr=1-1&amp;keywords=jan+ozer</a></li>\n</ul>\n<p>比特流规范:</p>\n<ul>\n<li><a href=\"http://www.itu.int/rec/T-REC-H.264-201610-I\" target=\"_blank\" rel=\"noopener\">http://www.itu.int/rec/T-REC-H.264-201610-I</a></li>\n<li><a href=\"http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&lang=en\" target=\"_blank\" rel=\"noopener\">http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&amp;lang=en</a></li>\n<li><a href=\"https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf\" target=\"_blank\" rel=\"noopener\">https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf</a></li>\n<li><a href=\"http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf\" target=\"_blank\" rel=\"noopener\">http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf</a></li>\n<li><a href=\"http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243\" target=\"_blank\" rel=\"noopener\">http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243</a></li>\n<li><a href=\"http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html\" target=\"_blank\" rel=\"noopener\">http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html</a></li>\n</ul>\n<p>软件:</p>\n<ul>\n<li><a href=\"https://ffmpeg.org/\" target=\"_blank\" rel=\"noopener\">https://ffmpeg.org/</a></li>\n<li><a href=\"https://ffmpeg.org/ffmpeg-all.html\" target=\"_blank\" rel=\"noopener\">https://ffmpeg.org/ffmpeg-all.html</a></li>\n<li><a href=\"https://ffmpeg.org/ffprobe.html\" target=\"_blank\" rel=\"noopener\">https://ffmpeg.org/ffprobe.html</a></li>\n<li><a href=\"https://trac.ffmpeg.org/wiki/\" target=\"_blank\" rel=\"noopener\">https://trac.ffmpeg.org/wiki/</a></li>\n<li><a href=\"https://software.intel.com/en-us/intel-video-pro-analyzer\" target=\"_blank\" rel=\"noopener\">https://software.intel.com/en-us/intel-video-pro-analyzer</a></li>\n<li><a href=\"https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8\" target=\"_blank\" rel=\"noopener\">https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8</a></li>\n</ul>\n<p>非-ITU 编解码器:</p>\n<ul>\n<li><a href=\"https://aomedia.googlesource.com/\" target=\"_blank\" rel=\"noopener\">https://aomedia.googlesource.com/</a></li>\n<li><a href=\"https://github.com/webmproject/libvpx/tree/master/vp9\" target=\"_blank\" rel=\"noopener\">https://github.com/webmproject/libvpx/tree/master/vp9</a></li>\n<li><a href=\"https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml\" target=\"_blank\" rel=\"noopener\">https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml</a></li>\n<li><a href=\"https://people.xiph.org/~jm/daala/revisiting/\" target=\"_blank\" rel=\"noopener\">https://people.xiph.org/~jm/daala/revisiting/</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=lzPaldsmJbk\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=lzPaldsmJbk</a></li>\n<li><a href=\"https://fosdem.org/2017/schedule/event/om_av1/\" target=\"_blank\" rel=\"noopener\">https://fosdem.org/2017/schedule/event/om_av1/</a></li>\n</ul>\n<p>编码概念:</p>\n<ul>\n<li><a href=\"http://x265.org/hevc-h265/\" target=\"_blank\" rel=\"noopener\">http://x265.org/hevc-h265/</a></li>\n<li><a href=\"http://slhck.info/video/2017/03/01/rate-control.html\" target=\"_blank\" rel=\"noopener\">http://slhck.info/video/2017/03/01/rate-control.html</a></li>\n<li><a href=\"http://slhck.info/video/2017/02/24/vbr-settings.html\" target=\"_blank\" rel=\"noopener\">http://slhck.info/video/2017/02/24/vbr-settings.html</a></li>\n<li><a href=\"http://slhck.info/video/2017/02/24/crf-guide.html\" target=\"_blank\" rel=\"noopener\">http://slhck.info/video/2017/02/24/crf-guide.html</a></li>\n<li><a href=\"https://arxiv.org/pdf/1702.00817v1.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1702.00817v1.pdf</a></li>\n<li><a href=\"https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors\" target=\"_blank\" rel=\"noopener\">https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors</a></li>\n<li><a href=\"http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html\" target=\"_blank\" rel=\"noopener\">http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html</a></li>\n<li><a href=\"http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html\" target=\"_blank\" rel=\"noopener\">http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html</a></li>\n<li><a href=\"https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/\" target=\"_blank\" rel=\"noopener\">https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/</a></li>\n</ul>\n<p>测试用视频序列:</p>\n<ul>\n<li><a href=\"http://bbb3d.renderfarming.net/download.html\" target=\"_blank\" rel=\"noopener\">http://bbb3d.renderfarming.net/download.html</a></li>\n<li><a href=\"https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx\" target=\"_blank\" rel=\"noopener\">https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx</a></li>\n</ul>\n<p>杂项:</p>\n<ul>\n<li><a href=\"http://stackoverflow.com/a/24890903\" target=\"_blank\" rel=\"noopener\">http://stackoverflow.com/a/24890903</a></li>\n<li><a href=\"http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264\" target=\"_blank\" rel=\"noopener\">http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264</a></li>\n<li><a href=\"http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html\" target=\"_blank\" rel=\"noopener\">http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html</a></li>\n<li><a href=\"http://vanseodesign.com/web-design/color-luminance/\" target=\"_blank\" rel=\"noopener\">http://vanseodesign.com/web-design/color-luminance/</a></li>\n<li><a href=\"http://www.biologymad.com/nervoussystem/eyenotes.htm\" target=\"_blank\" rel=\"noopener\">http://www.biologymad.com/nervoussystem/eyenotes.htm</a></li>\n<li><a href=\"http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf\" target=\"_blank\" rel=\"noopener\">http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf</a></li>\n<li><a href=\"http://www.csc.villanova.edu/~rschumey/csc4800/dct.html\" target=\"_blank\" rel=\"noopener\">http://www.csc.villanova.edu/~rschumey/csc4800/dct.html</a></li>\n<li><a href=\"http://www.explainthatstuff.com/digitalcameras.html\" target=\"_blank\" rel=\"noopener\">http://www.explainthatstuff.com/digitalcameras.html</a></li>\n<li><a href=\"http://www.hkvstar.com\" target=\"_blank\" rel=\"noopener\">http://www.hkvstar.com</a></li>\n<li><a href=\"http://www.hometheatersound.com/\" target=\"_blank\" rel=\"noopener\">http://www.hometheatersound.com/</a></li>\n<li><a href=\"http://www.lighterra.com/papers/videoencodingh264/\" target=\"_blank\" rel=\"noopener\">http://www.lighterra.com/papers/videoencodingh264/</a></li>\n<li><a href=\"http://www.red.com/learn/red-101/video-chroma-subsampling\" target=\"_blank\" rel=\"noopener\">http://www.red.com/learn/red-101/video-chroma-subsampling</a></li>\n<li><a href=\"http://www.slideshare.net/ManoharKuse/hevc-intra-coding\" target=\"_blank\" rel=\"noopener\">http://www.slideshare.net/ManoharKuse/hevc-intra-coding</a></li>\n<li><a href=\"http://www.slideshare.net/mwalendo/h264vs-hevc\" target=\"_blank\" rel=\"noopener\">http://www.slideshare.net/mwalendo/h264vs-hevc</a></li>\n<li><a href=\"http://www.slideshare.net/rvarun7777/final-seminar-46117193\" target=\"_blank\" rel=\"noopener\">http://www.slideshare.net/rvarun7777/final-seminar-46117193</a></li>\n<li><a href=\"http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf\" target=\"_blank\" rel=\"noopener\">http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf</a></li>\n<li><a href=\"http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx\" target=\"_blank\" rel=\"noopener\">http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx</a></li>\n<li><a href=\"http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&PageNum=1\" target=\"_blank\" rel=\"noopener\">http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&amp;PageNum=1</a></li>\n<li><a href=\"http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/\" target=\"_blank\" rel=\"noopener\">http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/</a></li>\n<li><a href=\"https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/\" target=\"_blank\" rel=\"noopener\">https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/</a></li>\n<li><a href=\"https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/\" target=\"_blank\" rel=\"noopener\">https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/</a></li>\n<li><a href=\"https://codesequoia.wordpress.com/category/video/\" target=\"_blank\" rel=\"noopener\">https://codesequoia.wordpress.com/category/video/</a></li>\n<li><a href=\"https://developer.apple.com/library/content/technotes/tn2224/_index.html\" target=\"_blank\" rel=\"noopener\">https://developer.apple.com/library/content/technotes/tn2224/_index.html</a></li>\n<li><a href=\"https://en.wikibooks.org/wiki/MeGUI/x264_Settings\" target=\"_blank\" rel=\"noopener\">https://en.wikibooks.org/wiki/MeGUI/x264_Settings</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/AOMedia_Video_1\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/AOMedia_Video_1</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Cone_cell\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Cone_cell</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Inter_frame\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Inter_frame</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Intra-frame_coding\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Intra-frame_coding</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Photoreceptor_cell\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Photoreceptor_cell</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Pixel_aspect_ratio\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Pixel_aspect_ratio</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Presentation_timestamp\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Presentation_timestamp</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Rod_cell\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Rod_cell</a></li>\n<li><a href=\"https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg\" target=\"_blank\" rel=\"noopener\">https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg</a></li>\n<li><a href=\"https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/\" target=\"_blank\" rel=\"noopener\">https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/</a></li>\n<li><a href=\"https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping\" target=\"_blank\" rel=\"noopener\">https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping</a></li>\n<li><a href=\"https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/\" target=\"_blank\" rel=\"noopener\">https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/</a></li>\n<li><a href=\"https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03\" target=\"_blank\" rel=\"noopener\">https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03</a></li>\n<li><a href=\"https://www.encoding.com/android/\" target=\"_blank\" rel=\"noopener\">https://www.encoding.com/android/</a></li>\n<li><a href=\"https://www.encoding.com/http-live-streaming-hls/\" target=\"_blank\" rel=\"noopener\">https://www.encoding.com/http-live-streaming-hls/</a></li>\n<li><a href=\"https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm\" target=\"_blank\" rel=\"noopener\">https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm</a></li>\n<li><a href=\"https://www.lifewire.com/cmos-image-sensor-493271\" target=\"_blank\" rel=\"noopener\">https://www.lifewire.com/cmos-image-sensor-493271</a></li>\n<li><a href=\"https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ\" target=\"_blank\" rel=\"noopener\">https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ</a></li>\n<li><a href=\"https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar\" target=\"_blank\" rel=\"noopener\">https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar</a></li>\n<li><a href=\"https://www.vcodex.com/h264avc-intra-precition/\" target=\"_blank\" rel=\"noopener\">https://www.vcodex.com/h264avc-intra-precition/</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=9vgtJJ2wwMA\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=9vgtJJ2wwMA</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=LFXN9PiOGtY\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=LFXN9PiOGtY</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=Lto-ajuqW3w&list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=Lto-ajuqW3w&amp;list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=LWxu4rkZBLw\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=LWxu4rkZBLw</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h1><p>因为工作关系需要了解数字视频相关技术，在学习的过程中找到了这份托管在github上的<strong>数字视频导论</strong>的材料。</p>\n<p>这份材料介绍了基本的数字视频相关技术，言简意赅但又不枯燥无味，即有理论又有丰富的实践操作，在我学习的所有材料中算是比较上乘的材料。</p>\n<p>基于如上的原因，将该材料的相关内容转载到此处。大家可以直接访问该材料的github仓库<a href=\"https://github.com/leandromoreira/digital_video_introduction\" target=\"_blank\" rel=\"noopener\"><strong>digital_video_introduction</strong></a>获取相关内容。如下的所有内容皆来自<a href=\"https://github.com/leandromoreira/digital_video_introduction\" target=\"_blank\" rel=\"noopener\"><strong>digital_video_introduction</strong></a>，特此标注。</p>\n<p><a href=\"https://img.shields.io/badge/license-BSD--3--Clause-blue.svg\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/license-BSD--3--Clause-blue.svg\" alt=\"license\"></a></p>","more":"<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>这是一份循序渐进的视频技术的介绍。尽管它面向的是软件开发人员/工程师，但我们希望<strong>对任何人而言</strong>，这份文档都能简单易学。这个点子产生于一个<a href=\"https://docs.google.com/presentation/d/17Z31kEkl_NGJ0M66reqr9_uTG6tI5EDDVXpdPKVuIrs/edit#slide=id.p\" target=\"_blank\" rel=\"noopener\">视频技术新手小型研讨会</a>期间。</p>\n<p>本文档旨在尽可能使用<strong>浅显的词语，丰富的图像和实际例子</strong>介绍数字视频概念，使这些知识能适用于各种场合。你可以随时反馈意见或建议，以改进这篇文档。</p>\n<h1 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h1><ul>\n<li><a href=\"#介绍\">介绍</a></li>\n<li><a href=\"#目录\">目录</a></li>\n<li><a href=\"#基本术语\">基本术语</a><ul>\n<li><a href=\"#编码彩色图像的其它方法\">编码彩色图像的其它方法</a></li>\n<li><a href=\"#自己动手：玩转图像和颜色\">自己动手：玩转图像和颜色</a></li>\n<li><a href=\"#DVD-的-DAR-是-4-3\">DVD 的 DAR 是 4:3</a></li>\n<li><a href=\"#自己动手：检查视频属性\">自己动手：检查视频属性</a></li>\n</ul>\n</li>\n<li><a href=\"#消除冗余\">消除冗余</a><ul>\n<li><a href=\"#颜色，亮度和我们的眼睛\">颜色，亮度和我们的眼睛</a><ul>\n<li><a href=\"#颜色模型\">颜色模型</a></li>\n<li><a href=\"#YCbCr-和-RGB-之间的转换\">YCbCr 和 RGB 之间的转换</a></li>\n<li><a href=\"#色度子采样\">色度子采样</a></li>\n<li><a href=\"#自己动手：检查-YCbCr-直方图\">自己动手：检查 YCbCr 直方图</a></li>\n</ul>\n</li>\n<li><a href=\"#帧类型\">帧类型</a><ul>\n<li><a href=\"#I-帧（帧内编码，关键帧）\">I 帧（内部，关键帧）</a></li>\n<li><a href=\"#P-帧（预测）\">P 帧（预测）</a><ul>\n<li><a href=\"#自己动手：具有单个-I-帧的视频\">自己动手：具有单个 I 帧的视频</a></li>\n</ul>\n</li>\n<li><a href=\"#B-帧（双向预测）\">B 帧（双向预测）</a><ul>\n<li><a href=\"#自己动手：使用-B-帧比较视频\">自己动手：使用 B 帧比较视频</a></li>\n</ul>\n</li>\n<li><a href=\"#小结\">小结</a></li>\n</ul>\n</li>\n<li><a href=\"#时间冗余（帧间预测）\">时间冗余（帧间预测）</a><ul>\n<li><a href=\"#自己动手：查看运动向量\">自己动手：查看运动向量</a></li>\n</ul>\n</li>\n<li><a href=\"#空间冗余（帧内预测）\">空间冗余（帧内预测）</a><ul>\n<li><a href=\"#自己动手：查看帧内预测\">自己动手：查看帧内预测</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#视频编解码器是如何工作的？\">视频编解码器是如何工作的？</a><ul>\n<li><a href=\"#是什么？为什么？怎么做？\">是什么？为什么？怎么做？</a></li>\n<li><a href=\"#历史\">历史</a><ul>\n<li><a href=\"#AV1-的诞生\">AV1 的诞生</a></li>\n</ul>\n</li>\n<li><a href=\"#通用编解码器\">通用编解码器</a></li>\n<li><a href=\"#第一步-图片分区\">第一步 - 图片分区</a><ul>\n<li><a href=\"#自己动手：查看分区\">自己动手：查看分区</a></li>\n</ul>\n</li>\n<li><a href=\"#第二步-预测\">第二步 - 预测</a></li>\n<li><a href=\"#第三步-转换\">第三步 - 转换</a><ul>\n<li><a href=\"#自己动手：丢弃不同的系数\">自己动手：丢弃不同的系数</a></li>\n</ul>\n</li>\n<li><a href=\"#第四步-量化\">第四步 - 量化</a><ul>\n<li><a href=\"#自己动手：量化\">自己动手：量化</a></li>\n</ul>\n</li>\n<li><a href=\"#第五步-熵编码\">第五步 - 熵编码</a><ul>\n<li><a href=\"#VLC-编码：\">VLC 编码</a></li>\n<li><a href=\"#算术编码\">算术编码</a></li>\n<li><a href=\"#自己动手：CABAC-vs-CAVLC\">自己动手：CABAC vs CAVLC</a></li>\n</ul>\n</li>\n<li><a href=\"#第六步-比特流格式\">第六步 - 比特流格式</a><ul>\n<li><a href=\"#H-264-比特流\">H.264 比特流</a></li>\n<li><a href=\"#自己动手：检查-H-264-比特流\">自己动手：检查 H.264 比特流</a></li>\n</ul>\n</li>\n<li><a href=\"#回顾\">回顾</a></li>\n<li><a href=\"#H-265-如何实现比-H-264-更好的压缩率\">H.265 如何实现比 H.264 更好的压缩率?</a></li>\n</ul>\n</li>\n<li><a href=\"#在线流媒体\">在线流媒体</a><ul>\n<li><a href=\"#通用架构\">通用架构</a></li>\n<li><a href=\"#渐进式下载和自适应流\">渐进式下载和自适应流</a></li>\n<li><a href=\"#内容保护\">内容保护</a></li>\n</ul>\n</li>\n<li><a href=\"#如何使用-jupyter\">如何使用 jupyter</a></li>\n<li><a href=\"#会议\">会议</a></li>\n<li><a href=\"#参考\">参考</a></li>\n</ul>\n<h1 id=\"基本术语\"><a href=\"#基本术语\" class=\"headerlink\" title=\"基本术语\"></a>基本术语</h1><p>一个<strong>图像</strong>可以视作一个<strong>二维矩阵</strong>。如果将<strong>色彩</strong>考虑进来，我们可以做出推广：将这个图像视作一个<strong>三维矩阵</strong>——多出来的维度用于储存色彩信息。</p>\n<p>如果我们选择三原色（红、绿、蓝）代表这些色彩，这就定义了三个平面：第一个是红色平面，第二个是绿色平面，最后一个是蓝色平面。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/image_3d_matrix_rgb.png\" alt=\"an image is a 3d matrix RGB\" title=\"An image is a 3D matrix\"></p>\n<p>我们把这个矩阵里的每一个点称为<strong>像素</strong>（图像元素）。像素的色彩由三原色的<strong>强度</strong>（通常用数值表示）表示。例如，一个<strong>红色像素</strong>是指强度为 0 的绿色，强度为 0 的蓝色和强度最大的红色。<strong>粉色像素</strong>可以通过三种颜色的组合表示。如果规定强度的取值范围是 0 到 255，<strong>红色 255、绿色 192、蓝色 203</strong> 则表示粉色。</p>\n<blockquote>\n<h3 id=\"编码彩色图像的其它方法\"><a href=\"#编码彩色图像的其它方法\" class=\"headerlink\" title=\"编码彩色图像的其它方法\"></a>编码彩色图像的其它方法</h3><p>还有许多其它模型也可以用来表示色彩，进而组成图像。例如，给每种颜色都标上序号（如下图），这样每个像素仅需一个字节就可以表示出来，而不是 RGB 模型通常所需的 3 个。在这样一个模型里我们可以用一个二维矩阵来代替三维矩阵去表示我们的色彩，这将节省存储空间，但色彩的数量将会受限。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/nes-color-palette.png\" alt=\"NES palette\" title=\"NES palette\"></p>\n</blockquote>\n<p>例如以下几张图片。第一张包含所有颜色平面。剩下的分别是红、绿、蓝色平面（显示为灰调）（译注：颜色强度高的地方显示为亮色，强度低为暗色）。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/rgb_channels_intensity.png\" alt=\"RGB channels intensity\" title=\"RGB channels intensity\"></p>\n<p>我们可以看到，对于最终的成像，红色平面对强度的贡献更多（三个平面最亮的是红色平面），蓝色平面（最后一张图片）的贡献大多只在马里奥的眼睛和他衣服的一部分。所有颜色平面对马里奥的胡子（最暗的部分）均贡献较少。</p>\n<p>存储颜色的强度，需要占用一定大小的数据空间，这个大小被称为颜色深度。假如每个颜色（平面）的强度占用 8 bit（取值范围为 0 到 255），那么颜色深度就是 24（8*3）bit，我们还可以推导出我们可以使用 2 的 24 次方种不同的颜色。</p>\n<blockquote>\n<p>很棒的学习材料：<a href=\"http://www.cambridgeincolour.com/tutorials/camera-sensors.htm\" target=\"_blank\" rel=\"noopener\">现实世界的照片是如何拍摄成 0 和 1 的</a>。</p>\n</blockquote>\n<p>图片的另一个属性是<strong>分辨率</strong>，即一个平面内像素的数量。通常表示成宽<em>高，例如下面这张 *</em>4x4** 的图片。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/resolution.png\" alt=\"image resolution\" title=\"image resolution\"></p>\n<blockquote>\n<h3 id=\"自己动手：玩转图像和颜色\"><a href=\"#自己动手：玩转图像和颜色\" class=\"headerlink\" title=\"自己动手：玩转图像和颜色\"></a>自己动手：玩转图像和颜色</h3><p>你可以使用 <a href=\"#如何使用-jupyter\">jupyter</a>（python, numpy, matplotlib 等等）<a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/image_as_3d_array.ipynb\" target=\"_blank\" rel=\"noopener\">玩转图像</a>。</p>\n<p>你也可以学习<a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/filters_are_easy.ipynb\" target=\"_blank\" rel=\"noopener\">图像滤镜（边缘检测，磨皮，模糊。。。）的原理</a>。</p>\n</blockquote>\n<p>图像或视频还有一个属性是宽高比，它简单地描述了图像或像素的宽度和高度之间的比例关系。</p>\n<p>当人们说这个电影或照片是 16:9 时，通常是指显示宽高比（DAR），然而我们也可以有不同形状的单个像素，我们称为像素宽高比（PAR）。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/DAR.png\" alt=\"display aspect ratio\" title=\"display aspect ratio\"></p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/PAR.png\" alt=\"pixel aspect ratio\" title=\"pixel aspect ratio\"></p>\n<blockquote>\n<h2 id=\"DVD-的-DAR-是-4-3\"><a href=\"#DVD-的-DAR-是-4-3\" class=\"headerlink\" title=\"DVD 的 DAR 是 4:3\"></a>DVD 的 DAR 是 4:3</h2><p>虽然 DVD 的实际分辨率是 704x480，但它依然保持 4:3 的宽高比，因为它有一个 10:11（704x10／480x11）的 PAR。</p>\n</blockquote>\n<p>现在我们可以将<strong>视频</strong>定义为在<strong>单位时间</strong>内<strong>连续的 n 帧</strong>，这可以视作一个新的维度，n 即为帧率，若单位时间为秒，则等同于 FPS (每秒帧数 Frames Per Second)。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/video.png\" alt=\"video\" title=\"video\"></p>\n<p>播放一段视频每秒所需的数据量就是它的<strong>比特率</strong>（即常说的码率）。</p>\n<blockquote>\n<p>比特率 = 宽 * 高 * 颜色深度 * 帧每秒</p>\n</blockquote>\n<p>例如，一段每秒 30 帧，每像素 24 bits，分辨率是 480x240 的视频，如果我们不做任何压缩，它将需要 <strong>82,944,000 比特每秒</strong>或 82.944 Mbps (30x480x240x24)。</p>\n<p>当<strong>比特率</strong>几乎恒定时称为恒定比特率（<strong>CBR</strong>）；但它也可以变化，称为可变比特率（<strong>VBR</strong>）。</p>\n<blockquote>\n<p>这个图形显示了一个受限的 VBR，当帧为黑色时不会花费太多的数据量。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/vbr.png\" alt=\"constrained vbr\" title=\"constrained vbr\"></p>\n</blockquote>\n<p>在早期，工程师们想出了一项技术能将视频的感官帧率加倍而<strong>没有消耗额外带宽</strong>。这项技术被称为<strong>隔行扫描</strong>；总的来说，它在一个时间点发送一个画面——画面用于填充屏幕的一半，而下一个时间点发送的画面用于填充屏幕的另一半。</p>\n<p>如今的屏幕渲染大多使用<strong>逐行扫描技术</strong>。这是一种显示、存储、传输运动图像的方法，每帧中的所有行都会被依次绘制。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/interlaced_vs_progressive.png\" alt=\"interlaced vs progressive\" title=\"interlaced vs progressive\"></p>\n<p>现在我们知道了数字化<strong>图像</strong>的原理；它的<strong>颜色</strong>的编排方式；给定<strong>帧率</strong>和<strong>分辨率</strong>时，展示一个视频需要花费多少<strong>比特率</strong>；它是恒定的（CBR）还是可变的（VBR）；还有很多其它内容，如隔行扫描和 PAR。</p>\n<blockquote>\n<h2 id=\"自己动手：检查视频属性\"><a href=\"#自己动手：检查视频属性\" class=\"headerlink\" title=\"自己动手：检查视频属性\"></a>自己动手：检查视频属性</h2><p>你可以<a href=\"https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#inspect-stream\" target=\"_blank\" rel=\"noopener\">使用 ffmpeg 或 mediainfo 检查大多数属性的解释</a>。</p>\n</blockquote>\n<h1 id=\"消除冗余\"><a href=\"#消除冗余\" class=\"headerlink\" title=\"消除冗余\"></a>消除冗余</h1><p>我们认识到，不对视频进行压缩是不行的；<strong>一个单独的一小时长的视频</strong>，分辨率为 720p 和 30fps 时将<strong>需要 278GB<sup>*</sup></strong>。仅仅使用无损数据压缩算法——如 DEFLATE（被PKZIP, Gzip, 和 PNG 使用）——也无法充分减少视频所需的带宽，我们需要找到其它压缩视频的方法。</p>\n<blockquote>\n<p><sup>*</sup>我们使用乘积得出这个数字 1280 x 720 x 24 x 30 x 3600 （宽，高，每像素比特数，fps 和秒数）</p>\n</blockquote>\n<p>为此，我们可以<strong>利用视觉特性</strong>：和区分颜色相比，我们区分亮度要更加敏锐。<strong>时间上的重复</strong>：一段视频包含很多只有一点小小改变的图像。<strong>图像内的重复</strong>：每一帧也包含很多颜色相同或相似的区域。</p>\n<h2 id=\"颜色，亮度和我们的眼睛\"><a href=\"#颜色，亮度和我们的眼睛\" class=\"headerlink\" title=\"颜色，亮度和我们的眼睛\"></a>颜色，亮度和我们的眼睛</h2><p>我们的眼睛<a href=\"http://vanseodesign.com/web-design/color-luminance/\" target=\"_blank\" rel=\"noopener\">对亮度比对颜色更敏感</a>，你可以看看下面的图片自己测试。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/luminance_vs_color.png\" alt=\"luminance vs color\" title=\"luminance vs color\"></p>\n<p>如果你看不出左图的<strong>方块 A 和方块 B</strong> 的颜色是<strong>相同的</strong>，那么好，是我们的大脑玩了一个小把戏，这让我们更多的去注意光与暗，而不是颜色。右边这里有一个使用同样颜色的连接器，那么我们（的大脑）就能轻易分辨出事实，它们是同样的颜色。</p>\n<blockquote>\n<p><strong>简单解释我们的眼睛工作的原理</strong></p>\n<p><a href=\"http://www.biologymad.com/nervoussystem/eyenotes.htm\" target=\"_blank\" rel=\"noopener\">眼睛是一个复杂的器官</a>，有许多部分组成，但我们最感兴趣的是视锥细胞和视杆细胞。眼睛有<a href=\"https://en.wikipedia.org/wiki/Photoreceptor_cell\" target=\"_blank\" rel=\"noopener\">大约1.2亿个视杆细胞和6百万个视锥细胞</a>。</p>\n<p><strong>简单来说</strong>，让我们把颜色和亮度放在眼睛的功能部位上。<a href=\"https://en.wikipedia.org/wiki/Rod_cell\" target=\"_blank\" rel=\"noopener\">视杆细胞</a><strong>主要负责亮度</strong>，而<a href=\"https://en.wikipedia.org/wiki/Cone_cell\" target=\"_blank\" rel=\"noopener\">视锥细胞</a><strong>负责颜色</strong>，有三种类型的视锥，每个都有不同的颜料，叫做：<a href=\"https://upload.wikimedia.org/wikipedia/commons/1/1e/Cones_SMJ2_E.svg\" target=\"_blank\" rel=\"noopener\">S-视锥（蓝色），M-视锥（绿色）和L-视锥（红色）</a>。</p>\n<p>既然我们的视杆细胞（亮度）比视锥细胞多很多，一个合理的推断是相比颜色，我们有更好的能力去区分黑暗和光亮。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/eyes.jpg\" alt=\"eyes composition\" title=\"eyes composition\"></p>\n</blockquote>\n<p>一旦我们知道我们对<strong>亮度</strong>（图像中的亮度）更敏感，我们就可以利用它。</p>\n<h3 id=\"颜色模型\"><a href=\"#颜色模型\" class=\"headerlink\" title=\"颜色模型\"></a>颜色模型</h3><p>我们最开始学习的<a href=\"#基本术语\">彩色图像的原理</a>使用的是 <strong>RGB 模型</strong>，但也有其他模型。有一种模型将亮度（光亮）和色度（颜色）分离开，它被称为 <strong>YCbCr</strong><sup>*</sup>。</p>\n<blockquote>\n<p><sup>*</sup> 有很多种模型做同样的分离。</p>\n</blockquote>\n<p>这个颜色模型使用 <strong>Y</strong> 来表示亮度，还有两种颜色通道：Cb（蓝色色度） 和 Cr（红色色度）。YCbCr 可以由 RGB 转换得来，也可以转换回 RGB。使用这个模型我们可以创建拥有完整色彩的图像，如下图。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/ycbcr.png\" alt=\"ycbcr 例子\" title=\"ycbcr 例子\"></p>\n<h3 id=\"YCbCr-和-RGB-之间的转换\"><a href=\"#YCbCr-和-RGB-之间的转换\" class=\"headerlink\" title=\"YCbCr 和 RGB 之间的转换\"></a>YCbCr 和 RGB 之间的转换</h3><p>有人可能会问，在<strong>不使用绿色（色度）</strong>的情况下，我们如何表现出所有的色彩？</p>\n<p>为了回答这个问题，我们将介绍从 RGB 到 YCbCr 的转换。我们将使用 <a href=\"https://en.wikipedia.org/wiki/ITU-R\" target=\"_blank\" rel=\"noopener\">ITU-R 小组</a>*建议的<a href=\"https://en.wikipedia.org/wiki/Rec._601\" target=\"_blank\" rel=\"noopener\">标准 BT.601</a> 中的系数。</p>\n<p>第一步是计算亮度，我们将使用 ITU 建议的常量，并替换 RGB 值。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Y &#x3D; 0.299R + 0.587G + 0.114B</span><br></pre></td></tr></table></figure>\n\n<p>一旦我们有了亮度后，我们就可以拆分颜色（蓝色色度和红色色度）：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Cb &#x3D; 0.564(B - Y)</span><br><span class=\"line\">Cr &#x3D; 0.713(R - Y)</span><br></pre></td></tr></table></figure>\n\n<p>并且我们也可以使用 YCbCr 转换回来，甚至得到绿色。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">R &#x3D; Y + 1.402Cr</span><br><span class=\"line\">B &#x3D; Y + 1.772Cb</span><br><span class=\"line\">G &#x3D; Y - 0.344Cb - 0.714Cr</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p><sup>*</sup>组织和标准在数字视频领域中很常见，它们通常定义什么是标准，例如，<a href=\"https://en.wikipedia.org/wiki/Rec._2020\" target=\"_blank\" rel=\"noopener\">什么是 4K？我们应该使用什么帧率？分辨率？颜色模型？</a></p>\n</blockquote>\n<p>通常，<strong>显示屏</strong>（监视器，电视机，屏幕等等）<strong>仅使用 RGB 模型</strong>，并以不同的方式来组织，看看下面这些放大效果：</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/new_pixel_geometry.jpg\" alt=\"pixel geometry\" title=\"pixel geometry\"></p>\n<h3 id=\"色度子采样\"><a href=\"#色度子采样\" class=\"headerlink\" title=\"色度子采样\"></a>色度子采样</h3><p>一旦我们能从图像中分离出亮度和色度，我们就可以利用人类视觉系统对亮度比色度更敏感的特点，选择性地剔除信息。<strong>色度子采样</strong>是一种编码图像时，使<strong>色度分辨率低于亮度</strong>的技术。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/ycbcr_subsampling_resolution.png\" alt=\"ycbcr 子采样分辨率\" title=\"ycbcr 子采样分辨率\"></p>\n<p>我们应该减少多少色度分辨率呢？已经有一些模式定义了如何处理分辨率和合并（<code>最终的颜色 = Y + Cb + Cr</code>）。</p>\n<p>这些模式称为子采样系统，并被表示为 3 部分的比率 - <code>a:x:y</code>，其定义了色度平面的分辨率，与亮度平面上的、分辨率为 <code>a x 2</code> 的小块之间的关系。</p>\n<ul>\n<li><code>a</code> 是水平采样参考 (通常是 4)，</li>\n<li><code>x</code> 是第一行的色度样本数（相对于 a 的水平分辨率），</li>\n<li><code>y</code> 是第二行的色度样本数。</li>\n</ul>\n<blockquote>\n<p>存在的一个例外是 4:1:0，其在每个亮度平面分辨率为 4 x 4 的块内提供一个色度样本。</p>\n</blockquote>\n<p>现代编解码器中使用的常用方案是： 4:4:4 (没有子采样)**, 4:2:2, 4:1:1, 4:2:0, 4:1:0 and 3:1:1。</p>\n<blockquote>\n<p>YCbCr 4:2:0 合并</p>\n<p>这是使用 YCbCr 4:2:0 合并的一个图像的一块，注意我们每像素只花费 12bit。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/ycbcr_420_merge.png\" alt=\"YCbCr 4:2:0 合并\" title=\"YCbCr 4:2:0 合并\"></p>\n</blockquote>\n<p>下图是同一张图片使用几种主要的色度子采样技术进行编码，第一行图像是最终的 YCbCr，而最后一行图像展示了色度的分辨率。这么小的损失确实是一个伟大的胜利。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/chroma_subsampling_examples.jpg\" alt=\"色度子采样例子\" title=\"色度子采样例子\"></p>\n<p>前面我们计算过我们需要 <a href=\"#消除冗余\">278GB 去存储一个一小时长，分辨率在720p和30fps的视频文件</a>。如果我们使用 <code>YCbCr 4:2:0</code> 我们能剪掉<code>一半的大小（139GB）</code><sup>*</sup>，但仍然不够理想。</p>\n<blockquote>\n<p><sup>*</sup> 我们通过将宽、高、颜色深度和 fps 相乘得出这个值。前面我们需要 24 bit，现在我们只需要 12 bit。</p>\n</blockquote>\n<blockquote>\n<h3 id=\"自己动手：检查-YCbCr-直方图\"><a href=\"#自己动手：检查-YCbCr-直方图\" class=\"headerlink\" title=\"自己动手：检查 YCbCr 直方图\"></a>自己动手：检查 YCbCr 直方图</h3><p>你可以<a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#generates-yuv-histogram\" target=\"_blank\" rel=\"noopener\">使用 ffmpeg 检查 YCbCr 直方图</a>。这个场景有更多的蓝色贡献，由<a href=\"https://en.wikipedia.org/wiki/Histogram\" target=\"_blank\" rel=\"noopener\">直方图</a>显示。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/yuv_histogram.png\" alt=\"ycbcr 颜色直方图\" title=\"ycbcr 颜色直方图\"></p>\n</blockquote>\n<h2 id=\"帧类型\"><a href=\"#帧类型\" class=\"headerlink\" title=\"帧类型\"></a>帧类型</h2><p>现在我们进一步消除<code>时间冗余</code>，但在这之前让我们来确定一些基本术语。假设我们一段 30fps 的影片，这是最开始的 4 帧。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_1.png\" alt=\"球 1\" title=\"球 1\"> <img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_2.png\" alt=\"球 2\" title=\"球 2\"> <img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_3.png\" alt=\"球 3\" title=\"球 3\"><br><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_4.png\" alt=\"球 4\" title=\"球 4\"></p>\n<p>我们可以在帧内看到<strong>很多重复内容</strong>，如<strong>蓝色背景</strong>，从 0 帧到第 3 帧它都没有变化。为了解决这个问题，我们可以将它们<strong>抽象地分类</strong>为三种类型的帧。</p>\n<h3 id=\"I-帧（帧内编码，关键帧）\"><a href=\"#I-帧（帧内编码，关键帧）\" class=\"headerlink\" title=\"I 帧（帧内编码，关键帧）\"></a>I 帧（帧内编码，关键帧）</h3><p>I 帧（可参考，关键帧，帧内编码）是一个<strong>自足的帧</strong>。它不依靠任何东西来渲染，I 帧与静态图片相似。第一帧通常是 I 帧，但我们将看到 I 帧被定期插入其它类型的帧之间。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_1.png\" alt=\"球 1\" title=\"球 1\"></p>\n<h3 id=\"P-帧（预测）\"><a href=\"#P-帧（预测）\" class=\"headerlink\" title=\"P 帧（预测）\"></a>P 帧（预测）</h3><p>P 帧利用了一个事实：当前的画面几乎总能<strong>使用之前的一帧进行渲染</strong>。例如，在第二帧，唯一的改变是球向前移动了。仅仅使用（第二帧）对前一帧的引用和差值，我们就能重建前一帧。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_1.png\" alt=\"球 1\" title=\"球 1\"> &lt;-  <img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_2_diff.png\" alt=\"球 2\" title=\"球 2\"></p>\n<blockquote>\n<h4 id=\"自己动手：具有单个-I-帧的视频\"><a href=\"#自己动手：具有单个-I-帧的视频\" class=\"headerlink\" title=\"自己动手：具有单个 I 帧的视频\"></a>自己动手：具有单个 I 帧的视频</h4><p>既然 P 帧使用较少的数据，为什么我们不能用<a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#1-i-frame-and-the-rest-p-frames\" target=\"_blank\" rel=\"noopener\">单个 I 帧和其余的 P 帧</a>来编码整个视频？</p>\n<p>编码完这个视频之后，开始观看它，并<strong>快进到视频的末尾部分</strong>，你会注意到<strong>它需要花一些时间</strong>才真正跳转到这部分。这是因为 <strong>P 帧需要一个引用帧</strong>（比如 I 帧）才能渲染。</p>\n<p>你可以做的另一个快速试验，是使用单个 I 帧编码视频，然后<a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#1-i-frames-per-second-vs-05-i-frames-per-second\" target=\"_blank\" rel=\"noopener\">再次编码且每 2 秒插入一个 I 帧</a>，并<strong>比较成品的大小</strong>。</p>\n</blockquote>\n<h3 id=\"B-帧（双向预测）\"><a href=\"#B-帧（双向预测）\" class=\"headerlink\" title=\"B 帧（双向预测）\"></a>B 帧（双向预测）</h3><p>如何引用前面和后面的帧去做更好的压缩？！简单地说 B 帧就是这么做的。</p>\n<p> <img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_1.png\" alt=\"球 1\" title=\"球 1\"> &lt;-  <img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_2_diff.png\" alt=\"球 2\" title=\"球 2\"> -&gt; <img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_background_ball_3.png\" alt=\"球 3\" title=\"球 3\"></p>\n<blockquote>\n<h4 id=\"自己动手：使用-B-帧比较视频\"><a href=\"#自己动手：使用-B-帧比较视频\" class=\"headerlink\" title=\"自己动手：使用 B 帧比较视频\"></a>自己动手：使用 B 帧比较视频</h4><p>你可以生成两个版本，一个使用 B 帧，另一个<a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#no-b-frames-at-all\" target=\"_blank\" rel=\"noopener\">全部不使用 B 帧</a>，然后查看文件的大小以及画质。</p>\n</blockquote>\n<h3 id=\"小结\"><a href=\"#小结\" class=\"headerlink\" title=\"小结\"></a>小结</h3><p>这些帧类型用于提供更好的压缩率，我们将在下一章看到这是如何发生的。现在，我们可以想到 I 帧是昂贵的，P 帧是便宜的，最便宜的是 B 帧。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/frame_types.png\" alt=\"帧类型例子\" title=\"帧类型例子\"></p>\n<h2 id=\"时间冗余（帧间预测）\"><a href=\"#时间冗余（帧间预测）\" class=\"headerlink\" title=\"时间冗余（帧间预测）\"></a>时间冗余（帧间预测）</h2><p>让我们探究去除<strong>时间上的重复</strong>，去除这一类冗余的技术就是<strong>帧间预测</strong>。</p>\n<p>我们将尝试<strong>花费较少的数据量</strong>去编码在时间上连续的 0 号帧和 1 号帧。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/original_frames.png\" alt=\"原始帧\" title=\"原始帧\"></p>\n<p>我们可以做个减法，我们简单地<strong>用 0 号帧减去 1 号帧</strong>，得到残差，这样我们就只需要<strong>对残差进行编码</strong>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/difference_frames.png\" alt=\"残差帧\" title=\"残差帧\"></p>\n<p>但我们有一个<strong>更好的方法</strong>来节省数据量。首先，我们将<code>0 号帧</code> 视为一个个分块的集合，然后我们将尝试将 <code>帧 1</code> 和 <code>帧 0</code> 上的块相匹配。我们可以将这看作是<strong>运动预测</strong>。</p>\n<blockquote>\n<h3 id=\"维基百科—块运动补偿\"><a href=\"#维基百科—块运动补偿\" class=\"headerlink\" title=\"维基百科—块运动补偿\"></a>维基百科—块运动补偿</h3><p>“运动补偿是一种描述相邻帧（相邻在这里表示在编码关系上相邻，在播放顺序上两帧未必相邻）差别的方法，具体来说是描述前面一帧（相邻在这里表示在编码关系上的前面，在播放顺序上未必在当前帧前面）的每个小块怎样移动到当前帧中的某个位置去。”</p>\n</blockquote>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/original_frames_motion_estimation.png\" alt=\"原始帧运动预测\" title=\"原始帧运动预测\"></p>\n<p>我们预计那个球会从 <code>x=0, y=25</code> 移动到 <code>x=6, y=26</code>，<strong>x</strong> 和 <strong>y</strong> 的值就是<strong>运动向量</strong>。<strong>进一步</strong>节省数据量的方法是，只编码这两者运动向量的差。所以，最终运动向量就是 <code>x=6 (6-0), y=1 (26-25)</code>。</p>\n<blockquote>\n<p>实际情况下，这个球会被切成 n 个分区，但处理过程是相同的。</p>\n</blockquote>\n<p>帧上的物体<strong>以三维方式移动</strong>，当球移动到背景时会变小。当我们尝试寻找匹配的块，<strong>找不到完美匹配的块</strong>是正常的。这是一张运动预测与实际值相叠加的图片。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/motion_estimation.png\" alt=\"运动预测\" title=\"运动预测\"></p>\n<p>但我们能看到当我们使用<strong>运动预测</strong>时，<strong>编码的数据量少于</strong>使用简单的残差帧技术。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/comparison_delta_vs_motion_estimation.png\" alt=\"运动预测 vs 残差 \" title=\"运动预测 vs 残差\"></p>\n<p>你可以<a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/frame_difference_vs_motion_estimation_plus_residual.ipynb\" target=\"_blank\" rel=\"noopener\">使用 jupyter 玩转这些概念</a>。</p>\n<blockquote>\n<h3 id=\"自己动手：查看运动向量\"><a href=\"#自己动手：查看运动向量\" class=\"headerlink\" title=\"自己动手：查看运动向量\"></a>自己动手：查看运动向量</h3><p>我们可以<a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#generate-debug-video\" target=\"_blank\" rel=\"noopener\">使用 ffmpeg 生成包含帧间预测（运动向量）的视频</a>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/motion_vectors_ffmpeg.png\" alt=\"ffmpeg 帧间预测（运动向量）\" title=\"ffmpeg 帧间预测（运动向量）\"></p>\n<p>或者我们也可使用 <a href=\"https://software.intel.com/en-us/intel-video-pro-analyzer\" target=\"_blank\" rel=\"noopener\">Intel® Video Pro Analyzer</a>（需要付费，但也有只能查看前 10 帧的免费试用版）。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/inter_prediction_intel_video_pro_analyzer.png\" alt=\"Intel® Video Pro Analyzer 使用帧间预测\" title=\"inter prediction intel video pro analyzer\"></p>\n</blockquote>\n<h2 id=\"空间冗余（帧内预测）\"><a href=\"#空间冗余（帧内预测）\" class=\"headerlink\" title=\"空间冗余（帧内预测）\"></a>空间冗余（帧内预测）</h2><p>如果我们分析一个视频里的<strong>每一帧</strong>，我们会看到有<strong>许多区域是相互关联的</strong>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/repetitions_in_space.png\" alt=\"空间内重复\" title=\"空间内重复\"></p>\n<p>让我们举一个例子。这个场景大部分由蓝色和白色组成。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_bg.png\" alt=\"smw 背景\" title=\"smw 背景\"></p>\n<p>这是一个 <code>I 帧</code>，我们<strong>不能使用前面的帧来预测</strong>，但我们仍然可以压缩它。我们将编码我们选择的那块红色区域。如果我们<strong>看看它的周围</strong>，我们可以<strong>估计它周围颜色的变化</strong>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_bg_block.png\" alt=\"smw 背景块\" title=\"smw 背景块\"></p>\n<p>我们预测:帧中的颜色在垂直方向上保持一致，这意味着<strong>未知像素的颜色与临近的像素相同</strong>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_bg_prediction.png\" alt=\"smw 背景预测\" title=\"smw 背景预测\"></p>\n<p>我们的<strong>预测会出错</strong>，所以我们需要先利用这项技术（<strong>帧内预测</strong>），然后<strong>减去实际值</strong>，算出残差，得出的矩阵比原始数据更容易压缩。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/smw_residual.png\" alt=\"smw 残差\" title=\"smw 残差\"></p>\n<blockquote>\n<h3 id=\"自己动手：查看帧内预测\"><a href=\"#自己动手：查看帧内预测\" class=\"headerlink\" title=\"自己动手：查看帧内预测\"></a>自己动手：查看帧内预测</h3><p>你可以<a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#generate-debug-video\" target=\"_blank\" rel=\"noopener\">使用 ffmpeg 生成包含宏块及预测的视频</a>。请查看 ffmpeg 文档以了解<a href=\"https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors\" target=\"_blank\" rel=\"noopener\">每个块颜色的含义</a>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/macro_blocks_ffmpeg.png\" alt=\"ffmpeg 帧内预测（宏块）\" title=\"ffmpeg 帧内预测（宏块）\"></p>\n<p>或者我们也可使用 <a href=\"https://software.intel.com/en-us/intel-video-pro-analyzer\" target=\"_blank\" rel=\"noopener\">Intel® Video Pro Analyzer</a>（需要付费，但也有只能查看前 10 帧的免费试用版）。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/intra_prediction_intel_video_pro_analyzer.png\" alt=\"Intel® Video Pro Analyzer 帧内预测\" title=\"Intel® Video Pro Analyzer 帧内预测\"></p>\n</blockquote>\n<h1 id=\"视频编解码器是如何工作的？\"><a href=\"#视频编解码器是如何工作的？\" class=\"headerlink\" title=\"视频编解码器是如何工作的？\"></a>视频编解码器是如何工作的？</h1><h2 id=\"是什么？为什么？怎么做？\"><a href=\"#是什么？为什么？怎么做？\" class=\"headerlink\" title=\"是什么？为什么？怎么做？\"></a>是什么？为什么？怎么做？</h2><p><strong>是什么？</strong> 就是用于压缩或解压数字视频的软件或硬件。<strong>为什么？</strong> 人们需要在有限带宽或存储空间下提高视频的质量。还记得当我们计算每秒 30 帧，每像素 24 bit，分辨率是 480x240 的视频<a href=\"#基本术语\">需要多少带宽</a>吗？没有压缩时是 <strong>82.944 Mbps</strong>。电视或互联网提供 HD/FullHD/4K 只能靠视频编解码器。<strong>怎么做？</strong> 我们将简单介绍一下主要的技术。</p>\n<blockquote>\n<p>视频编解码 vs 容器</p>\n<p>初学者一个常见的错误是混淆数字视频编解码器和<a href=\"https://en.wikipedia.org/wiki/Digital_container_format\" target=\"_blank\" rel=\"noopener\">数字视频容器</a>。我们可以将<strong>容器</strong>视为包含视频（也很可能包含音频）元数据的包装格式，<strong>压缩过的视频</strong>可以看成是它承载的内容。</p>\n<p>通常，视频文件的格式定义其视频容器。例如，文件 <code>video.mp4</code> 可能是 <a href=\"https://en.wikipedia.org/wiki/MPEG-4_Part_14\" target=\"_blank\" rel=\"noopener\">MPEG-4 Part 14</a> 容器，一个叫 <code>video.mkv</code> 的文件可能是 <a href=\"https://en.wikipedia.org/wiki/Matroska\" target=\"_blank\" rel=\"noopener\">matroska</a>。我们可以使用 <a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#inspect-stream\" target=\"_blank\" rel=\"noopener\">ffmpeg 或 mediainfo</a> 来完全确定编解码器和容器格式。</p>\n</blockquote>\n<h2 id=\"历史\"><a href=\"#历史\" class=\"headerlink\" title=\"历史\"></a>历史</h2><p>在我们跳进通用编解码器内部工作之前，让我们回头了解一些旧的视频编解码器。</p>\n<p>视频编解码器 <a href=\"https://en.wikipedia.org/wiki/H.261\" target=\"_blank\" rel=\"noopener\">H.261</a> 诞生在 1990（技术上是 1988），被设计为以 <strong>64 kbit/s 的数据速率</strong>工作。它已经使用如色度子采样、宏块，等等理念。在 1995 年，<strong>H.263</strong> 视频编解码器标准被发布，并继续延续到 2001 年。</p>\n<p>在 2003 年 <strong>H.264/AVC</strong> 的第一版被完成。在同一年，一家叫做 <strong>TrueMotion</strong> 的公司发布了他们的<strong>免版税</strong>有损视频压缩的视频编解码器，称为 <strong>VP3</strong>。在 2008 年，<strong>Google 收购了</strong>这家公司，在同一年发布 <strong>VP8</strong>。在 2012 年 12 月，Google 发布了 <strong>VP9</strong>，<strong>市面上大约有 3/4 的浏览器</strong>（包括手机）支持。</p>\n<p><a href=\"https://en.wikipedia.org/wiki/AOMedia_Video_1\" target=\"_blank\" rel=\"noopener\">AV1</a> 是由 <strong>Google, Mozilla, Microsoft, Amazon, Netflix, AMD, ARM, NVidia, Intel, Cisco</strong> 等公司组成的<a href=\"http://aomedia.org/\" target=\"_blank\" rel=\"noopener\">开放媒体联盟（AOMedia）</a>设计的一种新的视频编解码器，免版税，开源。<strong>第一版</strong> 0.1.0 参考编解码器<strong>发布于 2016 年 4 月 7 号</strong>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/codec_history_timeline.png\" alt=\"编解码器历史线路图\" title=\"编解码器历史线路图\"></p>\n<blockquote>\n<h3 id=\"AV1-的诞生\"><a href=\"#AV1-的诞生\" class=\"headerlink\" title=\"AV1 的诞生\"></a>AV1 的诞生</h3><p>2015 年早期，Google 正在 VP10 上工作，Xiph (Mozilla) 正在 Daala 上工作，Cisco 开源了它的称为 Thor 的免版税视频编解码器。</p>\n<p>接着 MPEG LA 宣布了 HEVC (H.265) 每年版税的的上限，比 H.264 高 8 倍，但很快他们又再次改变了条款：</p>\n<ul>\n<li><strong>不设年度收费上限</strong></li>\n<li><strong>收取内容费</strong>（收入的 0.5%）</li>\n<li><strong>每单位费用高于 h264 的 10 倍</strong></li>\n</ul>\n<p><a href=\"http://aomedia.org/about-us/\" target=\"_blank\" rel=\"noopener\">开放媒体联盟</a>由硬件厂商（Intel, AMD, ARM , Nvidia, Cisco），内容分发商（Google, Netflix, Amazon），浏览器维护者（Google, Mozilla），等公司创建。</p>\n<p>这些公司有一个共同目标，一个免版税的视频编解码器，所以 AV1 诞生时使用了一个更<a href=\"http://aomedia.org/license/patent/\" target=\"_blank\" rel=\"noopener\">简单的专利许可证</a>。<strong>Timothy B. Terriberry</strong> 做了一个精彩的介绍，<a href=\"https://www.youtube.com/watch?v=lzPaldsmJbk\" target=\"_blank\" rel=\"noopener\">关于 AV1 的概念，许可证模式和它当前的状态</a>，就是本节的来源。</p>\n<p>前往 <a href=\"http://aomanalyzer.org/\" target=\"_blank\" rel=\"noopener\">http://aomanalyzer.org/</a>， 你会惊讶于<strong>使用你的浏览器就可以分析 AV1 编解码器</strong>。<br><img src=\"/2020/02/28/Introduction-to-digital-video-technology/av1_browser_analyzer.png\" alt=\"av1 浏览器分析器\" title=\"浏览器分析器\"></p>\n<p>附：如果你想了解更多编解码器的历史，你需要了解<a href=\"https://www.vcodex.com/video-compression-patents/\" target=\"_blank\" rel=\"noopener\">视频压缩专利</a>背后的基本知识。</p>\n</blockquote>\n<h2 id=\"通用编解码器\"><a href=\"#通用编解码器\" class=\"headerlink\" title=\"通用编解码器\"></a>通用编解码器</h2><p>我们接下来要介绍<strong>通用视频编解码器背后的主要机制</strong>，大多数概念都很实用，并被现代编解码器如 VP9, AV1 和 HEVC 使用。需要注意：我们将简化许多内容。有时我们会使用真实的例子（主要是 H.264）来演示技术。</p>\n<h2 id=\"第一步-图片分区\"><a href=\"#第一步-图片分区\" class=\"headerlink\" title=\"第一步 - 图片分区\"></a>第一步 - 图片分区</h2><p>第一步是<strong>将帧</strong>分成几个<strong>分区</strong>，<strong>子分区</strong>甚至更多。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/picture_partitioning.png\" alt=\"图片分区\" title=\"图片分区\"></p>\n<p><strong>但是为什么呢？</strong>有许多原因，比如，当我们分割图片时，我们可以更精确的处理预测，在微小移动的部分使用较小的分区，而在静态背景上使用较大的分区。</p>\n<p>通常，编解码器<strong>将这些分区组织</strong>成切片（或瓦片），宏（或编码树单元）和许多子分区。这些分区的最大大小有所不同，HEVC 设置成 64x64，而 AVC 使用 16x16，但子分区可以达到 4x4 的大小。</p>\n<p>还记得我们学过的<strong>帧的分类</strong>吗？你也可以<strong>把这些概念应用到块</strong>，因此我们可以有 I 切片，B 切片，I 宏块等等。</p>\n<blockquote>\n<h3 id=\"自己动手：查看分区\"><a href=\"#自己动手：查看分区\" class=\"headerlink\" title=\"自己动手：查看分区\"></a>自己动手：查看分区</h3><p>我们也可以使用 <a href=\"https://software.intel.com/en-us/intel-video-pro-analyzer\" target=\"_blank\" rel=\"noopener\">Intel® Video Pro Analyzer</a>（需要付费，但也有只能查看前 10 帧的免费试用版）。这是 <a href=\"https://github.com/leandromoreira/digital_video_introduction/blob/master/encoding_pratical_examples.md#transcoding\" target=\"_blank\" rel=\"noopener\">VP9 分区</a>的分析。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/paritions_view_intel_video_pro_analyzer.png\" alt=\"Intel® Video Pro Analyzer VP9 分区视图 \" title=\"Intel® Video Pro Analyzer VP9 分区视图\"></p>\n</blockquote>\n<h2 id=\"第二步-预测\"><a href=\"#第二步-预测\" class=\"headerlink\" title=\"第二步 - 预测\"></a>第二步 - 预测</h2><p>一旦我们有了分区，我们就可以在它们之上做出预测。对于<a href=\"#时间冗余（帧间预测）\">帧间预测</a>，我们需要<strong>发送运动向量和残差</strong>；至于<a href=\"#空间冗余（帧内预测）\">帧内预测</a>，我们需要<strong>发送预测方向和残差</strong>。</p>\n<h2 id=\"第三步-转换\"><a href=\"#第三步-转换\" class=\"headerlink\" title=\"第三步 - 转换\"></a>第三步 - 转换</h2><p>在我们得到残差块（<code>预测分区-真实分区</code>）之后，我们可以用一种方式<strong>变换</strong>它，这样我们就知道<strong>哪些像素我们应该丢弃</strong>，还依然能保持<strong>整体质量</strong>。这个确切的行为有几种变换方式。</p>\n<p>尽管有<a href=\"https://en.wikipedia.org/wiki/List_of_Fourier-related_transforms#Discrete_transforms\" target=\"_blank\" rel=\"noopener\">其它的变换方式</a>，但我们重点关注离散余弦变换（DCT）。<a href=\"https://en.wikipedia.org/wiki/Discrete_cosine_transform\" target=\"_blank\" rel=\"noopener\">DCT</a> 的主要功能有：</p>\n<ul>\n<li>将<strong>像素</strong>块<strong>转换</strong>为相同大小的<strong>频率系数块</strong>。</li>\n<li><strong>压缩</strong>能量，更容易消除空间冗余。</li>\n<li><strong>可逆的</strong>，也意味着你可以还原回像素。</li>\n</ul>\n<blockquote>\n<p>2017 年 2 月 2 号，F. M. Bayer 和 R. J. Cintra 发表了他们的论文：<a href=\"https://arxiv.org/abs/1702.00817\" target=\"_blank\" rel=\"noopener\">图像压缩的 DCT 类变换只需要 14 个加法</a>。</p>\n</blockquote>\n<p>如果你不理解每个要点的好处，不用担心，我们会尝试进行一些实验，以便从中看到真正的价值。</p>\n<p>我们来看下面的<strong>像素块</strong>（8x8）：</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/pixel_matrice.png\" alt=\"像素值矩形\" title=\"像素值矩形\"></p>\n<p>下面是其渲染的块图像（8x8）：</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/gray_image.png\" alt=\"像素值矩形\" title=\"像素值矩形\"></p>\n<p>当我们对这个像素块<strong>应用 DCT</strong> 时， 得到如下<strong>系数块</strong>（8x8）：</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/dct_coefficient_values.png\" alt=\"系数值 values\" title=\"系数值\"></p>\n<p>接着如果我们渲染这个系数块，就会得到这张图片：</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/dct_coefficient_image.png\" alt=\"dct 系数图片\" title=\"dct 系数图片\"></p>\n<p>如你所见它看起来完全不像原图像，我们可能会注意到<strong>第一个系数</strong>与其它系数非常不同。第一个系数被称为直流分量，代表了输入数组中的<strong>所有样本</strong>，有点<strong>类似于平均值</strong>。</p>\n<p>这个系数块有一个有趣的属性：高频部分和低频部分是分离的。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/dctfrequ.jpg\" alt=\"dct 频率系数属性\" title=\"dct 频率系数属性\"></p>\n<p>在一张图像中，<strong>大多数能量</strong>会集中在<a href=\"https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm\" target=\"_blank\" rel=\"noopener\">低频部分</a>，所以如果我们将图像转换成频率系数，并<strong>丢掉高频系数</strong>，我们就能<strong>减少描述图像所需的数据量</strong>，而不会牺牲太多的图像质量。</p>\n<blockquote>\n<p>频率是指信号变化的速度。</p>\n</blockquote>\n<p>让我们通过实验学习这点，我们将使用 DCT 把原始图像转换为频率（系数块），然后丢掉最不重要的系数。</p>\n<p>首先，我们将它转换为其<strong>频域</strong>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/dct_coefficient_values.png\" alt=\"系数值\" title=\"系数值\"></p>\n<p>然后我们丢弃部分（67%）系数，主要是它的右下角部分。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/dct_coefficient_zeroed.png\" alt=\"系数清零\" title=\"系数清零\"></p>\n<p>然后我们从丢弃的系数块重构图像（记住，这需要可逆），并与原始图像相比较。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/original_vs_quantized.png\" alt=\"原始 vs 量化\" title=\"原始 vs 量化\"></p>\n<p>如我们所见它酷似原始图像，但它引入了许多与原来的不同，我们<strong>丢弃了67.1875%</strong>，但我们仍然得到至少类似于原来的东西。我们可以更加智能的丢弃系数去得到更好的图像质量，但这是下一个主题。</p>\n<blockquote>\n<h3 id=\"使用全部像素形成每个系数\"><a href=\"#使用全部像素形成每个系数\" class=\"headerlink\" title=\"使用全部像素形成每个系数\"></a>使用全部像素形成每个系数</h3><p>重要的是要注意，每个系数并不直接映射到单个像素，但它是所有像素的加权和。这个神奇的图形展示了如何计算出第一和第二个系数，使用每个唯一的索引做权重。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/applicat.jpg\" alt=\"dct 计算\" title=\"dct 计算\"></p>\n<p>来源：<a href=\"https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm\" target=\"_blank\" rel=\"noopener\">https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm</a></p>\n<p>你也可以尝试<a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/dct_better_explained.ipynb\" target=\"_blank\" rel=\"noopener\">通过查看在 DCT 基础上形成的简单图片来可视化 DCT</a>。例如，这是使用每个系数权重<a href=\"https://en.wikipedia.org/wiki/Discrete_cosine_transform#Example_of_IDCT\" target=\"_blank\" rel=\"noopener\">形成的字符 A</a>。</p>\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5e/Idct-animation.gif\" alt></p>\n</blockquote>\n<br>\n\n<blockquote>\n<h3 id=\"自己动手：丢弃不同的系数\"><a href=\"#自己动手：丢弃不同的系数\" class=\"headerlink\" title=\"自己动手：丢弃不同的系数\"></a>自己动手：丢弃不同的系数</h3><p>你可以玩转 <a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/uniform_quantization_experience.ipynb\" target=\"_blank\" rel=\"noopener\">DCT 变换</a></p>\n</blockquote>\n<h2 id=\"第四步-量化\"><a href=\"#第四步-量化\" class=\"headerlink\" title=\"第四步 - 量化\"></a>第四步 - 量化</h2><p>当我们丢弃一些系数时，在最后一步（变换），我们做了一些形式的量化。这一步，我们选择性地剔除信息（<strong>有损部分</strong>）或者简单来说，我们将<strong>量化系数以实现压缩</strong>。</p>\n<p>我们如何量化一个系数块？一个简单的方法是均匀量化，我们取一个块并<strong>将其除以单个的值</strong>（10），并舍入值。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/quantize.png\" alt=\"量化\" title=\"量化\"></p>\n<p>我们如何<strong>逆转</strong>（重新量化）这个系数块？我们可以通过<strong>乘以我们先前除以的相同的值</strong>（10）来做到。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/re-quantize.png\" alt=\"逆转量化\" title=\"逆转量化\"></p>\n<p>这<strong>不是最好的方法</strong>，因为它没有考虑到每个系数的重要性，我们可以使用一个<strong>量化矩阵</strong>来代替单个值，这个矩阵可以利用 DCT 的属性，多量化右下部，而少（量化）左上部，<a href=\"https://www.hdm-stuttgart.de/~maucher/Python/MMCodecs/html/jpegUpToQuant.html\" target=\"_blank\" rel=\"noopener\">JPEG 使用了类似的方法</a>，你可以通过<a href=\"https://github.com/google/guetzli/blob/master/guetzli/jpeg_data.h#L40\" target=\"_blank\" rel=\"noopener\">查看源码看看这个矩阵</a>。</p>\n<blockquote>\n<h3 id=\"自己动手：量化\"><a href=\"#自己动手：量化\" class=\"headerlink\" title=\"自己动手：量化\"></a>自己动手：量化</h3><p>你可以玩转<a href=\"https://github.com/wangwei1237/wangwei1237.github.io/blob/master/2020/02/28/Introduction-to-digital-video-technology/py/dct_experiences.ipynb\" target=\"_blank\" rel=\"noopener\">量化</a></p>\n</blockquote>\n<h2 id=\"第五步-熵编码\"><a href=\"#第五步-熵编码\" class=\"headerlink\" title=\"第五步 - 熵编码\"></a>第五步 - 熵编码</h2><p>在我们量化数据（图像块／切片／帧）之后，我们仍然可以以无损的方式来压缩它。有许多方法（算法）可用来压缩数据。我们将简单体验其中几个，你可以阅读这本很棒的书去深入理解：<a href=\"https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/\" target=\"_blank\" rel=\"noopener\">Understanding Compression: Data Compression for Modern Developers</a>。</p>\n<h3 id=\"VLC-编码：\"><a href=\"#VLC-编码：\" class=\"headerlink\" title=\"VLC 编码：\"></a>VLC 编码：</h3><p>让我们假设我们有一个符号流：<strong>a</strong>, <strong>e</strong>, <strong>r</strong> 和 <strong>t</strong>，它们的概率（从0到1）由下表所示。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>a</th>\n<th>e</th>\n<th>r</th>\n<th>t</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>概率</td>\n<td>0.3</td>\n<td>0.3</td>\n<td>0.2</td>\n<td>0.2</td>\n</tr>\n</tbody></table>\n<p>我们可以分配不同的二进制码，（最好是）小的码给最可能（出现的字符），大些的码给最少可能（出现的字符）。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>a</th>\n<th>e</th>\n<th>r</th>\n<th>t</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>概率</td>\n<td>0.3</td>\n<td>0.3</td>\n<td>0.2</td>\n<td>0.2</td>\n</tr>\n<tr>\n<td>二进制码</td>\n<td>0</td>\n<td>10</td>\n<td>110</td>\n<td>1110</td>\n</tr>\n</tbody></table>\n<p>让我们压缩 <strong>eat</strong> 流，假设我们为每个字符花费 8 bit，在没有做任何压缩时我们将花费 <strong>24 bit</strong>。但是在这种情况下，我们使用各自的代码来替换每个字符，我们就能节省空间。</p>\n<p>第一步是编码字符 <strong>e</strong> 为 <code>10</code>，第二个字符是 <strong>a</strong>，追加（不是数学加法）后是 <code>[10][0]</code>，最后是第三个字符 <strong>t</strong>，最终组成已压缩的比特流 <code>[10][0][1110]</code> 或 <code>1001110</code>，这只需 <strong>7 bit</strong>（比原来的空间少 3.4 倍）。</p>\n<p>请注意每个代码必须是唯一的前缀码，<a href=\"https://en.wikipedia.org/wiki/Huffman_coding\" target=\"_blank\" rel=\"noopener\">Huffman 能帮你找到这些数字</a>。虽然它有一些问题，但是<a href=\"https://en.wikipedia.org/wiki/Context-adaptive_variable-length_coding\" target=\"_blank\" rel=\"noopener\">视频编解码器仍然提供该方法</a>，它也是很多应用程序的压缩算法。</p>\n<p>编码器和解码器都<strong>必须知道</strong>这个（包含编码的）字符表，因此，你也需要传送这个表。</p>\n<h3 id=\"算术编码\"><a href=\"#算术编码\" class=\"headerlink\" title=\"算术编码\"></a>算术编码</h3><p>让我们假设我们有一个符号流：<strong>a</strong>, <strong>e</strong>, <strong>r</strong>, <strong>s</strong> 和 <strong>t</strong>，它们的概率由下表所示。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>a</th>\n<th>e</th>\n<th>r</th>\n<th>s</th>\n<th>t</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>概率</td>\n<td>0.3</td>\n<td>0.3</td>\n<td>0.15</td>\n<td>0.05</td>\n<td>0.2</td>\n</tr>\n</tbody></table>\n<p>考虑到这个表，我们可以构建一个区间，区间包含了所有可能的字符，字符按出现概率排序。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/range.png\" alt=\"初始算法区间\" title=\"初始算法区间\"></p>\n<p>让我们编码 <strong>eat</strong> 流，我们选择第一个字符 <strong>e</strong> 位于 <strong>0.3 到 0.6</strong> （但不包括 0.6）的子区间，我们选择这个子区间，按照之前同等的比例再次分割。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/second_subrange.png\" alt=\"第二个子区间\" title=\"第二个子区间\"></p>\n<p>让我们继续编码我们的流 <strong>eat</strong>，现在使第二个 <strong>a</strong> 字符位于 <strong>0.3 到 0.39</strong> 的区间里，接着再次用同样的方法编码最后的字符 <strong>t</strong>，得到最后的子区间 <strong>0.354 到 0.372</strong>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/arithimetic_range.png\" alt=\"最终算法区间\" title=\"最终算法区间\"></p>\n<p>我们只需从最后的子区间 0.354 到 0.372 里选择一个数，让我们选择 0.36，不过我们可以选择这个子区间里的任何数。仅靠这个数，我们将可以恢复原始流 <strong>eat</strong>。就像我们在区间的区间里画了一根线来编码我们的流。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/range_show.png\" alt=\"最终区间横断面\" title=\"最终区间横断面\"></p>\n<p><strong>反向过程</strong>（又名解码）一样简单，用数字 <strong>0.36</strong> 和我们原始区间，我们可以进行同样的操作，不过现在是使用这个数字来还原被编码的流。</p>\n<p>在第一个区间，我们发现数字落入了一个子区间，因此，这个子区间是我们的第一个字符，现在我们再次切分这个子区间，像之前一样做同样的过程。我们会注意到 <strong>0.36</strong> 落入了 <strong>a</strong> 的区间，然后我们重复这一过程直到得到最后一个字符 <strong>t</strong>（形成我们原始编码过的流 eat）。</p>\n<p>编码器和解码器都<strong>必须知道</strong>字符概率表，因此，你也需要传送这个表。</p>\n<p>非常巧妙，不是吗？人们能想出这样的解决方案实在是太聪明了，一些<a href=\"https://en.wikipedia.org/wiki/Context-adaptive_binary_arithmetic_coding\" target=\"_blank\" rel=\"noopener\">视频编解码器使用</a>这项技术（或至少提供这一选择）。</p>\n<p>关于无损压缩量化比特流的办法，这篇文章无疑缺少了很多细节、原因、权衡等等。作为一个开发者你<a href=\"https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/\" target=\"_blank\" rel=\"noopener\">应该学习更多</a>。刚入门视频编码的人可以尝试使用不同的<a href=\"https://en.wikipedia.org/wiki/Asymmetric_Numeral_Systems\" target=\"_blank\" rel=\"noopener\">熵编码算法，如ANS</a>。</p>\n<blockquote>\n<h3 id=\"自己动手：CABAC-vs-CAVLC\"><a href=\"#自己动手：CABAC-vs-CAVLC\" class=\"headerlink\" title=\"自己动手：CABAC vs CAVLC\"></a>自己动手：CABAC vs CAVLC</h3><p>你可以<a href=\"https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#cabac-vs-cavlc\" target=\"_blank\" rel=\"noopener\">生成两个流，一个使用 CABAC，另一个使用 CAVLC</a>，并比较生成每一个的时间以及最终的大小。</p>\n</blockquote>\n<h2 id=\"第六步-比特流格式\"><a href=\"#第六步-比特流格式\" class=\"headerlink\" title=\"第六步 - 比特流格式\"></a>第六步 - 比特流格式</h2><p>完成所有这些步之后，我们需要将<strong>压缩过的帧和内容打包进去</strong>。需要明确告知解码器<strong>编码定义</strong>，如颜色深度，颜色空间，分辨率，预测信息（运动向量，帧内预测方向），配置<sup>*</sup>，层级<sup>*</sup>，帧率，帧类型，帧号等等更多信息。</p>\n<blockquote>\n<p><sup>*</sup> 译注：原文为 profile 和 level，没有通用的译名</p>\n</blockquote>\n<p>我们将简单地学习 H.264 比特流。第一步是<a href=\"https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#generate-a-single-frame-h264-bitstream\" target=\"_blank\" rel=\"noopener\">生成一个小的 H.264<sup>*</sup> 比特流</a>，可以使用本 repo 和 <a href=\"http://ffmpeg.org/\" target=\"_blank\" rel=\"noopener\">ffmpeg</a> 来做。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.&#x2F;s&#x2F;ffmpeg -i &#x2F;files&#x2F;i&#x2F;minimal.png -pix_fmt yuv420p &#x2F;files&#x2F;v&#x2F;minimal_yuv420.h264</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p><sup>*</sup> ffmpeg 默认将所有参数添加为 <strong>SEI NAL</strong>，很快我们会定义什么是 NAL。</p>\n</blockquote>\n<p>这个命令会使用下面的图片作为帧，生成一个具有<strong>单个帧</strong>，64x64 和颜色空间为 yuv420 的原始 h264 比特流。</p>\n<blockquote>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/minimal.png\" alt=\"使用帧来生成极简 h264 比特流\" title=\"使用帧来生成极简 h264 比特流\"></p>\n</blockquote>\n<h3 id=\"H-264-比特流\"><a href=\"#H-264-比特流\" class=\"headerlink\" title=\"H.264 比特流\"></a>H.264 比特流</h3><p>AVC (H.264) 标准规定信息将在宏帧（网络概念上的）内传输，称为 <a href=\"https://en.wikipedia.org/wiki/Network_Abstraction_Layer\" target=\"_blank\" rel=\"noopener\">NAL</a>（网络抽象层）。NAL 的主要目标是提供“网络友好”的视频呈现方式，该标准必须适用于电视（基于流），互联网（基于数据包）等。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/nal_units.png\" alt=\"H.264 NAL 单元\" title=\"H.264 NAL 单元\"></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Frame_synchronization\" target=\"_blank\" rel=\"noopener\">同步标记</a>用来定义 NAL 单元的边界。每个同步标记的值固定为  <code>0x00 0x00 0x01</code> ，最开头的标记例外，它的值是  <code>0x00 0x00 0x00 0x01</code> 。如果我们在生成的 h264 比特流上运行 <strong>hexdump</strong>，我们可以在文件的开头识别至少三个 NAL。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/minimal_yuv420_hex.png\" alt=\"NAL 单元上的同步标记\" title=\"NAL 单元上的同步标记\"></p>\n<p>我们之前说过，解码器需要知道不仅仅是图片数据，还有视频的详细信息，如：帧、颜色、使用的参数等。每个 NAL 的<strong>第一位</strong>定义了其分类和<strong>类型</strong>。</p>\n<table>\n<thead>\n<tr>\n<th>NAL type id</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>0</td>\n<td>Undefined</td>\n</tr>\n<tr>\n<td>1</td>\n<td>Coded slice of a non-IDR picture</td>\n</tr>\n<tr>\n<td>2</td>\n<td>Coded slice data partition A</td>\n</tr>\n<tr>\n<td>3</td>\n<td>Coded slice data partition B</td>\n</tr>\n<tr>\n<td>4</td>\n<td>Coded slice data partition C</td>\n</tr>\n<tr>\n<td>5</td>\n<td><strong>IDR</strong> Coded slice of an IDR picture</td>\n</tr>\n<tr>\n<td>6</td>\n<td><strong>SEI</strong> Supplemental enhancement information</td>\n</tr>\n<tr>\n<td>7</td>\n<td><strong>SPS</strong> Sequence parameter set</td>\n</tr>\n<tr>\n<td>8</td>\n<td><strong>PPS</strong> Picture parameter set</td>\n</tr>\n<tr>\n<td>9</td>\n<td>Access unit delimiter</td>\n</tr>\n<tr>\n<td>10</td>\n<td>End of sequence</td>\n</tr>\n<tr>\n<td>11</td>\n<td>End of stream</td>\n</tr>\n<tr>\n<td>…</td>\n<td>…</td>\n</tr>\n</tbody></table>\n<p>通常，比特流的第一个 NAL 是 <strong>SPS</strong>，这个类型的 NAL 负责传达通用编码参数，如<strong>配置，层级，分辨率</strong>等。</p>\n<p>如果我们跳过第一个同步标记，就可以通过解码<strong>第一个字节</strong>来了解第一个 <strong>NAL 的类型</strong>。</p>\n<p>例如同步标记之后的第一个字节是 <code>01100111</code>，第一位（<code>0</code>）是 <strong>forbidden_zero_bit</strong> 字段，接下来的两位（<code>11</code>）告诉我们是 <strong>nal_ref_idc</strong> 字段，其表示该 NAL 是否是参考字段，其余 5 位（<code>00111</code>）告诉我们是 <strong>nal_unit_type</strong> 字段，在这个例子里是 NAL 单元 <strong>SPS</strong> (7)。</p>\n<p>SPS NAL 的第 2 位 (<code>binary=01100100, hex=0x64, dec=100</code>) 是 <strong>profile_idc</strong> 字段，显示编码器使用的配置，在这个例子里，我们使用<a href=\"https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Profiles\" target=\"_blank\" rel=\"noopener\">受限高配置</a>，一种没有 B（双向预测） 切片支持的高配置。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/minimal_yuv420_bin.png\" alt=\"SPS 二进制视图\" title=\"SPS 二进制视图\"></p>\n<p>当我们阅读 SPS NAL 的 H.264 比特流规范时，会为<strong>参数名称</strong>，<strong>分类</strong>和<strong>描述</strong>找到许多值，例如，看看字段 <code>pic_width_in_mbs_minus_1</code> 和 <code>pic_height_in_map_units_minus_1</code>。</p>\n<table>\n<thead>\n<tr>\n<th>参数名称</th>\n<th>分类</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>pic_width_in_mbs_minus_1</td>\n<td>0</td>\n<td>ue(v)</td>\n</tr>\n<tr>\n<td>pic_height_in_map_units_minus_1</td>\n<td>0</td>\n<td>ue(v)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>ue(v)</strong>: 无符号整形 <a href=\"https://pythonhosted.org/bitstring/exp-golomb.html\" target=\"_blank\" rel=\"noopener\">Exp-Golomb-coded</a></p>\n</blockquote>\n<p>如果我们对这些字段的值进行一些计算，将最终得出<strong>分辨率</strong>。我们可以使用值为 <code>119（ (119 + 1) * macroblock_size = 120 * 16 = 1920）</code>的 <code>pic_width_in_mbs_minus_1</code> 表示 <code>1920 x 1080</code>，再次为了减少空间，我们使用 <code>119</code> 来代替编码 <code>1920</code>。</p>\n<p>如果我们再次使用二进制视图检查我们创建的视频 (ex: <code>xxd -b -c 11 v/minimal_yuv420.h264</code>)，可以跳到帧自身上一个 NAL。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/slice_nal_idr_bin.png\" alt=\"h264 idr 切片头\" title=\"h264 idr 切片头\"></p>\n<p>我们可以看到最开始的 6 个字节：<code>01100101 10001000 10000100 00000000 00100001 11111111</code>。我们已经知道第一个字节告诉我们 NAL 的类型，在这个例子里， (<code>00101</code>) 是 <strong>IDR 切片 (5)</strong>，可以进一步检查它：</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/slice_header.png\" alt=\"h264 切片头规格\" title=\"h264 切片头规格\"></p>\n<p>对照规范，我们能解码切片的类型（<strong>slice_type</strong>），帧号（<strong>frame_num</strong>）等重要字段。</p>\n<p>为了获得一些字段（<code>ue(v), me(v), se(v) 或 te(v)</code>）的值，我们需要称为 <a href=\"https://pythonhosted.org/bitstring/exp-golomb.html\" target=\"_blank\" rel=\"noopener\">Exponential-Golomb</a> 的特定解码器来解码它。当存在很多默认值时，这个方法编码变量值特别高效。</p>\n<blockquote>\n<p>这个视频里 <strong>slice_type</strong> 和 <strong>frame_num</strong> 的值是 7（I 切片）和 0（第一帧）。</p>\n</blockquote>\n<p>我们可以将<strong>比特流视为一个协议</strong>，如果你想学习更多关于比特流的内容，请参考 <a href=\"http://www.itu.int/rec/T-REC-H.264-201610-I\" target=\"_blank\" rel=\"noopener\">ITU H.264 规范</a>。这个宏观图展示了图片数据（压缩过的 YUV）所在的位置。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/h264_bitstream_macro_diagram.png\" alt=\"h264 比特流宏观图\" title=\"h264 比特流宏观图\"></p>\n<p>我们可以探究其它比特流，如 <a href=\"https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf\" target=\"_blank\" rel=\"noopener\">VP9 比特流</a>，<a href=\"http://handle.itu.int/11.1002/1000/11885-en?locatt=format:pdf\" target=\"_blank\" rel=\"noopener\">H.265（HEVC）</a>或是我们的新朋友 <a href=\"https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8\" target=\"_blank\" rel=\"noopener\">AV1 比特流</a>，<a href=\"http://www.gpac-licensing.com/2016/07/12/vp9-av1-bitstream-format/\" target=\"_blank\" rel=\"noopener\">他们很相似吗？不</a>，但只要学习了其中之一，学习其他的就简单多了。</p>\n<blockquote>\n<h3 id=\"自己动手：检查-H-264-比特流\"><a href=\"#自己动手：检查-H-264-比特流\" class=\"headerlink\" title=\"自己动手：检查 H.264 比特流\"></a>自己动手：检查 H.264 比特流</h3><p>我们可以<a href=\"https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#generate-a-single-frame-video\" target=\"_blank\" rel=\"noopener\">生成一个单帧视频</a>，使用 <a href=\"https://en.wikipedia.org/wiki/MediaInfo\" target=\"_blank\" rel=\"noopener\">mediainfo</a> 检查它的 H.264 比特流。事实上，你甚至可以查看<a href=\"https://github.com/MediaArea/MediaInfoLib/blob/master/Source/MediaInfo/Video/File_Avc.cpp\" target=\"_blank\" rel=\"noopener\">解析 h264(AVC) 视频流的源代码</a>。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/mediainfo_details_1.png\" alt=\"mediainfo h264 比特流的详情 \" title=\"mediainfo h264 比特流的详情\"></p>\n<p>我们也可使用 <a href=\"https://software.intel.com/en-us/intel-video-pro-analyzer\" target=\"_blank\" rel=\"noopener\">Intel® Video Pro Analyzer</a>，需要付费，但也有只能查看前 10 帧的免费试用版，这已经够达成学习目的了。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/intel-video-pro-analyzer.png\" alt=\"Intel® Video Pro Analyzer h264 比特流的详情\" title=\"Intel® Video Pro Analyzer h264 比特流的详情\"></p>\n</blockquote>\n<h2 id=\"回顾\"><a href=\"#回顾\" class=\"headerlink\" title=\"回顾\"></a>回顾</h2><p>我们可以看到我们学了许多<strong>使用相同模型的现代编解码器</strong>。事实上，让我们看看 Thor 视频编解码器框图，它包含所有我们学过的步骤。你现在应该能更好地理解数字视频领域内的创新和论文。<br><img src=\"/2020/02/28/Introduction-to-digital-video-technology/thor_codec_block_diagram.png\" alt=\"thor 编解码器块图\" title=\"thor 编解码器块图\"></p>\n<p>之前我们计算过我们<a href=\"#色度子采样\">需要 139GB 来保存一个一小时，720p 分辨率和30fps的视频文件</a>，如果我们使用在这里学过的技术，如<strong>帧间和帧内预测，转换，量化，熵编码和其它</strong>我们能实现——假设我们<strong>每像素花费 0.031 bit</strong>——同样观感质量的视频，<strong>对比 139GB 的存储，只需 367.82MB</strong>。</p>\n<blockquote>\n<p>我们根据这里提供的示例视频选择<strong>每像素使用 0.031 bit</strong>。</p>\n</blockquote>\n<h2 id=\"H-265-如何实现比-H-264-更好的压缩率\"><a href=\"#H-265-如何实现比-H-264-更好的压缩率\" class=\"headerlink\" title=\"H.265 如何实现比 H.264 更好的压缩率\"></a>H.265 如何实现比 H.264 更好的压缩率</h2><p>我们已经更多地了解了编解码器的工作原理，那么就容易理解新的编解码器如何使用更少的数据量传输更高分辨率的视频。</p>\n<p>我们将比较 AVC 和 HEVC，要记住的是：我们几乎总是要在压缩率和更多的 CPU 周期（复杂度）之间作权衡。</p>\n<p>HEVC 比 AVC 有更大和更多的<strong>分区</strong>（和<strong>子分区</strong>）选项，更多<strong>帧内预测方向</strong>，<strong>改进的熵编码</strong>等，所有这些改进使得 H.265 比 H.264 的压缩率提升 50%。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/avc_vs_hevc.png\" alt=\"h264 vs h265\" title=\"H.264 vs H.265\"></p>\n<h1 id=\"在线流媒体\"><a href=\"#在线流媒体\" class=\"headerlink\" title=\"在线流媒体\"></a>在线流媒体</h1><h2 id=\"通用架构\"><a href=\"#通用架构\" class=\"headerlink\" title=\"通用架构\"></a>通用架构</h2><p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/general_architecture.png\" alt=\"general_architecture\"></p>\n<p>[TODO]</p>\n<h2 id=\"渐进式下载和自适应流\"><a href=\"#渐进式下载和自适应流\" class=\"headerlink\" title=\"渐进式下载和自适应流\"></a>渐进式下载和自适应流</h2><p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/progressive_download.png\" alt=\"progressive_download\"></p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/adaptive_streaming.png\" alt=\"adaptive_streaming\"></p>\n<p>[TODO]</p>\n<h2 id=\"内容保护\"><a href=\"#内容保护\" class=\"headerlink\" title=\"内容保护\"></a>内容保护</h2><p>我们可以用一个简单的令牌认证系统来保护视频。用户需要拥有一个有效的令牌才可以播放视频，CDN 会拒绝没有令牌的用户的请求。它与大多数网站的身份认证系统非常相似。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/token_protection.png\" alt=\"token_protection\"></p>\n<p>仅仅使用令牌认证系统，用户仍然可以下载并重新分发视频。DRM 系统可以用来避免这种情况。</p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/drm.png\" alt=\"drm\"></p>\n<p>实际情况下，人们通常同时使用这两种技术提供授权和认证。</p>\n<h3 id=\"DRM\"><a href=\"#DRM\" class=\"headerlink\" title=\"DRM\"></a>DRM</h3><h4 id=\"主要系统\"><a href=\"#主要系统\" class=\"headerlink\" title=\"主要系统\"></a>主要系统</h4><ul>\n<li>FPS - <a href=\"https://developer.apple.com/streaming/fps/\" target=\"_blank\" rel=\"noopener\"><strong>FairPlay Streaming</strong></a></li>\n<li>PR - <a href=\"https://www.microsoft.com/playready/\" target=\"_blank\" rel=\"noopener\"><strong>PlayReady</strong></a></li>\n<li>WV - <a href=\"http://www.widevine.com/\" target=\"_blank\" rel=\"noopener\"><strong>Widevine</strong></a></li>\n</ul>\n<h4 id=\"是什么\"><a href=\"#是什么\" class=\"headerlink\" title=\"是什么\"></a>是什么</h4><p>DRM 指的是数字版权管理，是一种<strong>为数字媒体提供版权保护</strong>的方法，例如数字视频和音频。尽管用在了很多场合，但它并<a href=\"https://en.wikipedia.org/wiki/Digital_rights_management#DRM-free_works\" target=\"_blank\" rel=\"noopener\">没有被普遍接受</a>.</p>\n<h4 id=\"为什么\"><a href=\"#为什么\" class=\"headerlink\" title=\"为什么\"></a>为什么</h4><p>内容的创作者（大多是工作室/制片厂）希望保护他们的知识产权，使他们的数字媒体免遭未经授权的分发。</p>\n<h4 id=\"怎么做\"><a href=\"#怎么做\" class=\"headerlink\" title=\"怎么做\"></a>怎么做</h4><p>我们将用一种简单的、抽象的方式描述 DRM</p>\n<p>现有一份<strong>内容 C1</strong>（如 HLS 或 DASH 视频流），一个<strong>播放器 P1</strong>（如 shaka-clappr, exo-player 或 iOS），装在<strong>设备 D1</strong>（如智能手机、电视或台式机/笔记本）上，使用 <strong>DRM 系统 DRM1</strong>（如 FairPlay Streaming, PlayReady, Widevine）</p>\n<p>内容 C1 由 DRM1 用一个<strong>对称密钥 K1</strong> 加密，生成<strong>加密内容 C’1</strong></p>\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/drm_general_flow.jpeg\" alt=\"DRM 一般流程\" title=\"DRM 一般流程\"></p>\n<p>设备 D1 上的播放器 P1 有一个非对称密钥对，密钥对包含一个<strong>私钥 PRK1</strong>（这个密钥是受保护的<sup>1</sup>，只有 <strong>D1</strong> 知道密钥内容），和一个<strong>公钥 PUK1</strong></p>\n<blockquote>\n<p><strong><sup>1</sup>受保护的</strong>: 这种保护可以<strong>通过硬件</strong>进行保护，例如, 将这个密钥存储在一个特殊的芯片（只读）中，芯片的工作方式就像一个用来解密的[黑箱]。 或<strong>通过软件</strong>进行保护（较低的安全系数）。DRM 系统提供了识别设备所使用的保护类型的方法。</p>\n</blockquote>\n<p>当 <strong>播放器 P1 希望播放**</strong>加密内容 C’1** 时，它需要与 <strong>DRM1</strong> 协商，将公钥 <strong>PUK1</strong> 发送给 DRM1, DRM1 会返回一个被公钥 <strong>PUK1</strong> <strong>加密过的 K1</strong>。按照推论，结果就是<strong>只有 D1 能够解密</strong>。</p>\n<p><code>K1P1D1 = enc(K1, PUK1)</code></p>\n<p><strong>P1</strong> 使用它的本地 DRM 系统（这可以使用 <a href=\"https://zh.wikipedia.org/wiki/系统芯片\" target=\"_blank\" rel=\"noopener\">SoC</a> ，一个专门的硬件和软件，这个系统可以使用它的私钥 PRK1 用来<strong>解密</strong>内容，它可以解密被加密过的<strong>K1P1D1 的对称密钥 K1</strong>。理想情况下，密钥不会被导出到内存以外的地方。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">K1 &#x3D; dec(K1P1D1, PRK1)</span><br><span class=\"line\"></span><br><span class=\"line\">P1.play(dec(C&#39;1, K1))</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"/2020/02/28/Introduction-to-digital-video-technology/drm_decoder_flow.jpeg\" alt=\"DRM 解码流程\" title=\"DRM 解码流程\"></p>\n<h1 id=\"如何使用-jupyter\"><a href=\"#如何使用-jupyter\" class=\"headerlink\" title=\"如何使用 jupyter\"></a>如何使用 jupyter</h1><p>确保你已安装 docker，只需运行 <code>./s/start_jupyter.sh</code>，然后按照控制台的说明进行操作。</p>\n<h1 id=\"会议\"><a href=\"#会议\" class=\"headerlink\" title=\"会议\"></a>会议</h1><ul>\n<li><a href=\"https://demuxed.com/\" target=\"_blank\" rel=\"noopener\">DEMUXED</a> - 您可以<a href=\"https://www.youtube.com/channel/UCIc_DkRxo9UgUSTvWVNCmpA\" target=\"_blank\" rel=\"noopener\">查看最近的2个活动演示</a>。</li>\n</ul>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p>这里有最丰富的资源，这篇文档包含的信息，均摘录、依据或受它们启发。你可以用这些精彩的链接，书籍，视频等深化你的知识。</p>\n<p>在线课程和教程：</p>\n<ul>\n<li><a href=\"https://www.coursera.org/learn/digital/\" target=\"_blank\" rel=\"noopener\">https://www.coursera.org/learn/digital/</a></li>\n<li><a href=\"https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf\" target=\"_blank\" rel=\"noopener\">https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf</a></li>\n<li><a href=\"https://xiph.org/video/vid1.shtml\" target=\"_blank\" rel=\"noopener\">https://xiph.org/video/vid1.shtml</a></li>\n<li><a href=\"https://xiph.org/video/vid2.shtml\" target=\"_blank\" rel=\"noopener\">https://xiph.org/video/vid2.shtml</a></li>\n<li><a href=\"http://slhck.info/ffmpeg-encoding-course\" target=\"_blank\" rel=\"noopener\">http://slhck.info/ffmpeg-encoding-course</a></li>\n<li><a href=\"http://www.cambridgeincolour.com/tutorials/camera-sensors.htm\" target=\"_blank\" rel=\"noopener\">http://www.cambridgeincolour.com/tutorials/camera-sensors.htm</a></li>\n<li><a href=\"http://www.slideshare.net/vcodex/a-short-history-of-video-coding\" target=\"_blank\" rel=\"noopener\">http://www.slideshare.net/vcodex/a-short-history-of-video-coding</a></li>\n<li><a href=\"http://www.slideshare.net/vcodex/introduction-to-video-compression-13394338\" target=\"_blank\" rel=\"noopener\">http://www.slideshare.net/vcodex/introduction-to-video-compression-1339433</a></li>\n<li><a href=\"https://developer.android.com/guide/topics/media/media-formats.html\" target=\"_blank\" rel=\"noopener\">https://developer.android.com/guide/topics/media/media-formats.html</a></li>\n<li><a href=\"http://www.slideshare.net/MadhawaKasun/audio-compression-23398426\" target=\"_blank\" rel=\"noopener\">http://www.slideshare.net/MadhawaKasun/audio-compression-23398426</a></li>\n<li><a href=\"http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf\" target=\"_blank\" rel=\"noopener\">http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf</a></li>\n</ul>\n<p>书籍:</p>\n<ul>\n<li><a href=\"https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&ie=UTF8&qid=1486395327&sr=1-1\" target=\"_blank\" rel=\"noopener\">https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1486395327&amp;sr=1-1</a></li>\n<li><a href=\"https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925\" target=\"_blank\" rel=\"noopener\">https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925</a></li>\n<li><a href=\"https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&ie=UTF8&qid=1486396914&sr=1-3&keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO\" target=\"_blank\" rel=\"noopener\">https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&amp;ie=UTF8&amp;qid=1486396914&amp;sr=1-3&amp;keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO</a></li>\n<li><a href=\"https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&ie=UTF8&qid=1486396940&sr=1-1&keywords=jan+ozer\" target=\"_blank\" rel=\"noopener\">https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1486396940&amp;sr=1-1&amp;keywords=jan+ozer</a></li>\n</ul>\n<p>比特流规范:</p>\n<ul>\n<li><a href=\"http://www.itu.int/rec/T-REC-H.264-201610-I\" target=\"_blank\" rel=\"noopener\">http://www.itu.int/rec/T-REC-H.264-201610-I</a></li>\n<li><a href=\"http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&lang=en\" target=\"_blank\" rel=\"noopener\">http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&amp;lang=en</a></li>\n<li><a href=\"https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf\" target=\"_blank\" rel=\"noopener\">https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf</a></li>\n<li><a href=\"http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf\" target=\"_blank\" rel=\"noopener\">http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf</a></li>\n<li><a href=\"http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243\" target=\"_blank\" rel=\"noopener\">http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243</a></li>\n<li><a href=\"http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html\" target=\"_blank\" rel=\"noopener\">http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html</a></li>\n</ul>\n<p>软件:</p>\n<ul>\n<li><a href=\"https://ffmpeg.org/\" target=\"_blank\" rel=\"noopener\">https://ffmpeg.org/</a></li>\n<li><a href=\"https://ffmpeg.org/ffmpeg-all.html\" target=\"_blank\" rel=\"noopener\">https://ffmpeg.org/ffmpeg-all.html</a></li>\n<li><a href=\"https://ffmpeg.org/ffprobe.html\" target=\"_blank\" rel=\"noopener\">https://ffmpeg.org/ffprobe.html</a></li>\n<li><a href=\"https://trac.ffmpeg.org/wiki/\" target=\"_blank\" rel=\"noopener\">https://trac.ffmpeg.org/wiki/</a></li>\n<li><a href=\"https://software.intel.com/en-us/intel-video-pro-analyzer\" target=\"_blank\" rel=\"noopener\">https://software.intel.com/en-us/intel-video-pro-analyzer</a></li>\n<li><a href=\"https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8\" target=\"_blank\" rel=\"noopener\">https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8</a></li>\n</ul>\n<p>非-ITU 编解码器:</p>\n<ul>\n<li><a href=\"https://aomedia.googlesource.com/\" target=\"_blank\" rel=\"noopener\">https://aomedia.googlesource.com/</a></li>\n<li><a href=\"https://github.com/webmproject/libvpx/tree/master/vp9\" target=\"_blank\" rel=\"noopener\">https://github.com/webmproject/libvpx/tree/master/vp9</a></li>\n<li><a href=\"https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml\" target=\"_blank\" rel=\"noopener\">https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml</a></li>\n<li><a href=\"https://people.xiph.org/~jm/daala/revisiting/\" target=\"_blank\" rel=\"noopener\">https://people.xiph.org/~jm/daala/revisiting/</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=lzPaldsmJbk\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=lzPaldsmJbk</a></li>\n<li><a href=\"https://fosdem.org/2017/schedule/event/om_av1/\" target=\"_blank\" rel=\"noopener\">https://fosdem.org/2017/schedule/event/om_av1/</a></li>\n</ul>\n<p>编码概念:</p>\n<ul>\n<li><a href=\"http://x265.org/hevc-h265/\" target=\"_blank\" rel=\"noopener\">http://x265.org/hevc-h265/</a></li>\n<li><a href=\"http://slhck.info/video/2017/03/01/rate-control.html\" target=\"_blank\" rel=\"noopener\">http://slhck.info/video/2017/03/01/rate-control.html</a></li>\n<li><a href=\"http://slhck.info/video/2017/02/24/vbr-settings.html\" target=\"_blank\" rel=\"noopener\">http://slhck.info/video/2017/02/24/vbr-settings.html</a></li>\n<li><a href=\"http://slhck.info/video/2017/02/24/crf-guide.html\" target=\"_blank\" rel=\"noopener\">http://slhck.info/video/2017/02/24/crf-guide.html</a></li>\n<li><a href=\"https://arxiv.org/pdf/1702.00817v1.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1702.00817v1.pdf</a></li>\n<li><a href=\"https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors\" target=\"_blank\" rel=\"noopener\">https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors</a></li>\n<li><a href=\"http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html\" target=\"_blank\" rel=\"noopener\">http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html</a></li>\n<li><a href=\"http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html\" target=\"_blank\" rel=\"noopener\">http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html</a></li>\n<li><a href=\"https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/\" target=\"_blank\" rel=\"noopener\">https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/</a></li>\n</ul>\n<p>测试用视频序列:</p>\n<ul>\n<li><a href=\"http://bbb3d.renderfarming.net/download.html\" target=\"_blank\" rel=\"noopener\">http://bbb3d.renderfarming.net/download.html</a></li>\n<li><a href=\"https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx\" target=\"_blank\" rel=\"noopener\">https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx</a></li>\n</ul>\n<p>杂项:</p>\n<ul>\n<li><a href=\"http://stackoverflow.com/a/24890903\" target=\"_blank\" rel=\"noopener\">http://stackoverflow.com/a/24890903</a></li>\n<li><a href=\"http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264\" target=\"_blank\" rel=\"noopener\">http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264</a></li>\n<li><a href=\"http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html\" target=\"_blank\" rel=\"noopener\">http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html</a></li>\n<li><a href=\"http://vanseodesign.com/web-design/color-luminance/\" target=\"_blank\" rel=\"noopener\">http://vanseodesign.com/web-design/color-luminance/</a></li>\n<li><a href=\"http://www.biologymad.com/nervoussystem/eyenotes.htm\" target=\"_blank\" rel=\"noopener\">http://www.biologymad.com/nervoussystem/eyenotes.htm</a></li>\n<li><a href=\"http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf\" target=\"_blank\" rel=\"noopener\">http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf</a></li>\n<li><a href=\"http://www.csc.villanova.edu/~rschumey/csc4800/dct.html\" target=\"_blank\" rel=\"noopener\">http://www.csc.villanova.edu/~rschumey/csc4800/dct.html</a></li>\n<li><a href=\"http://www.explainthatstuff.com/digitalcameras.html\" target=\"_blank\" rel=\"noopener\">http://www.explainthatstuff.com/digitalcameras.html</a></li>\n<li><a href=\"http://www.hkvstar.com\" target=\"_blank\" rel=\"noopener\">http://www.hkvstar.com</a></li>\n<li><a href=\"http://www.hometheatersound.com/\" target=\"_blank\" rel=\"noopener\">http://www.hometheatersound.com/</a></li>\n<li><a href=\"http://www.lighterra.com/papers/videoencodingh264/\" target=\"_blank\" rel=\"noopener\">http://www.lighterra.com/papers/videoencodingh264/</a></li>\n<li><a href=\"http://www.red.com/learn/red-101/video-chroma-subsampling\" target=\"_blank\" rel=\"noopener\">http://www.red.com/learn/red-101/video-chroma-subsampling</a></li>\n<li><a href=\"http://www.slideshare.net/ManoharKuse/hevc-intra-coding\" target=\"_blank\" rel=\"noopener\">http://www.slideshare.net/ManoharKuse/hevc-intra-coding</a></li>\n<li><a href=\"http://www.slideshare.net/mwalendo/h264vs-hevc\" target=\"_blank\" rel=\"noopener\">http://www.slideshare.net/mwalendo/h264vs-hevc</a></li>\n<li><a href=\"http://www.slideshare.net/rvarun7777/final-seminar-46117193\" target=\"_blank\" rel=\"noopener\">http://www.slideshare.net/rvarun7777/final-seminar-46117193</a></li>\n<li><a href=\"http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf\" target=\"_blank\" rel=\"noopener\">http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf</a></li>\n<li><a href=\"http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx\" target=\"_blank\" rel=\"noopener\">http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx</a></li>\n<li><a href=\"http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&PageNum=1\" target=\"_blank\" rel=\"noopener\">http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&amp;PageNum=1</a></li>\n<li><a href=\"http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/\" target=\"_blank\" rel=\"noopener\">http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/</a></li>\n<li><a href=\"https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/\" target=\"_blank\" rel=\"noopener\">https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/</a></li>\n<li><a href=\"https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/\" target=\"_blank\" rel=\"noopener\">https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/</a></li>\n<li><a href=\"https://codesequoia.wordpress.com/category/video/\" target=\"_blank\" rel=\"noopener\">https://codesequoia.wordpress.com/category/video/</a></li>\n<li><a href=\"https://developer.apple.com/library/content/technotes/tn2224/_index.html\" target=\"_blank\" rel=\"noopener\">https://developer.apple.com/library/content/technotes/tn2224/_index.html</a></li>\n<li><a href=\"https://en.wikibooks.org/wiki/MeGUI/x264_Settings\" target=\"_blank\" rel=\"noopener\">https://en.wikibooks.org/wiki/MeGUI/x264_Settings</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/AOMedia_Video_1\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/AOMedia_Video_1</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Cone_cell\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Cone_cell</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Inter_frame\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Inter_frame</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Intra-frame_coding\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Intra-frame_coding</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Photoreceptor_cell\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Photoreceptor_cell</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Pixel_aspect_ratio\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Pixel_aspect_ratio</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Presentation_timestamp\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Presentation_timestamp</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Rod_cell\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Rod_cell</a></li>\n<li><a href=\"https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg\" target=\"_blank\" rel=\"noopener\">https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg</a></li>\n<li><a href=\"https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/\" target=\"_blank\" rel=\"noopener\">https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/</a></li>\n<li><a href=\"https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping\" target=\"_blank\" rel=\"noopener\">https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping</a></li>\n<li><a href=\"https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/\" target=\"_blank\" rel=\"noopener\">https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/</a></li>\n<li><a href=\"https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03\" target=\"_blank\" rel=\"noopener\">https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03</a></li>\n<li><a href=\"https://www.encoding.com/android/\" target=\"_blank\" rel=\"noopener\">https://www.encoding.com/android/</a></li>\n<li><a href=\"https://www.encoding.com/http-live-streaming-hls/\" target=\"_blank\" rel=\"noopener\">https://www.encoding.com/http-live-streaming-hls/</a></li>\n<li><a href=\"https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm\" target=\"_blank\" rel=\"noopener\">https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm</a></li>\n<li><a href=\"https://www.lifewire.com/cmos-image-sensor-493271\" target=\"_blank\" rel=\"noopener\">https://www.lifewire.com/cmos-image-sensor-493271</a></li>\n<li><a href=\"https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ\" target=\"_blank\" rel=\"noopener\">https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ</a></li>\n<li><a href=\"https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar\" target=\"_blank\" rel=\"noopener\">https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar</a></li>\n<li><a href=\"https://www.vcodex.com/h264avc-intra-precition/\" target=\"_blank\" rel=\"noopener\">https://www.vcodex.com/h264avc-intra-precition/</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=9vgtJJ2wwMA\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=9vgtJJ2wwMA</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=LFXN9PiOGtY\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=LFXN9PiOGtY</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=Lto-ajuqW3w&list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=Lto-ajuqW3w&amp;list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=LWxu4rkZBLw\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=LWxu4rkZBLw</a></li>\n</ul>"}],"PostAsset":[{"_id":"source/_posts/compile-libyuv-for-Android/3.jpg","slug":"3.jpg","post":"ck9kyh06v00061pdbfxoe5yu0","modified":1,"renderable":0},{"_id":"source/_posts/ffmpeg-compiled-in-macOS-Catalina-runs-crash/3.jpg","slug":"3.jpg","post":"ck9kyh06x00071pdbatp45mr2","modified":1,"renderable":0},{"_id":"source/_posts/how-to-calculate-the-SSIM-in-FFMpeg/1.jpg","slug":"1.jpg","post":"ck9kyh075000g1pdbdfwzgz73","modified":1,"renderable":0},{"_id":"source/_posts/Introduction-to-digital-video-technology/intel-video-pro-analyzer.png","slug":"intel-video-pro-analyzer.png","post":"ck9kyh0cl002l1pdb0upwejyk","modified":1,"renderable":0},{"_id":"source/_posts/Introduction-to-digital-video-technology/py/dct_experiences.ipynb","slug":"py/dct_experiences.ipynb","post":"ck9kyh0cl002l1pdb0upwejyk","modified":1,"renderable":0},{"_id":"source/_posts/Introduction-to-digital-video-technology/py/frame_difference_vs_motion_estimation_plus_residual.ipynb","slug":"py/frame_difference_vs_motion_estimation_plus_residual.ipynb","post":"ck9kyh0cl002l1pdb0upwejyk","modified":1,"renderable":0},{"_id":"source/_posts/Introduction-to-digital-video-technology/py/uniform_quantization_experience.ipynb","slug":"py/uniform_quantization_experience.ipynb","post":"ck9kyh0cl002l1pdb0upwejyk","modified":1,"renderable":0},{"_id":"source/_posts/analysize-SRT-protocol-live-stream-with-wireshark/4.jpg","slug":"4.jpg","post":"ck9kyh06400001pdbg9p27a1d","modified":1,"renderable":0},{"_id":"source/_posts/introduction-to-image-pyramid/3.jpg","slug":"3.jpg","post":"ck9kyh076000i1pdbc2lrc8ap","modified":1,"renderable":0},{"_id":"source/_posts/Introduction-to-digital-video-technology/avc_vs_hevc.png","slug":"avc_vs_hevc.png","post":"ck9kyh0cl002l1pdb0upwejyk","modified":1,"renderable":0},{"_id":"source/_posts/Introduction-to-digital-video-technology/inter_prediction_intel_video_pro_analyzer.png","slug":"inter_prediction_intel_video_pro_analyzer.png","post":"ck9kyh0cl002l1pdb0upwejyk","modified":1,"renderable":0},{"_id":"source/_posts/how-to-calculate-the-MS-SSIM/1.jpg","post":"ck9kyh072000c1pdbhrwnfyje","slug":"1.jpg","modified":1,"renderable":1},{"_id":"source/_posts/how-to-calculate-the-MS-SSIM/test_msssim.cpp","post":"ck9kyh072000c1pdbhrwnfyje","slug":"test_msssim.cpp","modified":1,"renderable":1},{"_id":"source/_posts/where-of-what-is-past-is-prologue/1.jpg","slug":"1.jpg","post":"ck9kyh07g000x1pdb4ekn4uee","modified":1,"renderable":0},{"_id":"source/_posts/where-of-what-is-past-is-prologue/2.jpg","post":"ck9kyh07g000x1pdb4ekn4uee","slug":"2.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/1.png","post":"ck9kyh06p00021pdb2ye40dks","slug":"1.png","modified":1,"renderable":1},{"_id":"source/_posts/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/2.jpg","slug":"2.jpg","post":"ck9kyh06p00021pdb2ye40dks","modified":1,"renderable":0},{"_id":"source/_posts/Matplotlib-s-backends-and-non-interactive-backends-for-rendering/3.jpg","post":"ck9kyh06p00021pdb2ye40dks","slug":"3.jpg","modified":1,"renderable":1},{"_id":"source/_posts/compile-libyuv-for-Android/1.jpg","post":"ck9kyh06v00061pdbfxoe5yu0","slug":"1.jpg","modified":1,"renderable":1},{"_id":"source/_posts/compile-libyuv-for-Android/2.png","post":"ck9kyh06v00061pdbfxoe5yu0","slug":"2.png","modified":1,"renderable":1},{"_id":"source/_posts/how-to-calculate-the-SSIM-in-FFMpeg/2.jpg","slug":"2.jpg","post":"ck9kyh075000g1pdbdfwzgz73","modified":1,"renderable":0},{"_id":"source/_posts/how-to-calculate-the-SSIM-in-FFMpeg/3.jpg","post":"ck9kyh075000g1pdbdfwzgz73","slug":"3.jpg","modified":1,"renderable":1},{"_id":"source/_posts/use-hexo-and-github-for-blog/1.jpg","post":"ck9kyh07f000u1pdb33981rry","slug":"1.jpg","modified":1,"renderable":1},{"_id":"source/_posts/use-hexo-and-github-for-blog/2.jpg","post":"ck9kyh07f000u1pdb33981rry","slug":"2.jpg","modified":1,"renderable":1},{"_id":"source/_posts/use-hexo-and-github-for-blog/3.jpg","post":"ck9kyh07f000u1pdb33981rry","slug":"3.jpg","modified":1,"renderable":1},{"_id":"source/_posts/ffmpeg-compiled-in-macOS-Catalina-runs-crash/1.jpg","post":"ck9kyh06x00071pdbatp45mr2","slug":"1.jpg","modified":1,"renderable":1},{"_id":"source/_posts/ffmpeg-compiled-in-macOS-Catalina-runs-crash/2.jpg","post":"ck9kyh06x00071pdbatp45mr2","slug":"2.jpg","modified":1,"renderable":1},{"_id":"source/_posts/ffmpeg-compiled-in-macOS-Catalina-runs-crash/4.jpg","post":"ck9kyh06x00071pdbatp45mr2","slug":"4.jpg","modified":1,"renderable":1},{"_id":"source/_posts/analysize-SRT-protocol-live-stream-with-wireshark/1.png","post":"ck9kyh06400001pdbg9p27a1d","slug":"1.png","modified":1,"renderable":1},{"_id":"source/_posts/analysize-SRT-protocol-live-stream-with-wireshark/2.jpg","slug":"2.jpg","post":"ck9kyh06400001pdbg9p27a1d","modified":1,"renderable":0},{"_id":"source/_posts/analysize-SRT-protocol-live-stream-with-wireshark/3.jpg","post":"ck9kyh06400001pdbg9p27a1d","slug":"3.jpg","modified":1,"renderable":1},{"_id":"source/_posts/analysize-SRT-protocol-live-stream-with-wireshark/5.jpg","slug":"5.jpg","post":"ck9kyh06400001pdbg9p27a1d","modified":1,"renderable":0},{"_id":"source/_posts/thinking-about-QA-my-years-for-work-as-a-QA/1.jpg","post":"ck9kyh07c000p1pdbbk56eemv","slug":"1.jpg","modified":1,"renderable":1},{"_id":"source/_posts/thinking-about-QA-my-years-for-work-as-a-QA/2.png","slug":"2.png","post":"ck9kyh07c000p1pdbbk56eemv","modified":1,"renderable":0},{"_id":"source/_posts/thinking-about-QA-my-years-for-work-as-a-QA/3.jpg","post":"ck9kyh07c000p1pdbbk56eemv","slug":"3.jpg","modified":1,"renderable":1},{"_id":"source/_posts/thinking-about-QA-my-years-for-work-as-a-QA/4.jpg","post":"ck9kyh07c000p1pdbbk56eemv","slug":"4.jpg","modified":1,"renderable":1},{"_id":"source/_posts/thinking-about-QA-my-years-for-work-as-a-QA/5.jpg","post":"ck9kyh07c000p1pdbbk56eemv","slug":"5.jpg","modified":1,"renderable":1},{"_id":"source/_posts/thinking-about-QA-my-years-for-work-as-a-QA/6.jpg","slug":"6.jpg","post":"ck9kyh07c000p1pdbbk56eemv","modified":1,"renderable":0},{"_id":"source/_posts/introduction-to-image-pyramid/1.jpg","slug":"1.jpg","post":"ck9kyh076000i1pdbc2lrc8ap","modified":1,"renderable":0},{"_id":"source/_posts/introduction-to-image-pyramid/2.jpg","slug":"2.jpg","post":"ck9kyh076000i1pdbc2lrc8ap","modified":1,"renderable":0},{"_id":"source/_posts/introduction-to-image-pyramid/4.jpg","slug":"4.jpg","post":"ck9kyh076000i1pdbc2lrc8ap","modified":1,"renderable":0},{"_id":"source/_posts/introduction-to-image-pyramid/5.jpg","slug":"5.jpg","post":"ck9kyh076000i1pdbc2lrc8ap","modified":1,"renderable":0},{"_id":"source/_posts/introduction-to-image-pyramid/6.jpg","post":"ck9kyh076000i1pdbc2lrc8ap","slug":"6.jpg","modified":1,"renderable":1},{"_id":"source/_posts/introduction-to-image-pyramid/7.jpg","post":"ck9kyh076000i1pdbc2lrc8ap","slug":"7.jpg","modified":1,"renderable":1},{"_id":"source/_posts/introduction-to-image-pyramid/8.jpg","slug":"8.jpg","post":"ck9kyh076000i1pdbc2lrc8ap","modified":1,"renderable":0},{"_id":"source/_posts/introduction-to-image-pyramid/apple.jpg","post":"ck9kyh076000i1pdbc2lrc8ap","slug":"apple.jpg","modified":1,"renderable":1},{"_id":"source/_posts/introduction-to-image-pyramid/image_pyramid_blend.ipynb","slug":"image_pyramid_blend.ipynb","post":"ck9kyh076000i1pdbc2lrc8ap","modified":1,"renderable":0},{"_id":"source/_posts/introduction-to-image-pyramid/orange.jpg","post":"ck9kyh076000i1pdbc2lrc8ap","slug":"orange.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/py/dct_better_explained.ipynb","slug":"py/dct_better_explained.ipynb","post":"ck9kyh0cl002l1pdb0upwejyk","modified":1,"renderable":0},{"_id":"source/_posts/Introduction-to-digital-video-technology/py/filters_are_easy.ipynb","slug":"py/filters_are_easy.ipynb","post":"ck9kyh0cl002l1pdb0upwejyk","modified":1,"renderable":0},{"_id":"source/_posts/Introduction-to-digital-video-technology/DAR.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"DAR.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/H.264_block_diagram_with_quality_score.jpg","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"H.264_block_diagram_with_quality_score.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/PAR.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"PAR.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/adaptive_streaming.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"adaptive_streaming.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/applicat.jpg","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"applicat.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/arithimetic_range.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"arithimetic_range.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/av1_browser_analyzer.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"av1_browser_analyzer.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/chroma_subsampling_examples.jpg","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"chroma_subsampling_examples.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/codec_history_timeline.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"codec_history_timeline.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/comparison_delta_vs_motion_estimation.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"comparison_delta_vs_motion_estimation.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/dct_basis.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"dct_basis.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/dct_coefficient_image.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"dct_coefficient_image.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/dct_coefficient_values.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"dct_coefficient_values.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/dct_coefficient_zeroed.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"dct_coefficient_zeroed.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/dctfrequ.jpg","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"dctfrequ.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/difference_frames.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"difference_frames.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/drm.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"drm.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/drm_decoder_flow.jpeg","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"drm_decoder_flow.jpeg","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/drm_general_flow.jpeg","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"drm_general_flow.jpeg","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/eyes.jpg","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"eyes.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/frame_types.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"frame_types.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/general_architecture.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"general_architecture.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/gray_image.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"gray_image.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/h264_bitstream_macro_diagram.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"h264_bitstream_macro_diagram.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/h264_intra_predictions.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"h264_intra_predictions.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/image_3d_matrix_rgb.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"image_3d_matrix_rgb.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/interlaced_vs_progressive.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"interlaced_vs_progressive.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/intra_prediction_intel_video_pro_analyzer.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"intra_prediction_intel_video_pro_analyzer.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/jpeg_quantization_table.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"jpeg_quantization_table.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/kernel_convolution.jpg","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"kernel_convolution.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/luminance_vs_color.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"luminance_vs_color.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/macro_blocks_ffmpeg.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"macro_blocks_ffmpeg.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/mediainfo_details_1.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"mediainfo_details_1.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/mediainfo_details_2.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"mediainfo_details_2.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/minimal.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"minimal.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/minimal_yuv420_bin.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"minimal_yuv420_bin.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/minimal_yuv420_hex.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"minimal_yuv420_hex.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/minimal_yuv420_hex_string.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"minimal_yuv420_hex_string.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/motion_estimation.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"motion_estimation.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/motion_vectors_ffmpeg.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"motion_vectors_ffmpeg.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/nal_units.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"nal_units.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/nes-color-palette.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"nes-color-palette.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/new_pixel_geometry.jpg","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"new_pixel_geometry.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/original_frames.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"original_frames.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/original_frames_motion_estimation.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"original_frames_motion_estimation.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/original_vs_quantized.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"original_vs_quantized.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/paritions_view_intel_video_pro_analyzer.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"paritions_view_intel_video_pro_analyzer.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/picture_partitioning.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"picture_partitioning.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/pixel_matrice.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"pixel_matrice.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/progressive_download.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"progressive_download.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/py/image_as_3d_array.ipynb","slug":"py/image_as_3d_array.ipynb","post":"ck9kyh0cl002l1pdb0upwejyk","modified":1,"renderable":0},{"_id":"source/_posts/Introduction-to-digital-video-technology/py/image_transform_frequency_domain.ipynb","slug":"py/image_transform_frequency_domain.ipynb","post":"ck9kyh0cl002l1pdb0upwejyk","modified":1,"renderable":0},{"_id":"source/_posts/Introduction-to-digital-video-technology/quantize.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"quantize.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/range.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"range.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/range_show.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"range_show.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/re-quantize.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"re-quantize.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/real_world_motion_compensation.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"real_world_motion_compensation.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/repetition_in_time.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"repetition_in_time.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/repetitions_in_space.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"repetitions_in_space.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/resolution.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"resolution.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/rgb_channels_intensity.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"rgb_channels_intensity.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/s/cut_smaller_video.sh","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"s/cut_smaller_video.sh","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/s/download_video.sh","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"s/download_video.sh","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/s/ffmpeg","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"s/ffmpeg","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/s/mediainfo","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"s/mediainfo","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/s/start_jupyter.sh","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"s/start_jupyter.sh","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/s/vmaf","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"s/vmaf","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/second_subrange.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"second_subrange.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/slice_header.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"slice_header.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/slice_nal_idr_bin.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"slice_nal_idr_bin.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_background.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"smw_background.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_background_ball_1.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"smw_background_ball_1.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_background_ball_2.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"smw_background_ball_2.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_background_ball_2_diff.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"smw_background_ball_2_diff.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_background_ball_3.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"smw_background_ball_3.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_background_ball_4.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"smw_background_ball_4.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_bg.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"smw_bg.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_bg_block.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"smw_bg_block.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_bg_prediction.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"smw_bg_prediction.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/smw_residual.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"smw_residual.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/solid_background.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"solid_background.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/solid_background_1.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"solid_background_1.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/solid_background_2.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"solid_background_2.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/solid_background_3.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"solid_background_3.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/solid_background_4.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"solid_background_4.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/solid_background_ball_1.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"solid_background_ball_1.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/solid_background_ball_2.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"solid_background_ball_2.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/super_mario_head.jpg","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"super_mario_head.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/super_mario_head.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"super_mario_head.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/super_mario_world.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"super_mario_world.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/thor_codec_block_diagram.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"thor_codec_block_diagram.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/token_protection.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"token_protection.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/v/small_bunny_1080p_30fps.mp4","slug":"v/small_bunny_1080p_30fps.mp4","post":"ck9kyh0cl002l1pdb0upwejyk","modified":1,"renderable":0},{"_id":"source/_posts/Introduction-to-digital-video-technology/v/small_bunny_1080p_60fps.mp4","slug":"v/small_bunny_1080p_60fps.mp4","post":"ck9kyh0cl002l1pdb0upwejyk","modified":1,"renderable":0},{"_id":"source/_posts/Introduction-to-digital-video-technology/vbr.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"vbr.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/video.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"video.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/ycbcr.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"ycbcr.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/ycbcr_420_merge.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"ycbcr_420_merge.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/ycbcr_subsampling_resolution.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"ycbcr_subsampling_resolution.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/yuv_histogram.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"yuv_histogram.png","modified":1,"renderable":1},{"_id":"source/_posts/Introduction-to-digital-video-technology/z_char_8x8.png","post":"ck9kyh0cl002l1pdb0upwejyk","slug":"z_char_8x8.png","modified":1,"renderable":1}],"PostCategory":[{"post_id":"ck9kyh06x00071pdbatp45mr2","category_id":"ck9kyh06s00041pdb1h5f23cr","_id":"ck9kyh073000d1pdb5b7ggl2r"},{"post_id":"ck9kyh06400001pdbg9p27a1d","category_id":"ck9kyh06s00041pdb1h5f23cr","_id":"ck9kyh076000h1pdb8o4l1dcd"},{"post_id":"ck9kyh06z00081pdbbfh95nrm","category_id":"ck9kyh06s00041pdb1h5f23cr","_id":"ck9kyh077000j1pdb1xcbg94e"},{"post_id":"ck9kyh06p00021pdb2ye40dks","category_id":"ck9kyh06z00091pdb59w28ahl","_id":"ck9kyh07a000n1pdbfv60dy1i"},{"post_id":"ck9kyh072000c1pdbhrwnfyje","category_id":"ck9kyh06s00041pdb1h5f23cr","_id":"ck9kyh07d000q1pdb1dbha4ut"},{"post_id":"ck9kyh075000g1pdbdfwzgz73","category_id":"ck9kyh06s00041pdb1h5f23cr","_id":"ck9kyh07f000v1pdbf4eggmy0"},{"post_id":"ck9kyh06v00061pdbfxoe5yu0","category_id":"ck9kyh074000e1pdbht4t64md","_id":"ck9kyh07i000y1pdb5ufxad5z"},{"post_id":"ck9kyh071000b1pdb1nb3281z","category_id":"ck9kyh077000k1pdb7fcharb6","_id":"ck9kyh07j000z1pdb8vq92kb1"},{"post_id":"ck9kyh07f000u1pdb33981rry","category_id":"ck9kyh077000k1pdb7fcharb6","_id":"ck9kyh07k00121pdb4b3eheee"},{"post_id":"ck9kyh076000i1pdbc2lrc8ap","category_id":"ck9kyh07e000s1pdb6ytl8cot","_id":"ck9kyh07l00131pdbe7czfh9g"},{"post_id":"ck9kyh07c000p1pdbbk56eemv","category_id":"ck9kyh07j00101pdb7yw5f8el","_id":"ck9kyh07r00181pdb7jba0szu"},{"post_id":"ck9kyh07g000x1pdb4ekn4uee","category_id":"ck9kyh07j00101pdb7yw5f8el","_id":"ck9kyh07v001c1pdbevbx7kk4"},{"post_id":"ck9kyh0cl002l1pdb0upwejyk","category_id":"ck9kyh06s00041pdb1h5f23cr","_id":"ck9kyh0cp002n1pdb4b8x8lx1"}],"PostTag":[{"post_id":"ck9kyh06400001pdbg9p27a1d","tag_id":"ck9kyh06u00051pdbg1mhctj2","_id":"ck9kyh07b000o1pdbfavc7ktq"},{"post_id":"ck9kyh06400001pdbg9p27a1d","tag_id":"ck9kyh070000a1pdb9ihadhiw","_id":"ck9kyh07d000r1pdb4izjfwks"},{"post_id":"ck9kyh06400001pdbg9p27a1d","tag_id":"ck9kyh074000f1pdb30g01mxm","_id":"ck9kyh07f000w1pdb2sfghwta"},{"post_id":"ck9kyh06p00021pdb2ye40dks","tag_id":"ck9kyh078000l1pdb1ddt2j4i","_id":"ck9kyh07r00161pdb9kpg1wrw"},{"post_id":"ck9kyh06p00021pdb2ye40dks","tag_id":"ck9kyh07e000t1pdb7vc7an7f","_id":"ck9kyh07r00171pdbau8rchpj"},{"post_id":"ck9kyh06p00021pdb2ye40dks","tag_id":"ck9kyh07j00111pdb0hv36q4n","_id":"ck9kyh07u001a1pdbhgmc3055"},{"post_id":"ck9kyh06v00061pdbfxoe5yu0","tag_id":"ck9kyh07l00151pdb8cza2xee","_id":"ck9kyh07v001b1pdb1zaag8jg"},{"post_id":"ck9kyh06x00071pdbatp45mr2","tag_id":"ck9kyh07s00191pdbg6vlggn6","_id":"ck9kyh07x001g1pdb0i3x36br"},{"post_id":"ck9kyh06x00071pdbatp45mr2","tag_id":"ck9kyh07v001d1pdbc37ahfbb","_id":"ck9kyh07x001h1pdb35f9dj07"},{"post_id":"ck9kyh06x00071pdbatp45mr2","tag_id":"ck9kyh07w001e1pdbbjie9hpu","_id":"ck9kyh07z001j1pdbd3pucus6"},{"post_id":"ck9kyh06z00081pdbbfh95nrm","tag_id":"ck9kyh07x001f1pdbegu6ccn7","_id":"ck9kyh082001m1pdbhk4r7o3j"},{"post_id":"ck9kyh06z00081pdbbfh95nrm","tag_id":"ck9kyh07x001i1pdb41qv2zfy","_id":"ck9kyh082001n1pdb0xb311x5"},{"post_id":"ck9kyh06z00081pdbbfh95nrm","tag_id":"ck9kyh080001k1pdb56bzbnn2","_id":"ck9kyh083001p1pdb734r78y0"},{"post_id":"ck9kyh071000b1pdb1nb3281z","tag_id":"ck9kyh080001l1pdbgtj46sbi","_id":"ck9kyh083001q1pdbgdlu8fuy"},{"post_id":"ck9kyh072000c1pdbhrwnfyje","tag_id":"ck9kyh082001o1pdbe15tf9j7","_id":"ck9kyh085001t1pdbgsgmbvak"},{"post_id":"ck9kyh072000c1pdbhrwnfyje","tag_id":"ck9kyh084001r1pdb0d1hdbdz","_id":"ck9kyh08e001u1pdb2vk91lqa"},{"post_id":"ck9kyh075000g1pdbdfwzgz73","tag_id":"ck9kyh082001o1pdbe15tf9j7","_id":"ck9kyh08n001x1pdb9ixkhse4"},{"post_id":"ck9kyh075000g1pdbdfwzgz73","tag_id":"ck9kyh07s00191pdbg6vlggn6","_id":"ck9kyh08n001y1pdbbeay2wgf"},{"post_id":"ck9kyh076000i1pdbc2lrc8ap","tag_id":"ck9kyh08k001w1pdbd06cd9r1","_id":"ck9kyh0a200231pdb42r5c0vt"},{"post_id":"ck9kyh076000i1pdbc2lrc8ap","tag_id":"ck9kyh08n001z1pdb2n565vrg","_id":"ck9kyh0a500241pdbgujg151k"},{"post_id":"ck9kyh076000i1pdbc2lrc8ap","tag_id":"ck9kyh08o00201pdb1p8q960i","_id":"ck9kyh0aa00261pdb574g4gw0"},{"post_id":"ck9kyh076000i1pdbc2lrc8ap","tag_id":"ck9kyh09b00211pdb1t2fg9kk","_id":"ck9kyh0ak00271pdbbiszar5u"},{"post_id":"ck9kyh079000m1pdbhnef6ikm","tag_id":"ck9kyh07s00191pdbg6vlggn6","_id":"ck9kyh0an00291pdb1val1gr9"},{"post_id":"ck9kyh079000m1pdbhnef6ikm","tag_id":"ck9kyh082001o1pdbe15tf9j7","_id":"ck9kyh0ap002a1pdbdxll3bh6"},{"post_id":"ck9kyh07c000p1pdbbk56eemv","tag_id":"ck9kyh0ak00281pdbd4u3hsmw","_id":"ck9kyh0b4002c1pdbctti9bmf"},{"post_id":"ck9kyh07f000u1pdb33981rry","tag_id":"ck9kyh080001l1pdbgtj46sbi","_id":"ck9kyh0bm002g1pdb27wuhphm"},{"post_id":"ck9kyh07f000u1pdb33981rry","tag_id":"ck9kyh0bi002d1pdb9ppph6fv","_id":"ck9kyh0bn002h1pdb8gw4bmws"},{"post_id":"ck9kyh07f000u1pdb33981rry","tag_id":"ck9kyh0bm002e1pdbela66hx2","_id":"ck9kyh0by002i1pdb72zlef6q"},{"post_id":"ck9kyh07g000x1pdb4ekn4uee","tag_id":"ck9kyh0ak00281pdbd4u3hsmw","_id":"ck9kyh0by002j1pdbfwhr4atq"},{"post_id":"ck9kyh0cl002l1pdb0upwejyk","tag_id":"ck9kyh07x001f1pdbegu6ccn7","_id":"ck9kyh0cm002m1pdbds36ca5o"}],"Tag":[{"name":"Wireshark","_id":"ck9kyh06u00051pdbg1mhctj2"},{"name":"SRT","_id":"ck9kyh070000a1pdb9ihadhiw"},{"name":"网络协议分析","_id":"ck9kyh074000f1pdb30g01mxm"},{"name":"python","_id":"ck9kyh078000l1pdb1ddt2j4i"},{"name":"matplotlib","_id":"ck9kyh07e000t1pdb7vc7an7f"},{"name":"non-interactive rendering","_id":"ck9kyh07j00111pdb0hv36q4n"},{"name":"libyuv","_id":"ck9kyh07l00151pdb8cza2xee"},{"name":"FFMpeg","_id":"ck9kyh07s00191pdbg6vlggn6"},{"name":"编译","_id":"ck9kyh07v001d1pdbc37ahfbb"},{"name":"macOS Catalina","_id":"ck9kyh07w001e1pdbbjie9hpu"},{"name":"数字视频","_id":"ck9kyh07x001f1pdbegu6ccn7"},{"name":"digital video","_id":"ck9kyh07x001i1pdb41qv2zfy"},{"name":"视频基础知识","_id":"ck9kyh080001k1pdb56bzbnn2"},{"name":"hexo","_id":"ck9kyh080001l1pdbgtj46sbi"},{"name":"SSIM","_id":"ck9kyh082001o1pdbe15tf9j7"},{"name":"MS-SSIM","_id":"ck9kyh084001r1pdb0d1hdbdz"},{"name":"图像金字塔","_id":"ck9kyh08k001w1pdbd06cd9r1"},{"name":"高斯金字塔","_id":"ck9kyh08n001z1pdb2n565vrg"},{"name":"拉普拉斯金字塔","_id":"ck9kyh08o00201pdb1p8q960i"},{"name":"image pyramid","_id":"ck9kyh09b00211pdb1t2fg9kk"},{"name":"测试工作","_id":"ck9kyh0ak00281pdbd4u3hsmw"},{"name":"github","_id":"ck9kyh0bi002d1pdb9ppph6fv"},{"name":"建站","_id":"ck9kyh0bm002e1pdbela66hx2"}]}}